{
    "tables": [
        {
            "banking": [
                "age",
                "job",
                "marital",
                "education",
                "default",
                "housing",
                "loan",
                "contact",
                "month",
                "day_of_week",
                "duration",
                "campaign",
                "pdays",
                "previous",
                "poutcome",
                "emp_var_rate",
                "cons_price_idx",
                "cons_conf_idx",
                "euribor3m",
                "nr_employed",
                "y"
            ]
        }
    ],
    "analyzed_topics": [
        {
            "topic": "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies",
            "ML_Models": "Random Forest, Gradient Boosting, Logistic Regression, Neural Networks",
            "reasoning": "The banking dataset contains customer demographic data (age, job, marital status, education) and interaction history (contact, duration, campaign, previous) that can be used to predict which customers are likely to leave. The target variable 'y' could represent churn status, allowing models to identify patterns of customers at risk of leaving."
        },
        {
            "topic": "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates",
            "ML_Models": "XGBoost, Support Vector Machines, Decision Trees, Ensemble Methods",
            "reasoning": "The dataset includes financial indicators (default, housing, loan) and economic variables (emp_var_rate, cons_price_idx, euribor3m) that can be used to assess creditworthiness. These features can help predict the likelihood of loan defaults, enabling more accurate risk assessment and better loan approval decisions."
        },
        {
            "topic": "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI",
            "ML_Models": "K-Means Clustering, Collaborative Filtering, Decision Trees, Naive Bayes",
            "reasoning": "Campaign-related columns (campaign, pdays, poutcome, previous) combined with customer profiles (age, job, education) can be used to segment customers and predict campaign success. Models can identify which customers are most likely to respond to specific marketing approaches, optimizing campaign targeting."
        },
        {
            "topic": "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions",
            "ML_Models": "ARIMA, LSTM Networks, Prophet, Random Forest Regressors",
            "reasoning": "Economic indicators in the dataset (emp_var_rate, cons_price_idx, cons_conf_idx, euribor3m, nr_employed) can be used to predict market trends and economic conditions. These forecasts can help the bank adjust strategies, interest rates, and product offerings based on anticipated market changes."
        }
    ],
    "csv_files": [
        "csv_test/banking.csv"
    ],
    "topic": [
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies",
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates",
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI",
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions"
    ],
    "ScrapedArticles": {
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies": "GitHub - arnav-ds/customer-churn-prediction: A machine learning project ... : \nNavigation Menu\n\narnav-ds/customer-churn-prediction\n\nFolders and files\n\nLatest commit\n\nHistory\n\nRepository files navigation\n\nCustomer Churn Prediction Project\n\nOverview\n\nThis project focuses on developing a machine learning model to predict customer churn. The goal is to identify customers who are likely to stop using a service, enabling proactive retention efforts. The entire workflow, from data exploration to model evaluation, is implemented in a Jupyter Notebook using Python and popular data science libraries.\n\nMotivation\n\nRetaining existing customers is often more cost-effective than acquiring new ones. By building an accurate churn prediction model, businesses can:\n\nJupyter Notebook (customer_churn_prediction.ipynb)\n\nThe core of this project is the Jupyter Notebook, which encompasses the following key stages:\n\nKey Findings (Example - Replace with your actual findings)\n\nTechnical Skills Demonstrated\n\nHow to Run the Code\n\nDataset\n\nThis project assumes the use of a customer churn dataset in CSV format (your_dataset.csv)(Telco Customer Churn dataset from Kaggle). The dataset should contain features relevant to customer behavior and a target variable indicating whether a customer has churned or not.\n\nPotential Improvements and Next Steps\n\nAbout\n\nA machine learning project focused on predicting customer churn to enable proactive retention strategies. Built using Python in a Jupyter Notebook, it includes data handling, model building (Logistic Regression, Random Forest, Gradient Boosting), performance evaluation, and identification of key churn drivers through visualizations.\n\nResources\n\nStars\n\nWatchers\n\nForks\n\nReleases\n\nPackages\n\nContributors\n2\n\nLanguages\n\nFooter\n\nFooter navigation\n\nCustomer churn prediction model based on hybrid neural networks - Nature : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nAdvertisement\n\nCustomer churn prediction model based on hybrid neural networks\n\nScientific Reports\n 14, Article number: 30707 (2024) Cite this article\n\n3557 Accesses\n\nMetrics\n\nAbstract\n\nIn today’s competitive market environment, accurately identifying potential churn customers and taking effective retention measures are crucial for improving customer retention and ensuring the sustainable development of an organization. However, traditional machine learning algorithms and single deep learning models have limitations in extracting complex nonlinear and time-series features, resulting in unsatisfactory prediction results. To address this problem, this study proposes a hybrid neural network-based customer churn prediction model, CCP-Net. In the data preprocessing stage, the ADASYN sampling algorithm balances the sample sizes of churned and non-churned customers to eliminate the negative impact of sample imbalance on the model performance. In the feature extraction stage, CCP-Net uses Multi-Head Self-Attention to learn the global dependencies of the input sequences, combines with BiLSTM to capture the long-term dependencies in the sequential data, and uses CNN to extract the local features, and ultimately generates the prediction results. Experimental results of cross-validation on Telecom, Bank, Insurance, and News datasets show that CCP-Net outperforms the comparison algorithms in all performance metrics. For example, CCP-Net achieves a Precision of 92.19% on the Telecom dataset, 91.96% on the Bank dataset, 95.87% on the Insurance dataset, and 95.12% on the News dataset, which compares to other hybrid neural network models, the performance improvement of CCP-Net ranges from 1% to 3%. These results indicate that the design of the CCP-Net model effectively improves the accuracy and robustness of churn prediction, enabling it to be widely applied to different industries, especially in the financial, telecommunication, and media fields, to provide more comprehensive and effective churn management strategies for enterprises.\n\nSimilar content being viewed by others\n\nCustomer churn prediction using composite deep learning technique\n\nA novel classification algorithm for customer churn prediction based on hybrid Ensemble-Fusion model\n\nEnhancing customer retention in telecom industry with machine learning driven churn prediction\n\nIntroduction\n\nIn today’s increasingly competitive business environment, the impact of customer churn on organizations is becoming increasingly significant. Churn refers to the behavior of customers who stop purchasing or using a company’s products or services, including canceling subscriptions, terminating services, or switching to competitors. This phenomenon directly cuts down an organization’s revenue and may damage its reputation, thus threatening its long-term development.\n\nWith the advent of the big data era, the application of Customer Relationship Management (CRM) systems in customer data analysis is becoming increasingly critical. CRM systems can integrate customer contact information, interaction records, and purchase history, helping organizations comprehensively understand customers’ needs and gain real-time access to customer trends1. Studies have shown that increased customer churn significantly reduces business profits, and the cost of acquiring new customers is usually five times higher than maintaining existing ones2,3. Therefore, in the fierce market competition, enterprises need to carry out customer churn prediction, identify potential lost customers, and take targeted retention strategies to improve customer loyalty and ensure the sustainable development of enterprises4.\n\nRelated work\n\nLiterature review\n\nIn the field of customer churn prediction, traditional methods mainly rely on machine learning algorithms, including Support Vector Machine(SVM)1,5, K-means clustering algorithm (K-means), Logistic Regression (LR)6,7, Naive Bayes (NB)6,8, K-nearest Neighbor (KNN), Decision Tree (DT)7,8,9, Random Forest (RF)7,8, LGBM, Adaboost10, and XGBoost11.\n\nXiahou et al.1 proposed using K-means combined with SVM for customer churn prediction. The study showed that customer clustering by K-means significantly improves prediction accuracy. NV et al.6 explored the application of algorithms such as LR and NB in banking datasets and found that NB outperforms LR. Kiguchi et al.7 investigated the predictive effectiveness of LR, DT, and RF in the education market and the results showed that LR has the best performance. Ahmad et al.9 tested algorithms such as DT, RF, and XGBoost in the Spark environment and found that XGBoost performs best. Lalwani et al.10 performed feature selection by gravitational search algorithms and found that AdaBoost and XGBoost are the leading performances among multiple models. Sikri et al.12 proposed a Ratio-based sample balancing technique for unbalanced customer churn data and combined it with multiple independent machine learning algorithms for prediction. Dhanawade et al.11 significantly improved the model’s performance for minority-class customer identification by combining multiple machine learning algorithms with SMOTE. The model can identify customers in a few categories and its prediction accuracy. He et al.13 proposed an integrated learning approach called Ensemble-Fusion, which combines 17 algorithms and outperforms a single model.\n\nAlthough traditional machine learning algorithms perform well in processing structured data and simple feature engineering tasks, they are limited in extracting complex nonlinear features and processing large-scale datasets as the size and complexity of the data increase, leading to a decrease in prediction accuracy.\n\nIn contrast to machine learning algorithms, deep learning algorithms show significant advantages in dealing with high-dimensional data and complex feature relationships, so more and more researchers are beginning to apply deep learning to customer churn prediction. Common deep learning algorithms include Backpropagation (BP)14, Artificial Neural Networks (ANN)15,16, Multi-Layer Perceptron (MLP)17,18, Convolutional Neural Network (CNN)19, Recurrent Neural Network (RNN)20, Long Short-Term Memory (LSTM)19,20, Bi-directional Long Short-Term Memory (BiLSTM)21, TabNet22,and Transformer23.\n\nKhine et al.24 proposed a model combining K-means and MLP for bank customer churn prediction, which performs user clustering through K-means and subsequently uses MLP for prediction, and the results show that the model has high prediction accuracy and short training time. Venkatesh et al.25 developed an MLP model based on Artificial Fish Swarm Algorithm for customer churn prediction in IoT environment with an accuracy of 93.52%. In addition, Saha et al.15 tested integrated learning algorithms (AdaBoost, RF, XGBoost, LGBM) with ANNs and CNNs and found that ANNs and CNNs exhibited the highest accuracy in the two datasets, respectively. Zhang et al.21 explored the application of BiLSTM for customer churn prediction, and the experiments showed that BiLSTM outperforms traditional machine learning methods in terms of accuracy. Latheef et al.26 proposed the use of SMOTE to deal with category imbalance before using LSTM for churn prediction, and the experimental results showed that SMOTE substantially improved the prediction performance. Aditsania et al.14 used Adaptive Synthetic Sampling (ADASYN) to process the data and combined it with the BP algorithm for customer churn prediction and achieved good results.\n\nDespite its excellent performance, a single deep-learning model still has limitations. For example, RNN and LSTM are good at capturing long-term dependencies. Still, they may be inadequate in extracting local features, while CNN, although outstanding in extracting local features, has limited effectiveness in dealing with long-term dependencies. Therefore, it is difficult for a single model to handle long-term dependencies and local feature extraction.\n\nHybrid neural network models have emerged to overcome the limitations of traditional machine learning and single deep learning algorithms. Such models significantly improve the accuracy and efficiency of churn prediction by combining different types of neural networks. Hybrid neural networks show great potential in this field.\n\nZhou et al.19 proposed a hybrid neural network model (LSTM-CNN) that combines LSTM and CNN, which can capture both long-term dependencies and local features, thus significantly improving the accuracy of prediction. Hu et al.20 proposed a PRNN model that combines an RNN with an LSTM and incorporates a product operation to enhance the interaction capability between features. Khattak et al.27 further innovated based on LSTM-CNN by using BiLSTM instead of LSTM, which improves the understanding of sequence information and enhances the performance of customer churn prediction. Wu18 proposed a hybrid neural network model called HNNSAE that combines the entity embedding layer, feature extraction layer, and MLP; it significantly enhances the richness of feature representation and the ability to capture the correlation information between data using Multi-Head Self-Attention, which significantly improves the prediction performance. Wang et al.28 introduced the Attention mechanism into the LSTM model to form an LSTM-Attention model, which enables the model to focus more on key information and improves the understanding of sequential data. Experimental results show that LSTM-Attention outperforms a single LSTM model in a customer churn prediction task. Wang et al.29 proposed a FCLCNN-LSTM model, which, by combining the fully-connected layers, CNN, and LSTM, captures features from different layers and performs well in the customer churn prediction task.\n\nThese studies show that hybrid neural network models have a promising application in customer churn prediction. They can overcome the limitations of a single deep learning model, understand data features more comprehensively, and improve prediction accuracy and efficiency.\n\nResearch motivations\n\nAlthough existing models have made some progress in improving the performance of customer churn prediction, they still face several key challenges, mainly including the following three points:\n\nClass imbalance problem: In actual customer churn datasets, the ratio of churned customers to non-churned customers is usually severely imbalanced. This imbalance can cause the model to overfit the majority class (non-churned customers) and under-identify the minority class (churned customers) during training, thus affecting the overall prediction performance.\n\nLimitations of feature extraction: existing hybrid neural network models are relatively homogeneous in their customer feature extraction methods, making it difficult to adequately capture the diversity and complexity of customer features and limiting the model’s prediction accuracy.\n\nInsufficient generalization capability: most of the hybrid neural network studies have been validated only on datasets from a single industry, and the lack of evaluation of generalization performance on datasets from different domains limits the wide application of the model and its practical value.\n\nResearch contributions\n\nTo address the above challenges, this study proposes a hybrid neural network-based customer churn prediction model, Customer Churn Prediction Networks (CCP-Net), with the following key contributions:\n\nSolving the category imbalance problem: In the data preprocessing stage, this study adopts the ADASYN sampling algorithm to effectively address the category imbalance problem in the dataset. Compared with the traditional oversampling method, the synthetic samples generated by ADASYN are closer to the original data distribution. This avoids the model training bias caused by the inconsistent sample distribution, thus improving the negative impact of category imbalance on the prediction performance.\n\nEnhance feature extraction capability: By combining Multi-Head Self-Attention, BiLSTM, and CNN, CCP-Net gives full play to the advantages of each model, makes up for the deficiencies in the structural design of the existing hybrid neural networks, and significantly improves the comprehensiveness and accuracy of feature extraction. This multi-network synergistic design is of great significance to improve the accuracy of customer churn prediction.\n\nEnhanced generalization capability: through experiments on datasets from multiple industries (Telecom, Bank, Insurance, News), it is found that the CCP-Net model is not only suitable for churn prediction of Telecom, but also for churn prediction of Bank, Insurance, and News, and shows good prediction performance. This indicates that CCP-Net has a strong generalization ability and has the potential to be widely applied in different industries.\n\nProposed methodology\n\nCustomer churn prediction involves complex multimodal data, which requires models that can handle multiple feature types simultaneously. Traditional single deep learning models (CNN or LSTM) each focus on different data features, making it difficult to comprehensively handle this type of complex data. Specifically, CNN excels in capturing spatial features and is particularly suitable for processing image data, but has limited effectiveness in processing time-series data; LSTM excels in capturing long-term dependencies in time-series and can effectively process time-series data, but is relatively deficient in extracting local features. In addition, in the face of complex scenarios such as feature interaction and category imbalance, it is often difficult for a single model to achieve ideal prediction performance.\n\nTo solve the above problems, we propose the CCP-Net model. CCP-Net consists of three main modules: Multi-Head Self-Attention, BiLSTM, and CNN. The following is a detailed description of the CCP-Net model.\n\nMulti-head self-attention\n\nSince the release of the Transformer model, Multi-Head Self-Attention has received a lot of attention. Multi-Head Self-Attention allows the attention heads to learn in parallel on different subspaces, enabling each attention head to focus on different aspects of the information in the sequence and, through the computation of different weight matrices, to generate finer-grained feature representations that better capture the various dependencies in the sequence. In the task of customer churn prediction, Multi-Head Self-Attention can help the model capture the key features and patterns in customer transaction data more effectively and improve the model’s understanding and prediction of customer churn behavior. The structure of the improved Multi-Head Self-Attention mechanism model is shown in Figure 1:\n\nStructure of the improved Multi-Head Self-Attention model.\n\nThe computational steps of Multi-Head Self-Attention are as follows:\n\n(1) Projection\n\nThe input sequence X is linearly mapped to obtain the representation of multiple subspaces respectively.\n\n(2) Attention matrix computation\n\nAttention computation is performed for each projected subspace separately. The attention module of each subspace requires matrix operations, and the output of the attention matrix can be computed using Equation 1, where Q denotes the query vector, K denotes the key vector,v denotes the value vector, and \\(d_{k}\\) denotes the dimension of K. The output of each attention head can be derived using Equation 2, where \\(head_{i}\\) denotes the output of the ith attention head, and \\(W_i^QW_i^KW_i^V\\) denotes the weight of the ith attention head of the Q,K,V vector, respectively.\n\n(3) Combination\n\nAs shown in Equation 3, the outputs of each attention head are combined by a linear mapping, and Concat denotes the combination operation.\n\n(4) Output\n\nThe final output of Multi-Head Self-Attention is obtained by linear mapping the combined outputs as shown in Equation 4, \\(W^o\\) denotes the weights of the linear mapping.\n\nTo alleviate the gradient vanishing and promote the information transfer, we introduce the residual connection and Layer Normalization (LN) on Multi-Head Self-Attention, which improves the expressive ability of the model and accelerates the convergence speed, to train the model more effectively. The residual connection and Layer Normalisation are shown in Equation 5 and Equation 6 respectively:\n\n\\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation of input Y, respectively, while \\(\\gamma\\) and \\(\\beta\\) are learnable parameters.\n\nBiLSTM\n\nBiLSTM processes the input data by forward LSTM and reverse LSTM, in which LSTM has an input gate, forgetting gate, cell gate, and output gate, which can effectively learn and memorize the historical and future temporal information in the data. For example, if a customer has recently made frequent transactions with a company, the probability of churn is significantly reduced; on the contrary, the risk of churn increases. The LSTM model structure is shown in Figure 2:\n\nStructure of LSTM model.\n\n(1) Forgetting Gate\n\nThe function of the forgetting gate is to decide what information to discard from the cell gate. The input of the forgetting gate consists of the output of the hidden state in the previous time step \\(h_{t-1}\\) and the input of the current time step \\(X_{t}\\), which are multiplied with the corresponding weights \\(W_{fh}\\) and \\(W_{fx}\\), respectively, and then added with the bias of the forgetting gate \\(b_{f}\\), which is mapped to the interval [0,1] by the Sigmoid function \\(\\sigma\\) to decide the information to be discarded as shown in Equation 7 to get the output of the forgetting gate \\(f_{t}\\).\n\n(2) Input gate\n\nThe function of the input gate is to determine the information to be stored in the cell gate. The output of the previous time step \\(h_{t-1}\\) and the input of the current time step \\(X_{t}\\) will be multiplied by the corresponding weights \\(W_{ix}\\) and \\(W_{ih}\\), respectively, and then added to the input gate bias \\(b_{i}\\). These values are mapped to the [0,1] interval by the Sigmoid function \\(\\sigma\\) indicating the degree of updating the cell gate as shown in Equation 8 to obtain the output of the input gate \\(i_{t}\\).\n\n(3) Cell gate\n\nThe cell gate consists of two key steps: computing the new candidate value vector and updating the cell gate.\n\nTo compute the new candidate value vector, it is necessary to multiply the hidden state of the previous time step \\(h_{t-1}\\) and the input of the current time step \\(X_{t}\\) with the corresponding weights \\(W_{ch}\\) and \\(W_{cx}\\), respectively, and then add the bias of the cell gate candidate values \\(b_{c}\\), which are mapped by the hyperbolic tangent function tanh into the interval [-1, 1] as shown in Equation 9, to generate a new candidate value vector \\(\\tilde{C}_{t}\\). This new candidate value vector represents the candidate values to be updated to the cell gate.\n\nFor the step of updating the cell gate, the cell gate \\(C_{t-1}\\) from the previous time step is multiplied element-by-element with the forget gate \\(f_{t}\\) via \\(\\bigodot\\), discarding the information that needs to be discarded. Then the new candidate value \\(\\tilde{C}_{t}\\) is multiplied element by element with the input gate \\(i_{t}\\) via \\(\\bigodot\\) to add the information that needs to be input. As shown in Equation 10, the two are added together to get the updated cell gate \\(C_{t}\\).\n\n(4) Output gate\n\nThe output gate controls the extent to which the hidden state of the current time step is output to the external output. As shown in Equation 11, the output gate \\(O_{t}\\) is computed from the output of the previous time step \\(h_{t-1}\\) and the input of the current time step \\(X_{t}\\), which are multiplied with the corresponding weights \\(W_{oh}\\) and \\(W_{ox}\\), respectively, and then added to the bias of the output gate \\(b_{o}\\). These values are mapped to the interval [0,1] by the Sigmoid function \\(\\sigma\\) indicating which parts of the hidden state \\(h_{t}\\) of the current time step will be output to the outside of the network.\n\nAs shown in Equation 12, the hidden state \\(h_{t}\\) update is accomplished by the element-by-element product \\(\\bigodot\\) and the hyperbolic tangent function tanh. The output gate \\(O_{t}\\) controls the extent to which tanh acts on the cell gate \\(C_{t}\\) to obtain the hidden state \\(h_{t}\\) for the current time step.\n\nThe BiLSTM model structure is shown in Figure 3.\n\nStructure of BiLSTM model.\n\nThe output of BiLSTM \\(Output_{BiLSTM}\\) is a concatenation of the hidden state \\(h_{forward}\\) of the forward propagation LSTM and the hidden state \\(h_{backward}\\) of the backward propagation LSTM as shown in Equation 13:\n\nCNN\n\nCNN is a feed-forward neural network, which consists of a convolutional layer, activation layer, pooling layer, and fully connected layer, and can extract the local features of the data through different sizes of filters, effectively learning the spatial structure of the input data and convert it into high-dimensional feature representations, and help the CCP-Net model to learn more complex and advanced features from the transaction data through multiple convolution, activation, and pooling operations. The CNN model structure is shown in Figure 4.\n\nStructure of CNN model.\n\n(1) Convolutional layer\n\nThe data is fed into the convolutional layer and a convolution operation will be performed to capture the local features and generate the feature map, the convolution operation can be expressed by Equation 14. Where X is the input data, W is the convolution kernel, the step size is s, \\(Conv_{i}\\) is the ith neuron in the output of the convolution layer, \\(X_{i\\cdot s+j}\\) is the \\(i\\cdot s+j\\)th element of the input data, \\(W_{j}\\) is the jth weight of the convolution kernel and b is the bias.\n\n(2) Activation layer\n\nAfter the convolutional layer, an activation function will be used to introduce nonlinear features to enhance the expressive ability of the neural network, this model uses the Relu activation function, which can overcome the problem of gradient vanishing and make the model have a faster training speed, the Relu activation function is shown in Equation 15. \\(A_{i}\\) denotes the ith neuron in the output of the activation layer;\n\n(3) Pooling layer\n\nThe data is fed into a pooling layer that reduces the size of the feature map and the number of parameters while retaining key information; this model uses maximum pooling as shown in Equation 16. P is the size of the pooling window, and \\(P_{i}\\) denotes the ith neuron output from the pooling layer;\n\n(4) Fully connected layer\n\nThe fully connected layer spreads the previously extracted features to get the prediction result fc. As shown in Equation 17. \\(W_{fc}\\) is the weight of the fully connected layer and \\(b_{fc}\\) is the bias of the fully connected layer.\n\nCCP-Net model\n\nExisting hybrid neural network models have some shortcomings in their structural design. For example, the PRNN model mainly relies on RNN and LSTM, but RNN and LSTM are prone to gradient vanishing or gradient explosion problems when dealing with long-term dependencies, which limits their performance in customer churn prediction tasks. The HNNSAE model relies only on multi-head self-attention for feature extraction, and the single LSTM-Attention and GRU-Attention models are excellent at time series feature extraction, but are deficient in local feature extraction, resulting in limited prediction accuracy. In contrast, the LSTM-CNN, BiLSTM-CNN, and FCLCNN-LSTM models are capable of extracting both local and global features, but are still deficient in dealing with complex global dependencies, while Multi-Head Self-Attention is more advantageous in capturing complex global dependencies, which enhances the model’s feature extraction capability.\n\nStructure of CCP-Net model.\n\nTo address these limitations, this study proposes a customer churn prediction network model called CCP-Net, CCP-Net combines three types of neural networks: Multi-Head Self-Attention, BiLSTM, and CNN. The structure of CCP-Net is shown in Figure 5.\n\nMulti-Head Self-Attention Module:\n\nCCP-Net first uses the Multi-Head Self-Attention module to process customer transaction sequences. This module focuses on different parts of the information in the sequence by decomposing the input sequence into multiple attention heads that learn in parallel in different subspaces. Compared with the single-attention mechanism, Multi-Head Attention can capture the complex relationships between features and effectively extract key information and potential patterns in the transaction data by weighted summation of the outputs of individual attention heads.\n\nMulti-Head Self-Attention can capture global dependencies and process information in parallel, avoiding the information decay triggered by the step-by-step processing of traditional sequential models (e.g., LSTM and GRU). In addition, the multi-head mechanism enables the model to focus on the contextual information of different parts of the input sequence to fully capture the complex dependencies between features, especially in the task of customer churn prediction, which helps to capture global contextual information in long time series, such as customers’ historical behaviors, consumption habits, and long-term dependencies. In this way, the model can capture complex dependencies quickly, avoiding the problem of gradient vanishing or gradient explosion, resulting in a more comprehensive understanding of customer behavior patterns.\n\nBiLSTM module:\n\nThe output of the Multi-Head Self-Attention is then passed to the BiLSTM module, which, by processing the input sequences in both directions, can take into account both the information flows from the past and the future, leading to a more comprehensive understanding of the context of the sequence data. Compared to BiGRU, BiLSTM introduces three important gating mechanisms - input gates, forgetting gates, and output gates - which effectively manage the storing and forgetting of information to ensure that important contextual information is retained over long sequences. This gives BiLSTM a significant advantage in dealing with complex long-term dependencies, especially in customer churn prediction, where customer churn behavior is usually influenced by multiple factors that may be distributed at different locations in the time series.\n\nThrough the two-way propagation mechanism, BiLSTM is not only able to capture past behavioral information but also predict future changes, which comprehensively improves the model’s prediction capability. In contrast, although BiGRU is also capable of bi-directional processing, it has a relatively simplified structure and relies only on update gates and reset gates to control the flow of information, which reduces the amount of computation, but BiGRU is more limited than BiLSTM in the modeling of long-term dependencies. As a result, BiLSTM is more suitable for capturing long-term behavioral patterns in customer churn prediction and can more accurately model important features in long-term time series.\n\nCNN module:\n\nThe output of the BiLSTM module is then passed to the CNN module, which extracts local features through convolutional operations to capture short-term patterns in the transactional data. The CNN’s translation invariance enables it to identify local features efficiently. It combines with the ReLU activation function to introduce nonlinearities and reduce the feature dimensions through a maximal pooling operation to transform the local features into a higher-dimensional representation. Through multi-layer convolution operation, CNN performs layer-by-layer feature extraction on the input data and can discover important local patterns. In the task of churn prediction, local features such as short-term behavioral patterns and immediate changes in customer interactions are also critical, and CNNs help the model better understand short-term customer behavioral changes by capturing these local features, which enhances the model’s prediction capability.\n\nIn summary, by combining the advantages of Multi-Head Self-Attention, BiLSTM, and CNN, CCP-Net overcomes the deficiencies of existing models in capturing global and local dependencies and comprehensively improves the performance of customer churn prediction. Multi-Head Self-Attention provides a global perspective, helping the model to capture complex customer behavior patterns; BiLSTM strengthens long-term dependency learning in time series and effectively handles multi-dimensional information of customer churn; CNN excels in local feature extraction, enabling the model to achieve efficient learning at different feature levels. Through this comprehensive design, CCP-Net not only significantly improves the prediction accuracy, but also enhances its applicability in practical applications.\n\nExperiment\n\nThe experiment is divided into the following four steps:(1) Data acquisition, (2) Data preprocessing, (3) Building the CCP-Net model and training, and (4) Experimental analysis. The flow of the experiment is shown in Figure 6.\n\nFlow chart of the experiment.\n\nData acquisition\n\nFour customer churn datasets from the Kaggle data science competition platform are used in this study, including Telecom30, Bank31, Insurance32, and News33. These datasets are widely used in the field of customer churn prediction, facilitating the comparison of the model proposed in this paper with other models. Table 1 summarises the details of each dataset and shows that all these datasets suffer from significant category imbalance.\n\nThe Telecom dataset contains customers’ transaction frequency, service usage, and churn history.\n\nThe Bank dataset records customers’ transaction behavior, account balances, and other financial activities.\n\nThe Insurance dataset provides historical information about customers and product usage.\n\nThe News dataset includes information about users’ reading habits, language, address, and more.\n\nData preprocessing\n\nData preprocessing is a critical step to ensure model performance. Our preprocessing steps for the four datasets are as follows:\n\nIn our initial data exploration, we found a small number of missing and duplicate values in the dataset. Since these problematic data accounted for a relatively small percentage of the data, it was straightforward to remove rows that contained missing or duplicate values. For example, the ’Language’, ’weekly fee’, and ’Nielsen Prizm’ columns in the News dataset had some missing values, and deleting the corresponding rows was sufficient. In addition, entries with similar but different expressions are unified, e.g., in the Telecom dataset, ’No internet service’ and ’No phone service’ are unified as ’No’.\n\nThe dataset contains both numeric and categorical data.\n\nFor numerical data, we used standardization to adjust their mean to 0 and variance to 1 to ensure the consistency of different features on the numerical scale.\n\nFor category-based data, since the format of category-based data is string-based, it cannot be directly used for further calculations, so we need to convert the string data to numeric representation through label encoding and convert the string format of category-based data to numeric representation so that the model can recognize the relationship between different categories. For example, in the Bank dataset, the category values of ’Geography’ and ’Gender’ are encoded as 0, 1, 2, etc.\n\nTo reduce feature dimensionality and remove noise, we use correlation heatmaps to identify features with low correlation with the target variable. Heatmap is a visualization tool that demonstrates the correlation between features through shades of colors, and its core principle is based on the calculation of the correlation coefficient, which is used to measure the strength and direction of the linear relationship between two variables and takes the value in the range of [-1, 1]. Specifically, 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\n\nBy analyzing the correlation coefficients on the correlation heatmap, we can exclude feature columns with low correlation with the target variable, thus avoiding overfitting the model to unimportant features. This strategy not only reduces the training time but also enhances the generalization ability of the model. The main reason for choosing to remove features with correlation coefficients below a specific threshold is to reduce noise and improve the validity and interpretability of the model. Features with low correlation may not have a significant impact on the prediction of the target variable, and retaining them may introduce interference and lead to a decrease in model performance. By removing unimportant features, the model can focus more on features that are more relevant to the target variable, thus improving generalization on new data and reducing the risk of overfitting, as well as shortening training time.\n\nFigures 7, 8, 9, 10 show the correlation heatmaps for the datasets Telecom, Bank, Insurance, and News, respectively.\n\nHeatmap of Telecom dataset correlation.\n\nHeatmap of Bank dataset correlation.\n\nHeatmap of Insurance dataset correlation.\n\nHeatmap of News dataset correlation.\n\nTelecom dataset\n\nIn the Telecom dataset, we removed feature columns with correlation coefficients less than 0.1, including ‘customerID’, ‘Gender’, ‘PhoneService’, ‘MultipleLines’, ‘InternetService’, ‘StreamingTV’, and ‘ StreamingMovies’. These features are not considered important for the following reasons:\n\n‘customerID’: this feature is a unique identifier for each customer and does not provide meaningful correlation information when predicting churn. It is only an identifier and does not reflect the customer’s behavior or other characteristics that can be used for churn prediction.\n\n‘Gender’ and ‘PhoneService’: while gender and having a phone service not may have an impact on customer behavior in some scenarios, in this dataset these features are weakly correlated with churn, showing a low correlation that may lead to overfitting of the model or learning to noise.\n\n‘MultipleLines’, ‘InternetService’, ‘StreamingTV’, and ‘StreamingMovies’: these features are more indirectly related to whether or not a customer is churning, and the correlation is low, especially when there is no further data to support it (e.g., the customer’s specific usage). For example, whether or not a customer uses a particular service does not always reflect whether or not they will churn, and therefore these features were not considered sufficiently important in this experiment.\n\nBank dataset\n\nFor the Bank dataset, we removed feature columns with correlation coefficients less than 0.03: ‘CustomerId’, ‘Surname’, ‘Tenure’, ‘HasCrCard’, and ‘EstimatedSalary’. The reason for deleting these features is as follows:\n\n‘CustomerId’ and ‘Surname’: these two features are unique identifiers or surnames of customers and are unsuitable for predicting churn as they do not contain any key information about customer behavior or correlation to churn. As identifying information only, they have no real value for model training.\n\n‘Tenure’ and ‘EstimatedSalary’: the low correlation of these features suggests that they contribute less to churn prediction.’ Tenure’ may be correlated with customer churn, but the correlation with churn is weak in this dataset. ‘EstimatedSalary’ also has limited predictive power and may not be as meaningful as other financial or behavioral characteristics of customers.\n\nInsurance dataset\n\nIn the Insurance dataset, we removed feature columns with correlation coefficients less than 0.05: “feature_0”, “feature_2”, “feature_7”, “feature_10”, and “feature_14”. The reasons for the deletion of these features are as follows:\n\n“feature_0”, “feature_2”, “feature_7”, “feature_10”, and “feature_14”: these features had a weak correlation with the target variable (churn) in the preliminary analysis. They do not provide enough information to help distinguish whether customers are churning or not, and may simply be noise or irrelevant information in the data. Retaining these features introduces unnecessary complexity and leads to a reduction in model performance, so they were chosen to be removed.\n\nNews dataset\n\nFor the News dataset, we removed feature columns with correlation coefficients less than 0.05: ‘Ethnicity’, ‘dummy for Children’, ‘Address’, ‘City’, ‘County’, ‘Zip Code’, ‘weekly fee’, ‘Nielsen Prizm’ and ‘ Source Channel’. The reasons for removing these features are as follows:\n\n‘Ethnicity’ and ‘dummy for Children’: these two features are of low relevance and may introduce bias or sensitivity issues, particularly in social science data. They fail to provide a meaningful contribution to the prediction of attrition rates.\n\n‘Address’, ‘City’, ‘County’, and ‘Zip Code’: this geolocation information is usually not directly related to customer behavior, especially when there are no other details about the customer’s location. Whilst they may be valid in some specific scenarios, in the current dataset they are less relevant and tend to introduce noise.\n\n‘weekly fee’, ‘Nielsen Prizm’, and ‘Source Channel’: these features are weakly correlated and may not be directly related to the target variable (churn). In particular, ‘weekly fee’ and ‘Nielsen Prizm’ are not very useful in predicting churn and may simply reflect non-critical user behavior information.\n\nThrough this series of feature selection processes, we not only improve the training efficiency of the model but also enhance its generalization ability. Removing irrelevant or low-relevance features makes the model more focused on features that are more relevant to the target variable, which ultimately improves the accuracy of the prediction and reduces the risk of overfitting.\n\nIn the four datasets (Telecom, Bank, Insurance, and News) used in the experiment, the churn rates are 26. 58%, 20. 37%, 11. 70% and 19. 15%, respectively, all of which suffer from a significant category imbalance. To address this issue, this experiment uses the ADASYN technique to balance the dataset.\n\nADASYN is an improved oversampling technique, the core idea of which is to generate new synthetic samples by interpolating the feature space of a small number of class samples. Compared with the traditional SMOTE algorithm, ADASYN introduces a dynamic consideration mechanism of sample importance, which adjusts the number of synthetic samples generated according to the importance of each minority class sample in the classification. This dynamic adjustment mechanism makes the model pay more attention to those samples that have a critical impact on the classification task during the training process, effectively reducing the risk of overfitting to the majority class.\n\nADASYN pays special attention to the difficulty and criticality of minority samples when generating synthetic samples. Unlike traditional oversampling methods, ADASYN determines the importance of the minority samples based on their neighborhood density, which in turn determines the number of synthetic samples to be generated. Specifically, ADASYN calculates the neighbourhood density \\(D(x_i)\\) for each minority class sample \\(x_i\\),as shown in Equation 18:\n\n\\(|N_k(x_i)|\\) denotes the number of minority class samples adjacent to sample \\(x_i\\), and \\(|N_k(x)|\\) is the total number of neighbors of all minority class samples. Based on this density, ADASYN can generate the number of synthetic samples \\(N_{new}(x_i)\\) for each minority class sample, as shown in Equation 19.\n\nThis adjustment mechanism allows more synthetic samples to be generated for hard-to-classify samples near the decision boundary, thus helping the model to pay more attention to these ‘boundary’ samples and improve its ability to recognize a small number of classes.\n\nIn addition, by generating more synthetic samples for these critical boundary samples, ADASYN enhances the model’s performance in these hard-to-distinguish regions and improves its overall generalization. In this way, ADASYN not only balances the class distribution but also significantly improves the recognition of minority classes.\n\nAfter determining the number of generated samples, ADASYN selects the closest k neighbor samples \\(\\{x_{i_1}, x_{i_2}, \\ldots , x_{i_k}\\}\\) that are closest to each of the minority class samples \\(x_i\\). Then, synthetic samples \\(x_{new}\\) are generated by Equation 20:\n\nWhere \\(x_i\\) is one of the selected neighbors and \\(\\lambda\\) is a random number between 0 and 1 indicating that the newly generated sample is located somewhere between \\(x_i\\) and its neighbor \\(x_j\\).\n\nFigure 11 show the changes in the distribution of categories before and after the application of ADASYN, further validating the effectiveness of ADASYN in balancing the dataset.\n\nChanges in category distribution before and after treatment with ADASYN.\n\nBuilding the CCP-Net model and training\n\nIn this experiment, we choose to use Jupyter Notebook as the development environment and conduct the experiment on the Windows 10 operating system. Data preprocessing, model evaluation, and the implementation of machine learning algorithms are all based on the Scikit-learn library, while the construction and training of the CCP-Net model are based on the PyTorch framework. To ensure the training efficiency and reliability of the results, this experiment is configured with two Intel Xeon E5-2620 v4 processors, 64 GB of RAM, and four NVIDIA Tesla M40 GPUs to accelerate the model training and processing of large-scale datasets.\n\nIn the output phase of the CCP-Net model, a Sigmoid activation function is used to limit the prediction results between 0 and 1, as shown in Equation 21:\n\nWhere if the output value is less than 0.5, it means that the customer will not churn; if the output value is greater than or equal to 0.5, it means that the customer has a high probability of churn. This probability prediction provides a clear indication of the binary classification problem and facilitates subsequent decision-making.\n\nFor the task of predicting customer churn in binary classification, we choose Binary Cross Entropy Loss (BCELoss) as the loss function to quantify the difference between the model predictions and the true labels. This loss function is defined as shown in Equation 22:\n\nWhere \\(\\hat{y}\\) denotes the predicted output of the model,y denotes the true labels, and L is 0 when y is equal to 1, indicating no loss.\n\nTo improve the generalization ability of the model, a 10-fold cross-validation method was used in this study. Specifically, the dataset is divided into ten equal parts, and each time, one part is kept as the validation set, and the remaining nine parts are used as the training set. This process is repeated ten times and the average of the ten experimental results is calculated to ensure the stability and reliability of the results.\n\nDuring the training process, we use Adam optimizer, an optimization algorithm that combines momentum and adaptive learning rate to accelerate model training and improve convergence speed. The initial learning rate is set to 0.001 and is dynamically adjusted during training to maintain model stability. In addition, we introduce the Early Stopping technique to prevent model overfitting. When the validation set loss does not improve significantly within some epochs, the training is stopped early to save computational resources and improve model stability. The pseudo-code of the CCP-Net model is shown in Algorithm 1.\n\nTraining Process for CCP-Net Model.\n\nExperimental analysis\n\nEvaluation metrics\n\nIn the customer churn prediction task, the evaluation of the model performance can be achieved using a confusion matrix, which consists of four key components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). Among them:\n\nTP represents the number of samples correctly predicted as churn customers.\n\nTN represents the number of samples correctly predicted as non-churning customers.\n\nFP represents the number of samples incorrectly predicted as churn customers.\n\nFN represents the number of samples incorrectly predicted as non-churning customers.\n\nTable 2 demonstrates the structure of the confusion matrix:\n\nTo comprehensively evaluate the performance of different classification algorithms in customer churn prediction, the following four main evaluation metrics are selected in this study: Accuracy, Precision, Recall, and F1. These metrics can reflect the model’s performance in the churn prediction task from multiple perspectives.\n\nAccuracy refers to the proportion of samples correctly predicted by the model to the total samples, as shown in Equation 23. For the churn prediction task, Accuracy is used to measure the overall prediction accuracy of the model.\n\nPrecision is the proportion of actual churn customers in the sample predicted as churn by the model, as shown in Equation 24. In churn prediction, a higher Precision means that the model has a lower false alarm rate in churn prediction.\n\nRecall is the proportion of actual churn customers successfully predicted as churn customers by the model, as shown in Equation 25. In churn prediction, a higher Recall indicates that the model performs well in capturing real churn customers and has a lower underreporting rate.\n\nF1 is the reconciled average of Precision and Recall, as shown in Equation 26. F1 integrates the model’s prediction accuracy and ability to capture real churn customers, and can effectively assess the overall performance of the model.\n\nAnalysis of experimental parameters\n\nIn this experiment, a systematic grid search was conducted to analyze the key parameters of the CCP-Net model, focusing on two key parameters in Multi-Head Self-Attention: Attentional dimensions and Attentional head counts. In the evaluation process, we chose the F1 value as the main metric to evaluate the performance of the model in the customer churn prediction task under different parameter configurations. The parameter ranges for the grid search are set as follows: Attentional dimensions are 32, 64, and 128, and Attentional head counts are 4, 8, 16, 32, and 64. Figure 12 show the variation in the performance of the CCP-Net model in the four industry datasets (Telecom, Bank, Insurance, News) for different attentional dimensions and attentional head counts.\n\nF1 values for different attentional dimensions and attentional head counts.\n\nThe experimental results show that different datasets exhibit significant performance differences under attentional dimensions and attentional head counts configurations. For example, in the Telecom dataset, the model performance reaches the highest with an F1 value of 91.18% when the attentional dimensions are 64 and attentional head counts are 4, while in the Bank dataset, the attentional dimensions are 64 and attentional head counts are 8, the F1 value reaches 90.19%. For the Insurance dataset, the configuration with attentional dimensions of 128 and attentional head counts of 32 performs the best with an F1 value of 95.43%, while in the News dataset, the configuration with attentional dimensions of 128 and the configuration with attentional head counts of 16 achieved the best results with an F1 value of 94.22%.\n\nThese results show that datasets with larger sample sizes (Insurance and News) typically require larger attentional dimensions (128) and more attentional head counts (16 or 32) to capture more features and complex patterns in the data. This suggests that when the data volume increases, the model requires a higher representation capability to extract information efficiently. In contrast, for datasets with relatively small sample sizes (Telecom and Bank), smaller attentional dimensions (64) and fewer attentional head counts (4 or 8) are sufficient to satisfy the model performance requirements and help avoid overfitting.\n\nIn summary, we can see that in Multi-Head Self-Attention, the selection of attentional dimensions and attentional head counts should be adjusted according to the characteristics of the dataset, especially when the dataset has a large sample size, the use of larger attentional dimensions and more attentional head counts can significantly improve the model performance. While datasets with smaller sample sizes, smaller attentional dimensions, and attentional head counts can achieve excellent performance.\n\nBased on the results of the grid search, we determined the optimal configuration of each parameter of the CCP-Net model, as shown in Table 3. With this optimal parameter configuration, the CCP-Net model demonstrates the best performance in the customer churn prediction task, which fully demonstrates the importance of optimal parameter selection to enhance the model’s prediction capability. With this configuration, the model can effectively capture the complex patterns and features in the data, thus achieving excellent classification results on datasets from different industries.\n\nComparative experimental analysis\n\nTo validate the performance of the CCP-Net model, a variety of state-of-the-art machine learning and deep learning algorithms commonly used for customer churn prediction were selected for systematic comparison in this study, including SVM, Decision Tree, KNN, XGBoost, Random Forest, MLP, TabNet, CNN, GRU, LSTM, BiGRU, BiLSTM, Transformer, GRU-CNN, LSTM-CNN, BiGRU-CNN, BiLSTM-CNN, LSTM-Attention, and FCLCNN-LSTM. Table 4 summarises the comparative experimental results of each model, and the results show that CCP-Net performs excellently on datasets from four different industries and has obvious advantages in feature extraction and processing complex data. By combining Multi-Head Self-Attention, BiLSTM, and CNN, CCP-Net can capture global and local features efficiently, which makes it more advantageous in processing complex data. In contrast, while single models (e.g., LSTM, GRU) excel in time series data processing, they cannot adequately model global dependencies. At the same time, traditional machine learning methods are highly dependent on feature engineering and are less adaptable.\n\nThe selected machine learning models (e.g., SVM, Decision Tree, KNN, etc.) rely on complex feature engineering and hyper-parameter tuning, and thus the performance fluctuates widely on different datasets. For example, the choice of regularisation parameters and kernel function for SVM, the tree depth for the Decision Tree, and the number of neighbors K for KNN are all key factors affecting model performance. If these hyperparameters are not reasonably tuned, the model performance may be significantly degraded. For example, on the Insurance dataset, the accuracy of the Decision Tree is only 78.11%. At the same time, its Precision, Recall, and F1 values are 78.57%, 77.31%, and 77.92%, respectively, which are much lower than the CCP-Net model’s values of 95.86%, 95.87%, 95.03%, and 95.43%. As a hybrid neural network, CCP-Net can automatically learn the data representation, significantly reduce the reliance on manual feature engineering, and effectively extract complex features through its internal structure. Compared to algorithms such as KNN which are sensitive to noise and outliers, CCP-Net’s deep structure can better adapt to different data distributions, thus showing greater robustness in dealing with outliers. This design allows CCP-Net to consistently maintain high performance on multiple datasets, whereas traditional machine learning methods often perform poorly in the face of feature fluctuations.\n\nWhile integrated learning methods (such as XGBoost and Random Forest) offer improved performance compared to a single model, their complexity and computational cost subsequently increase, and the interpretability of the model is often inferior to that of a single model. CCP-Net, through its deep learning structure, provides an approach that balances performance and interpretability. In experiments on four datasets, CCP-Net generally outperforms these machine learning models on all metrics, with increases ranging from 1% to 6%. For example, in the Telecom dataset, CCP-Net’s Precision is 92.19%, while KNN’s Precision is only 83.50%; in the Bank dataset, CCP-Net’s F1 value is 90.19%, much higher than Random Forest’s 87.01%. These results demonstrate the effectiveness of CCP-Net in dealing with complex features.\n\nFor single deep learning algorithms such as MLP, TabNet, CNN, GRU, LSTM, BiGRU, BiLSTM, and Transformer, these models may not extract information comprehensively when processing data. For example, MLP has a simple structure containing only input, hidden, and output layers, and lacks a specialized feature extraction structure, leading to its poor performance on various metrics; its Accuracy of 78.42% and F1 of 78.44% on the Insurance dataset are the worst performers among the listed deep learning algorithms. Although TabNet performs well with structured datasets, it does not capture long-term dependencies in time series data as effectively as models specifically designed for time series prediction (e.g., GRU, LSTM), and therefore performs poorly with data containing time series.CNNs typically use fixed-size convolutional kernels, which limits their performance when dealing with data that has variable length or complex ability when dealing with sequence data with variable length or complex temporal dependencies. Although GRU, LSTM, BiGRU, and BiLSTM have memory capabilities when processing sequence data, they may not be able to effectively capture important information in the sequences when processing long sequence data due to the gradient vanishing or explosion problem. In contrast, CCP-Net combines Multi-Head Self-Attention, BiLSTM, and CNN, which fully utilizes the advantages of global and local feature extraction to overcome the gradient problem in long sequence processing, and therefore exhibits stronger prediction ability on diverse features. In addition, the Transformer model’s self-attention mechanism excels in capturing global dependencies but lacks the flexibility to deal with local features, whereas the design of CCP-Net takes both into account, resulting in superior performance in the face of complex time series data.\n\nIn the comparison experiments, the CCP-Net model excels in all performance metrics. For example, on the Telecom dataset, the Precision of CCP-Net is 92.19%, while the Precision of LSTM and BiLSTM is 91.17% and 91.61%, respectively. On the News dataset, the F1 value of CCP-Net is 94.22%, while the F1 values of Transformer and CNN are 90.67% and 87.74%, respectively. These results show that CCP-Net can perform effective feature extraction on diverse features through its unique architectural design, demonstrating strong generalization ability.\n\nTable 5 shows the performance comparison of GRU, LSTM, BiGRU, and BiLSTM on the Telecom dataset. Although GRU and LSTM are similar in accuracy, LSTM has a slight advantage in Precision and Recall, with an F1-Score of 88.06%, which is 0.37% higher than GRU. This difference indicates that LSTM is better at capturing complex dependencies in long time series; especially when dealing with long-time dependencies, it can effectively avoid the problem of gradient disappearance, thus enhancing the overall performance of the model.\n\nIn the comparison between BiGRU and BiLSTM, the F1-Score of BiLSTM (88.31%) is slightly higher than that of BiGRU (88.13%). This difference is mainly attributed to the gating mechanism of LSTM, which gives BiLSTM an advantage in modeling long-term dependencies. Although BiGRU is better in terms of computational efficiency, BiLSTM is more suitable for processing complex time series data, especially in capturing long-term dependencies.\n\nAfter combining with CNN, the F1-Score of BiLSTM-CNN improves to 90.84%, which is higher than the 90.53% of BiGRU-CNN. This result shows that the CNN layer can effectively enhance the local feature extraction and pattern capture of BiLSTM, and further improve the overall performance of the model, especially in the high-dimensional feature space.\n\nBy integrating Multi-Head Self-Attention, BiLSTM, and CNN, CCP-Net achieves an F1-Score of 91.18% on the Telecom dataset, outperforming Attention-BiGRU-CNN (90.86%). The Multi-Head Self-Attention module captures global dependencies, BiLSTM handles local dependencies, and the CNN layer further refines features. The combination of the three gives CCP-Net a significant advantage in handling complex features and modeling long and short-term dependencies.\n\nAlthough the training time of CCP-Net (32.94 seconds) is slightly longer compared to Attention-BiGRU-CNN (30.35 seconds), the increase in this time mainly stems from the BiLSTM module, which significantly improves the expressive power and predictive accuracy of the model. Thus, despite being slightly less computationally efficient, the performance improvement of CCP-Net justifies these additional overheads and shows its unique advantages in complex feature modeling and global dependency capture.\n\nAlthough LSTM-CNN, BiLSTM-CNN, LSTM-Attention, and FCLCNN-LSTM have improved performance compared to a single neural network, CCP-Net combines Multi-Head Self-Attention, BiLSTM, and CNNs to show stronger learning capabilities and obtain better performance results. This combination not only enhances the feature extraction ability of the model but also improves the generalization ability of the model to better cope with data with different characteristics. In Precision, a performance metric, the CCP-Net model generally outperforms other models, with an increase of between 1% and 3%. For example, on the Telecom dataset, the F1 value of CCP-Net is 91.18%, while the F1 values of LSTM-CNN and BiLSTM-CNN are 88.79% and 90.84%, respectively; on the News dataset, the F1 value of CCP-Net is 94.22%, while the F1 values of LSTM-Attention and FCLCNN-LSTM have F1 values of 91.81% and 92.89%, respectively. These results show the obvious advantage of CCP-Net in model prediction performance, especially when dealing with complex features, its multi-level feature fusion capability makes it outperform traditional hybrid models.\n\nError bars\n\nTo analyze the stability and robustness of different hybrid neural network models in more depth, we compare the F1 value performance of BiLSTM-CNN, LSTM-Attention, FCLCNN-LSTM, and CCP-Net on the four datasets of Telecom, Bank, Insurance, and News, and use the error bar to which is visualized (shown in Figure 13). From the results, it can be seen that CCP-Net exhibits the smallest performance fluctuation on all the test datasets, indicating that it has more stability. In particular, in the Bank dataset, the confidence interval of CCP-Net is \\(90.17 \\pm 0.31\\), which is significantly smaller than that of other models, further demonstrating its superior stability and robustness. In contrast, BiLSTM-CNN has an F1 value of \\(88.10\\pm 0.36\\), LSTM-Attention \\(86.75\\pm 0.35\\), and FCLCNN-LSTM \\(89.08\\pm 0.33\\), and the performance of these models fluctuates more on the Bank dataset, with a higher width of confidence intervals, suggesting that their performances are relatively unstable, and they may be more stable in different customer churn prediction tasks with greater uncertainty. It can be seen that CCP-Net not only performs well in performance but also demonstrates stronger adaptability to complex tasks through smaller volatility and narrower confidence intervals, which is of great significance for customer churn prediction in practical applications.\n\nT-tests\n\nIn machine learning and statistical analyses, it is critical to assess the significance of differences in performance between models. The T-test is a commonly used statistical method for comparing the difference between two sample means and determining whether that difference is statistically significant. The T-test enables us to understand whether a model’s performance on a particular task is by chance or reflects an inherent difference in its performance. To further validate the performance improvement of CCP-Net over other hybrid neural network models, we conducted an independent samples T-test to assess the significance of the differences in the F1 values of the models across the datasets. The results of the analysis are shown in Table 6: It can be seen through Table 6 that CCP-Net exhibits a trend of significantly outperforming other hybrid neural network models on all datasets. This indicates that the design and implementation of CCP-Net can effectively enhance the F1 value and thus improve the prediction performance. On the Telecom, Bank, Insurance, and News datasets, the P-values of CCP-Net with other hybrid neural network models (e.g., BiLSTM-CNN, LSTM-Attention, and FCLCNN-LSTM) are all less than 0.05, which demonstrates its significant advantage over other models. These results not only demonstrate the superiority of CCP-Net but also provide a reliable statistical basis for its wide use in practical applications. With the help of a T-test, we confirm the consistency and performance improvement of CCP-Net on multiple datasets, which further proves its effectiveness and reliability in complex tasks such as customer churn prediction.\n\nCCP-Net significantly outperforms other models in various performance metrics and demonstrates higher stability and consistency in prediction performance on different industry datasets, reflecting its excellent generalization ability. This generalization capability enables CCP-Net to flexibly adapt to diverse data characteristics and make full use of the synergy between Multi-Head Self-Attention, BiLSTM, and CNN, thus maintaining high prediction accuracy when dealing with complex customer behavior patterns.\n\nF1 Scores with Confidence Intervals across Different Models and Datasets.\n\nThe design of CCP-Net takes into account the characteristics of different industries to ensure its broad applicability in multiple domains. For example, in the telecom industry (e.g., Telecom), the global dependency learning capability of the Multi-Head Self-Attention module significantly enhances the capture of complex customer behavioral patterns. CCP-Net achieves an Accuracy of 91.17% on the Telecom dataset, significantly higher than the BiLSTM-CNN’s 90.91% and LSTM-CNN’s 89.51%. This enables CCP-Net to handle large amounts of time-series data and identify potential churn risks promptly. In the financial industry (e.g., Bank and Insurance), the capability of the BiLSTM module in modeling long-term dependencies significantly improves the understanding of the changing dynamics of customer churn. In the Bank dataset, CCP-Net achieves an Accuracy of 89.68%, which is significantly improved compared to BiLSTM’s 87.23%; in the Insurance dataset, CCP-Net’s F1 value of 95.43% outperforms the other algorithms, especially XGBoost’s 86.12% and Random Forest of 85.73%, further validating its effectiveness in capturing customer transaction history and behavioral patterns. In addition, CCP-Net’s feature extraction capability lays the foundation for its application in the media industry (e.g. News), helping companies to deeply analyze users’ reading habits and preferences. In the News dataset, CCP-Net’s Precision reaches 95.12%, much higher than CNN’s 86.78% and BiLSTM’s 91.62%, which enables enterprises to formulate customer retention strategies more effectively.\n\nIn the error bar analysis, CCP-Net shows the smallest performance fluctuation on all test datasets, indicating that it has stronger stability. In particular, in the Bank dataset, the confidence interval of CCP-Net is \\(90.17 \\pm 0.31\\), which is significantly lower than that of other models, which further demonstrates its superior stability and robustness. In contrast, other models such as BiLSTM-CNN and LSTM-Attention have wider confidence intervals on this dataset, suggesting that their performance is relatively unstable, which may lead to greater uncertainty in practical applications.\n\nTo further validate the performance improvement of CCP-Net over other hybrid neural network models, we conducted an independent sample t-test. The results show that in the Telecom, Bank, Insurance, and News datasets, the P-values between CCP-Net and BiLSTM-CNN, LSTM-Attention, and FCLCNN-LSTM are all less than 0.05, indicating significant advantages. For example, in the Bank dataset, CCP-Net vs BiLSTM-CNN has a T-statistic of 37.80 and a P-value of 1.47e-42, showing a significant performance difference. These results provide a solid statistical basis for the effectiveness of CCP-Net in customer churn prediction.\n\nIn summary, by comparing with other models, CCP-Net performs well on datasets from different domains, fully proving its effectiveness and reliability in dealing with complex customer churn prediction problems. Overall, CCP-Net not only performs well in the task of customer churn prediction but also it’s flexible structure and powerful feature learning capability provide it with a wide range of application potentials in different industries, thus providing effective customer relationship management solutions for enterprises.\n\nAblation experiment analysis\n\nThis section provides an in-depth analysis of the importance of each module in the task of customer churn prediction and its impact on model performance through ablation experiments on each module of the CCP-Net model. The results of the ablation experiments are shown in Table 7.\n\nThe Multi-Head Self-Attention module plays a crucial role in capturing global dependencies. When this module is removed (the BiLSTM and CNN modules are retained), the overall performance of the model decreases significantly, especially in the extraction of global features. This is specifically shown in:\n\nIn the Telecom dataset, after the removal of multi-head self-attention, accuracy decreased from 91. 17% to 90. 91%, precision decreased from 92. 19% to 92. 09%, recall decreased from 90. 24% to 89. 66%, F1 value decreased from 91. 18% to 90. 84%.\n\nIn the Insurance dataset, removing Multi-Head Self-Attenti reduced Accuracy from 95.86% to 93.96%, Precision from 95.87% to 94.60%, Recall from 95.03% to 93.00%, and F1 value from 95.43% to 93.77%.\n\nThese changes suggest that Multi-Head Self-Attention is essential for learning global dependencies, although BiLSTM and CNN can effectively capture time series dependencies and local features. Especially in the Insurance dataset, the module significantly improves the prediction accuracy and enhances the model’s ability to capture complex patterns.\n\nThe BiLSTM module performs well in time series feature extraction. When the BiLSTM module is removed (the Multi-Head Self-Attention and CNN modules are retained), the model performance decreases significantly, especially in the reduction of Recall and F1 values. The specific performance is shown as follows:\n\nIn the Telecom dataset, after removing BiLSTM, Recall decreases from 90.24% to 86.20% and the F1 value decreases from 91.18% to 88.93%.\n\nIn the Bank dataset, after removing BiLSTM, Recall decreased from 88.41% to 82.93% and the F1 value decreased from 90.19% to 86.03%.\n\nThese results show that BiLSTM plays a key role in capturing long-term dependencies of customer behavior, and is particularly good at capturing temporal trends, effectively improving the classification ability of the model.\n\nThe importance of CNN modules in local feature extraction cannot be ignored. The model performance is also significantly affected when removing the CNN module (retaining the Multi-Head Self-Attention and BiLSTM modules). Specifically, it is shown as follows:\n\nIn the Insurance dataset, after removing CNN, Accuracy decreases from 95.86% to 94.44%, Precision decreases from 95.87% to 94.64%, while Recall and F1 values show a corresponding decrease.\n\nIn the News dataset, after removing the CNN, the Recall of the model decreased from 93.24% to 91.68% and the F1 value decreased from 94.22% to 93.08%.\n\nThese results show that the CNN module performs particularly well in dealing with localized and nonlinear features, and can significantly improve the overall performance of the model, especially when faced with complex and diverse datasets.\n\nThe results of the ablation experiments of the CCP-Net model on different industry datasets show that the complementary functionality of its modules improves the generalization ability of the model. For example, the performance degradation caused by the removal of Multi-Head Self-Attention is particularly noticeable in the Telecom and Insurance datasets, suggesting that this module is indispensable in dealing with industries with complex customer behavior patterns. Meanwhile, BiLSTM is more effective in handling time-series data, which is particularly suitable for the financial sector where long-term dependencies need to be captured. This flexible structural design enables CCP-Net to be widely applied to customer churn prediction tasks in different fields, providing a good basis for its promotion in various industries.\n\nImpact of ADASYN on the performance of the CCP-Net model\n\nTo assess the potential impact of ADASYN on the performance of the CCP-Net model, this experiment analyses the performance changes of the CCP-Net model before and after applying ADASYN. The experimental results are shown in Figure 14.\n\nChanges in model performance before and after processing with ADASYN.\n\nAfter processing with ADASYN, all performance metrics of the CCP-Net model are significantly improved. For example, in the Telecom dataset, the accuracy of the CCP-Net model is 87.64% using data not processed by ADASYN, and the accuracy of the CCP-Net model improves from 87.64% to 91.17% using data processed by ADASYN; meanwhile, the precision improves from 78. 52% to 92. 19%, the recall improves from 72. 74% to 90. 24%, and F1 improves from 75.49% to 91.18%. Similar trends were observed in other datasets; for example, in the Insurance dataset, accuracy improved from 89. 46% to 95. 86%, precision from 84. 49% to 95. 87%, recall from 82. 79% to 95. 03% and F1 from 83.72% to 95.43%.\n\nThis result shows that applying ADASYN as an effective category balancing processing method to the CCP-Net model significantly enhances its overall performance in the customer churn prediction task. This not only demonstrates the effectiveness of ADASYN in dealing with the category imbalance problem but also demonstrates its potential to enhance the model generalization ability and improve the various metrics of the classification task.\n\nAlgorithm complexity analysis\n\nIn the task of customer churn prediction, the computational complexity of a model directly affects its training and prediction efficiency, which in turn affects its feasibility and applicability in practical applications. Model complexity not only affects the execution speed but also determines the resource requirements, such as memory and computational power. To better understand the advantages and limitations of CCP-Net, this paper compares it with several other hybrid neural network models (LSTM-CNN, BiLSTM-CNN, LSTM-Attention, FCLCNN-LSTM), analyses its temporal and spatial complexity, and explores the trade-offs between complexity and performance. The specific meanings of the symbols are shown in Table 8:\n\nCCP-Net consists of three modules: Multi-Head Self-Attention, BiLSTM, and CNN. The time complexity for each attention head in Multi-Head Self-Attention to compute the linear transformation of the input features and perform the weighted summation is \\(O(n \\cdot d^2 \\cdot h)\\), where h is the number of attention heads. The computational complexity of BiLSTM in capturing long-term dependencies in the time series is \\(O(n \\cdot h^2 \\cdot l)\\), where l is the length of the sequence. The computational complexity of CNN in performing convolutional operations on the input sequence is: \\(O(k \\cdot n \\cdot c)\\), where k is the filter size, and c is the number of channels. Thus, the overall time complexity of CCP-Net is \\(O(n \\cdot d^2 \\cdot h + n \\cdot h^2 \\cdot l + k \\cdot n \\cdot c)\\).\n\nIn contrast, the time complexity of LSTM-CNN, BiLSTM-CNN, and FCLCNN-LSTM is: \\(O(n \\cdot h^2 + k \\cdot n \\cdot c)\\), and the time complexity of LSTM-Attention is: \\(O(n \\cdot h^2 + n \\cdot d^2)\\).\n\nFor CCP-Net, Multi-Head Self-Attention has a space complexity of \\(O(n \\cdot d)\\), which is used to store the attention weights and intermediate variables. BiLSTM has a space complexity of \\(O(n \\cdot h \\cdot l)\\), which is used to store the intermediate variables of the hidden state and the gating unit. CNN has a space complexity of \\(O(n \\cdot c)\\), which is used to store the intermediate results of the convolution operation. Therefore, the overall space complexity of CCP-Net is \\(O(n \\cdot d + n \\cdot h \\cdot l + n \\cdot c)\\).\n\nIn contrast, the space complexity of LSTM-CNN, BiLSTM-CNN, and FCLCNN-LSTM is \\(O(n \\cdot h + n \\cdot c)\\), and the space complexity of LSTM-Attention is \\(O(n \\cdot h + n \\cdot d)\\).\n\nTable 9 shows the computation time and GPU memory usage of different models when running 100 epochs on the Telecom dataset. From the data in the table, it can be seen that there is a significant difference in the time overhead and GPU memory usage of each model during training, which is closely related to the complexity and structural characteristics of the models.\n\nTime overhead\n\nThe training time of LSTM-CNN and BiLSTM-CNN is 18.72 seconds and 18.85 seconds respectively, which is not much different from each other, reflecting the lower computational requirements of these models, which are suitable for tasks with high response time requirements.\n\nThe training time of LSTM-Attention is significantly longer at 27.10 seconds, which is closely related to the introduction of the Attention mechanism, which requires more computational resources to capture the global dependency information, which increases the computation time of the model.\n\nThe training time of FCLCNN-LSTM is 19.85 seconds, which is slightly higher than that of LSTM-CNN and BiLSTM-CNN, but still within a reasonable range, indicating that the model is also more efficient.\n\nThe training time of CCP-Net is 32.94 seconds, which is the longest among all models. Due to its combination of the Multi-Head Self-Attention, BiLSTM, and CNN modules, the computational requirements are significantly higher. In particular, the introduction of the Attention module and the BiLSTM layer allows the model to handle more complex sequence dependencies and feature interactions, which improves the model’s predictive ability, but at the expense of computational efficiency.\n\nGPU memory usage\n\nLSTM-CNN and BiLSTM-CNN show relatively low memory requirements with 137.31 MB and 148.49 MB of GPU memory usage, respectively. This is due to their relatively simple structure, which does not introduce complex global dependency modeling modules (e.g., Multi-Head Self-Attention), and thus is more efficient in terms of memory usage.\n\nThe GPU memory usage of LSTM-Attention is 152.66 MB, which is slightly higher than that of LSTM-CNN and BiLSTM-CNN, which is mainly because the Multi-Head Self-Attention mechanism requires more memory to store the attention weights and intermediate computation results during the training process.\n\nThe memory usage of FCLCNN-LSTM is 142.25 MB, which is slightly higher than that of LSTM-CNN and BiLSTM-CNN, but still lighter compared to LSTM-Attention.\n\nCCP-Net has the highest GPU memory usage of 168.06 MB, which is closely related to its complex architecture (containing Multi-Head Self-Attention, BiLSTM, and CNN). The Multi-Head Self-Attention module in particular has a significant increase in overhead in terms of memory requirements, resulting in a relatively high memory usage for CCP-Net. Net’s relatively high memory usage.\n\nIn summary, CCP-Net is higher than other hybrid neural network models in both time and space complexity, especially when dealing with complex behavioral patterns, and its ability to capture multi-dimensional features significantly improves prediction accuracy. However, the higher computational resource requirements and training time may cause some limitations in scenarios with high real-time requirements or limited resources. In contrast, models such as LSTM-CNN, BiLSTM-CNN, LSTM-Attention and FCLCNN-LSTM can complete training and prediction in a shorter time due to lower time and space complexity, which is suitable for tasks that require fast response or operate in resource-constrained environments; however, these models are not as good as CCP-LSTM in capturing the global dependency information and handling complex feature interactions are not as good as CCP-Net.\n\nIn practice, the selection of an appropriate model should take into account the specific application requirements. If the task emphasizes high performance and complex data processing capability, CCP-Net is a better choice; if it focuses more on real-time performance and resource efficiency, lightweight models (LSTM-CNN or FCLCNN-LSTM) are more suitable. Meanwhile, BiLSTM-CNN and LSTM-Attention can also be used as a compromise between performance and efficiency for scenarios with high requirements for the extraction of temporal features.\n\nConclusions and future work\n\nIn this paper, we propose a customer churn prediction model, CCP-Net, which integrates Multi-Head Self-Attention, BiLSTM, and CNN. We aim to solve the customer churn problem effectively. To verify the validity of the model, we conducted systematic data preprocessing on datasets from four different industries and implemented ten-fold cross-validation to ensure the reliability of the evaluation results.\n\nAfter analyzing the experimental parameters, we determine the optimal configuration of each parameter in the CCP-Net model. Through comparative experiments, we can see that CCP-Net outperforms a variety of common deep learning algorithms and hybrid neural networks, demonstrating significant superiority and strong generalization ability. In addition, ablation experiments further validate the architectural soundness of CCP-Net, showing that the combination of Multi-Head Self-Attention, BiLSTM, and CNN endows the model with excellent prediction performance. We also explore the application of ADASYN technology in improving the predictive performance of the model, and the results show that all the performance metrics of CCP-Net are significantly improved after training with ADASYN-treated data, which fully demonstrates the effectiveness of ADASYN in customer churn prediction.\n\nAlthough CCP-Net performs well in prediction accuracy, its high time and space complexity limits its feasibility in real-time applications. Therefore, we plan to improve the model in future research, working to reduce time and space complexity while maintaining prediction accuracy. Future work will explore more advanced data processing techniques, model optimization methods, and innovative algorithms. We expect that the results of these studies will help organizations to deeply understand and manage their customer relationships, and thus achieve sustainable business development.\n\nData availability\n\nUnderlying data supporting the results can be provided by sending a request to the corresponding author.\n\nCode availability\n\nThe code used for this research is publicly available at https://github.com/segujn/CCP-Net.git.\n\nReferences\n\nXiahou, X. & Harada, Y. B2c e-commerce customer churn prediction based on k-means and svm. J. Theor. Appl. Electron. Commer. Res. 17, 458–475 (2022).\n\nArticle\n  MATH\n  Google Scholar\n\nEn Xia, G. & Dong Jin, W. Model of customer churn prediction on support vector machine. Systems Engineering - Theory & Practice 28, 71–77 (2008).\n\nArticle\n  MATH\n  Google Scholar\n\nKarimi, N., Dash, A., Rautaray, S. S. & Pandey, M. Customer profiling and retention using recommendation system and factor identification to predict customer churn in telecom industry. Machine Learning: Theoretical Foundations and Practical Applications 155–172 (2021).\n\nMishra, A. & Reddy, U. S. A comparative study of customer churn prediction in telecom industry using ensemble based classifiers. 2017 International Conference on Inventive Computing and Informatics (ICICI) 721–725 (2017).\n\nKim, S., Shin, K.-s. & Park, K. An application of support vector machines for customer churn analysis: Credit card case. In International Conference on Natural Computation, 636–647 (Springer, 2005).\n\nNV, M. K., KK, B. K. & Mudhol, A. C. Machine learning based prediction of customer churning in banking sector. In 2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS), 474–481 (IEEE, 2022).\n\nKiguchi, M., Saeed, W. & Medi, I. Churn prediction in digital game-based learning using data mining techniques: Logistic regression, decision tree, and random forest. Appl. Soft Comput. 118, 108491 (2022).\n\nArticle\n  MATH\n  Google Scholar\n\nAl-Najjar, D., Al-Rousan, N. & Al-Najjar, H. M. Machine learning to develop credit card customer churn prediction. J. Theor. Appl. Electron. Commer. Res. 17, 1529–1542 (2022).\n\nArticle\n  MATH\n  Google Scholar\n\nAhmad, A. K., Jafar, A. & Aljoumaa, K. Customer churn prediction in telecom using machine learning in big data platform. Journal of Big Data 6 (2019).\n\nLalwani, P., Mishra, M. K., Chadha, J. S. & Sethi, P. Customer churn prediction system: a machine learning approach. Computing 104, 271–294 (2021).\n\nArticle\n  Google Scholar\n\nDhanawade, A., Mahapatra, B. & Bhatt, A. A smote-based churn prediction system using machine learning techniques. 2023 1st DMIHER International Conference on Artificial Intelligence in Education and Industry 4.0 (IDICAIEI) 1, 1–7 (2023).\n\nSikri, A., Jameel, R., Idrees, S. M. & Kaur, H. Enhancing customer retention in telecom industry with machine learning driven churn prediction. Scientific Reports 14, 13097 (2024).\n\nArticle\n  ADS\n  CAS\n  PubMed\n  PubMed Central\n  Google Scholar\n\nHe, C. & Ding, C. H. A novel classification algorithm for customer churn prediction based on hybrid ensemble-fusion model. Scientific Reports 14, 20179 (2024).\n\nArticle\n  CAS\n  PubMed\n  PubMed Central\n  MATH\n  Google Scholar\n\nAditsania, A., Adiwijaya & Saonard, A. L. Handling imbalanced data in churn prediction using adasyn and backpropagation algorithm. 2017 3rd International Conference on Science in Information Technology (ICSITech) 533–536 (2017).\n\nSaha, L., Tripathy, H. K., Gaber, T., El-Gohary, H. & El-kenawy, E.-S.M. Deep churn prediction method for telecommunication industry. Sustainability 15, 4543 (2023).\n\nArticle\n  MATH\n  Google Scholar\n\nTsai, C.-F. & Lu, Y.-H. Customer churn prediction by hybrid neural networks. Expert Syst. Appl. 36, 12547–12553 (2009).\n\nArticle\n  MATH\n  Google Scholar\n\nTang, Q., Xia, G., Zhang, X. & Long, F. A customer churn prediction model based on xgboost and mlp. In 2020 International Conference on Computer Engineering and Application (ICCEA), 608–612 (IEEE, 2020).\n\nWu, H. A high-performance customer churn prediction system based on self-attention. ArXiv abs/2206.01523 (2022).\n\nZhou, J., Yan, J., Yang, L., Wang, M. & Xia, P. Customer churn prediction model based on lstm and cnn in music streaming. DEStech Transactions on Engineering and Technology Research (2019).\n\nHu, J. et al. prnn: A recurrent neural network based approach for customer churn prediction in telecommunication sector. 2018 IEEE International Conference on Big Data (Big Data) 4081–4085 (2018).\n\nZhang, L. & Wei, Q. Personalized and contextualized data analysis for e-commerce customer retention improvement with bi-lstm churn prediction. IEEE Transactions on Consumer Electronics (2024).\n\nArik, S. Ö. & Pfister, T. Tabnet: Attentive interpretable tabular learning. ArXiv abs/1908.07442 (2019).\n\nVaswani, A. Attention is all you need. Advances in Neural Information Processing Systems (2017).\n\nKhine, S. T. & Myo, W. W. Mining customer churns for banking industry using k-means and multi-layer perceptron. In 2023 IEEE Conference on Computer Applications (ICCA), 220–225 (IEEE, 2023).\n\nVenkatesh, S. & Jeyakarthic, M. Artificial fish swarm algorithm-based multilayer perceptron model for customer churn prediction in iot with cloud environment. International Journal of Business Information Systems 44, 442–465 (2023).\n\nArticle\n  MATH\n  Google Scholar\n\nLatheef, J. & Vineetha, S. G. Lstm model to predict customer churn in banking sector with smote data preprocessing. 2021 2nd International Conference on Advances in Computing, Communication, Embedded and Secure Systems (ACCESS) 86–90 (2021).\n\nKhattak, A. M. et al. Customer churn prediction using composite deep learning technique. Scientific Reports 13 (2023).\n\nWang, Y., Zheng, S., Liu, G. & Li, J.-J. Research on bank customer churn model based on attention network. 2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA) 346–350 (2023).\n\nWang, C., Rao, C., Hu, F., Xiao, X. & Goh, M. Risk assessment of customer churn in telco using fclcnn-lstm model. Expert Systems with Applications 248, 123352 (2024).\n\nArticle\n  Google Scholar\n\nZhuang, S. Telecom customer churn prediction datasets. kaggle: https://www.kaggle.com/datasets/shilongzhuang/telecom-customer-churn-by-maven-analytics (2019).\n\nDhakad, S. Bank customer churn prediction datasets. kaggle: https://www.kaggle.com/datasets/shantanudhakadd/bank-customer-churn-prediction (2022).\n\nKumar, V. Insurance churn prediction: Weekend hackathon. kaggle: https://www.kaggle.com/datasets/k123vinod/insurance-churn-prediction-weekend-hackathon (2020).\n\nAndieminogue. Newspaper churn. kaggle: https://www.kaggle.com/datasets/andieminogue/newspaper-churn (2018).\n\nDownload references\n\nAuthor information\n\nAuthors and Affiliations\n\nCollege of Computer Science and Engineering, Guangxi Normal University, Guilin, 541000, China\n\nXinyu Liu, Guoen Xia, Xianquan Zhang & Chunqiang Yu\n\nCollege of Business Administration, Guangxi University, Nanning, 530000, China\n\nGuoen Xia\n\nCollege of Business Administration, Guangxi University of Finance and Economics, Nanning, 530000, China\n\nWenbin Ma\n\nContributions\n\nAll authors contributed greatly to this paper, Xinyu Liu designed the experiments and wrote the main manuscript, Guoen Xia, Xianquan Zhang, Wenbin Ma, and Chunqiang Yu suggested revisions to the paper and touched up the language. All authors reviewed the manuscript.\n\nCorresponding author\n\nCorrespondence to Xianquan Zhang.\n\nEthics declarations\n\nCompeting interests\n\nThe authors declare no competing interests.\n\nAdditional information\n\nPublisher’s note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nRights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nLiu, X., Xia, G., Zhang, X. et al. Customer churn prediction model based on hybrid neural networks. Sci Rep 14, 30707 (2024). https://doi.org/10.1038/s41598-024-79603-9\n\nDownload citation\n\nReceived\n04 June 2024\n\nAccepted\n11 November 2024\n\nPublished\n28 December 2024\n\nDOI\nhttps://doi.org/10.1038/s41598-024-79603-9\n\nShare this article\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nKeywords\n\nSubjects\n\nScientific Reports (Sci Rep)\n\nISSN 2045-2322 (online)\n\nAbout Nature Portfolio\n\nDiscover content\n\nPublishing policies\n\nAuthor & Researcher services\n\nLibraries & institutions\n\nAdvertising & partnerships\n\nProfessional development\n\nRegional websites\n\n© 2025 Springer Nature Limited\n\nCustomer Churn Prediction Model Using Deep Learning : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nCustomer Churn Prediction Model Using Deep Learning\n\nAccess provided by American University of Beirut\n\nPart of the book series: Algorithms for Intelligent Systems ((AIS))\n\nIncluded in the following conference series:\n\n191 Accesses\n\nAbstract\n\nRetaining existing customers is a huge challenge for businesses. As customer retention can improve profit margins. Predicting the churn can help with that. Customer churn prediction is a data-driven approach to help companies reduce customer churn and increase customer retention. Machine learning algorithms are applied to analyze past customer data in order to find trends and behaviors which indicate to a high risk of churn. These can include factors such as reduced engagement, complaints, and service usage patterns. By proactively identifying and addressing these issues, companies can retain more customers, reduce churn, and increase revenue. With the emergence of deep learning paradigms, it has been observed that algorithms give a new perspective to this business problem. Conventional algorithms have been utilized to predict churn and then develop a number of client retention strategies. This paper involves data collection, data exploration, data visualization, and creation of a churn prediction model that can be integrated into a company's existing customer retention strategy. The churn prediction model works on customer information, their usage of the service, and contextual features. The deciding factors along with the probability of churn are determined. churn factors are also demonstrated for the companies to take action.\n\nAccess provided by American University of Beirut. Download conference paper PDF\n\nSimilar content being viewed by others\n\nCustomer Churn Prediction Using Deep Learning\n\nCustomer churn prediction using composite deep learning technique\n\nPredict customer churn using combination deep learning networks model\n\nKeywords\n\n1 Introduction\n\nChurn is described as the customer who transfers from one provider of telephone services to another [1]. To be precise, loyal customer relationship results in cost cutting. The price of attracting a new client is substantially lower against the price of serving an existing one. Customers create revenue for many different types of businesses. Each year that a customer stays with a firm, the earnings rise. In the financial services, a 5% improvement in client retention results for a profit rise of almost 25%. Returning clients tend to gradually purchase more from a corporation. As they do, your serving operations expenses decline. Additionally, regular customers recommend people to the business. And they frequently pay a higher price to carry on doing business rather than going to a rival, somebody they are not familiar with or comfortable. Typically following types of data are used in customer churn prediction model.\n\nCustomer demographic data: This includes information such as age, gender, income level, education, marital status, which can provide insights into different customer segments and their churn patterns.\n\nBilling and payment data: This includes details about customers' billing and payment history, such as payment amount, payment frequency, payment method, outstanding balance. These data points can indicate customers' financial stability and their likelihood to churn.\n\nUsage and interaction data: This includes data about customers' usage patterns, such as frequency and duration of product usage, feature or service utilization, interactions with customer support, website/app logins. This data helps understand customers' engagement with the product or service and identify early indicators of potential churn. A data-driven approach to predicting customer churn differs from traditional methods in several ways:\n\nUse of extensive data: A data-driven approach relies on large amounts of data are gathered from several sources, including consumer demographics, purchase history, usage trends, customer interactions, and feedback. Conventional techniques could rely on scant or manually gathered data.\n\nAdvanced analytics techniques: Data-driven approaches use cutting-edge analytics tools like machine learning, predictive modeling, and data mining to examine data trends and find insights that conventional methods might miss. Traditional approaches could use qualitative analysis or straightforward rule-based systems.\n\nAutomated prediction process: Data-driven approaches typically involve automated processes that allow for faster and more efficient prediction. Traditional approaches could include manual evaluation and interpretation, which can be time-consuming and subject to human biases.\n\nReal-time insights: Data-driven approaches can provide real-time insights into customer churn by continuously analyzing and updating the data. Traditional methods may provide retrospective analysis but lack the ability to react to changes in customer behavior promptly.\n\nDeep learning to churn prediction model in various ways:\n\nFeature Learning: Deep learning models have the ability to automatically learn relevant and intricate patterns from raw data, unlike traditional algorithms that rely on manual feature engineering. This allows deep learning models to capture more complex relationships and dependencies in the data, which can be essential for accurate churn prediction.\n\nNon-linear Relationships: Traditional algorithms often assume linear relationships between features and the target variable. However, churn prediction is a complex problem with non-linear relationships. Deep learning models, with their multi-layer architectures and activation functions, can model these non-linear relationships effectively, providing more accurate predictions.\n\nTransfer Learning: Deep learning models can leverage transfer learning, where a pre-trained model, trained on a large dataset is fine-tuned on a smaller churn prediction dataset. This helps the model to generalize better and achieve better accuracy with limited training data, which can be especially useful in scenarios where churn data is scarce.\n\nHandling Big Data: Deep learning is well-suited for handling large volumes of data, which is often the case in churn prediction scenarios. Deep learning models can effectively process and learn from this big data, enabling them to capture more subtle patterns and make more accurate predictions. A number of things, such as exorbitant costs, subpar customer support, poor product quality, or the emergence of a more suitable substitute on the market, might result in customer churn. Churn can also be influenced by factors outside of a company's control, such as changes in economic conditions or shifts in consumer preferences [2]. Customer churn prediction is an essential task for any company that provides subscription-based services or products. It involves predicting which customers are likely to stop using a company’s services or products and taking proactive measures to retain those customers before they defect to a competitor. This can include offering incentives, personalized communication, and improving the overall customer experience [3].\n\n1.1 Customer Churn Prediction\n\nToday the world is moving at a fast pace with the help of the Internet. Internet users have grown by almost 103% in the last 10 years. There is a compelling case for the rapid growth of GPRS-based medium broadband systems, which are currently at market penetration levels of 70–80% in several EU countries [4]. In addition, long-distance developments are brought about by the decoupling and merging the cellular telecommunications sector alongside different businesses such as media and IT [5]. Companies in the telecom sector are trying to capture a customer base and existing companies are restless to retain their customers. In these circumstances, the customer churn prediction model can be very crucial [6]. Accurately predicting customer churn can highly improve customer retention. As an alternative to the conventional approach of merely providing phone services, telecommunications firms have introduced a new product called triple play that combines TV, broadband, and phone offerings [7]. The triple play raises Average income Per User (ARPU), immediately boosting the company's earnings while also aiding in client retention. [8]. The challenge of triple play service is driven by the goal to offer clients with an unparalleled experience regardless of whether they use the service or request help from their service provider. [9]. The rapidly growing business may be impacted by this churning problem. Numerous customer churn prediction (CCP) models have been developed as a result, but they often fail to produce the expected CCP performance because of undiscovered potential variation variables that may affect customer churn (CC) [10].\n\n2 Literature Review\n\nCustomer churn is forecasted in many organizations using various statistical and machine learning methods. Figure 1 shows a survey of the articles that employed such approaches [11] (Table 1).\n\nTotal number of papers on each topic\n\n3 Proposed Model\n\nThe suggested approach uses a binary classification deep learning neural network for predicting whether a client would churn [18]. Customer churn prediction can be very fruitful for companies. Machine learning techniques can be used to anticipate consumer turnover [19]. The total amount of money a business might anticipate from a single client throughout their whole customer lifetime is termed as customer lifetime value (CLV). The corporation can dramatically boost its sales by raising the CLV without needing to spend more on marketing. That is one of the many reasons to go for Customer retention. In this paper, there are three phases:—(a) Data Preprocessing, (b) Data Visualization. (c) Model Building\n\n3.1 Data Preprocessing\n\nAuthors were able to determine a clear relationship between the customer's behavior and churn thanks to this dataset consisting of numerous customer-related information. The dataset has 7043 rows and 21 features [20]. In the dataset, each row corresponds to a customer, and each column provides information about that customer's qualities that is detailed in the column Metadata.\n\nHandling missing data is crucial as it affects statistical analysis through the loss of knowledge and data non-uniformities [21]. It is extremely important to pinpoint the location of missing values. In the IBM Telco dataset, the Total Charges contain 0.16% missing values. All the Total Charges null values occur when the Tenure value is zero. Zero is assigned to each missing value.\n\nOHE is a commonly used technique that uses binary encoding, in which the presence of a group is denoted by a 1, and its absence by a 0. Conversion of each category to a vector of N-dimension is done with an element of 0 or 1 with N number of categories in the nominal attribute. The raw data is not harmed by this method. However, OHE has a number of issues, including “sparse data” and “high dimensionality”, which will raise the computational cost of learning methods. OHE should not be used for high cardinality columns, because of this [22].\n\nTo improve the prediction results Standard Scaler is applied to normalize the data. With a mean of zero and a standard deviation of one (unit variance), the Standard Scaler assists in obtaining a standardized distribution. By omitting the feature's mean value and dividing the result by the feature's standard deviation, it standardizes the feature.\n\n3.2 Data Visualization\n\nEDA stands for Exploratory Data Analysis, which is an approach to analyzing and summarizing a dataset in order to gain insights and identify patterns and relationships between variables. EDA involves visualizing and manipulating data using various statistical and computational methods, in order to identify patterns, anomalies, and trends. Some common techniques used in EDA include scatter plots, histograms, box plots, and correlation analysis.\n\nThe goal of EDA is to understand the data and the relationships between variables, so that author can identify potential problems, such as missing data, outliers, or errors in the dataset, and address them appropriately. Making judgements on how to move forward with data modeling and analysis requires that the first phase in the data analysis process, called exploratory data analysis (EDA), be completed.\n\n3.3 Model Building\n\nThe created neural network has 12 input layer neurons, 6 hidden layer neurons, and 1 output layer neuron. No of epochs were 50 and batch size 80, respectively. Input layer and the hidden layer had the ReLU activation function and the output layer had Sigmoid activation function. Binary cross entropy was given as the loss function. Test size was chosen as 20%. This model is further improved by applying hyperparameter tuning techniques such as GridSearchCV and BayesianSeachCV, Dropout, and Early Stopping are also applied later on.\n\n4 Results\n\nSimple Dropout produced the highest accuracy. Our model obtained 83.1% accuracy. After several iterations 24 neurons for input layer 12 neurons for hidden layer and a dropout rate of 0.01 or 1% turned out be the best. It uses the Adam optimizer, a stochastic gradient descent technique that relies on adaptive estimate of first and second order moments. Following are the step-wise results.\n\n4.1 Data Preprocessing\n\nThese are all the columns and their data types, non-null counts (Fig. 2).\n\nDataset information\n\n4.2 Data Visualization\n\nThis plot is representing the correlation of every other feature with the dependent variable “Churn”. As noticeable InternetService, MonthlyCharges, PaperlessBilling are some features that are positively correlated with churn. On the other hand, Partner, TotalCharges, Dependents are some features that are negatively correlated with churn (Fig. 3).\n\nCorrelation plot\n\nAccording to the statistics, 73.4% of consumers have stopped churning and are still contributing to the firm, while 26.6% have switched to a different service provider, as seen in Fig. 4.\n\nChurn Rate in IBM Telco\n\nFrom the figure above, both the genders are equally likely to churn as well as to not churn. The effect of gender is negligible on the churn probability, as shown in Fig. 5.\n\nChurn distribution with gender in IBM Telco\n\nHighest churn percentage was seen from the payment method “Electronic check” which is 33.6%, as demonstrated in Fig. 6. “Mailed check” payment method has 22.8% churn percentage, “Bank transfer” and “Credit card” payment method has 21.9% and 21.6% churn percentage, respectively, (Fig. 7).\n\nChurn distribution with payment method in IBM Telco\n\nKernel density estimate plot of churn w.r.t monthly charges\n\nThis plot represents the distribution of “Churn” with respect to monthly charges. Visibly, the increase in monthly charges increases churn, slightly decreases retention (Fig. 8).\n\nKernel density estimate plot of churn w.r.t total charges\n\nThis plot represents the distribution of “Churn” with respect to Total Charges. Interestingly the increase in Total Charges decreases churn, also slightly decreases retention.\n\n5 Conclusion\n\nDeep learning techniques can be very beneficial and favorable because businesses today deal with large amount of data points that machine learning techniques cannot handle. Most of the recent research acknowledged the value of customer churn prediction models, particularly in the telecom sector. Many studies have started using deep learning algorithms to create models that can effectively forecast client attrition. Since the accuracy of the current deep learning models needs to be increased, there is still a gap in the field. This research is extremely important since it implements a deep learning model which can predict customer turnover with reasonably high accuracy when contrasted with current approaches and manage vast amounts of data. In this study, the ANN model forecasted customer attrition with a precision of 83.1% to be precise. The model having the total of 37 neurons including input layer with 24 neurons and one single neuron output layer, one hidden layer consisting of 12 neurons, 82 batch size, and 70 epochs gave the best results. The dropout layer is there between the input layer and the hidden layer. Hyperparameter tuning only resulted marginal improvements.\n\nReferences\n\nMishra, A., Reddy, U.S.: A comparative study of customer churn prediction in telecom industry using ensemble based classifiers. In: 2017 International Conference on Inventive Computing and Informatics (ICICI) (2017). https://doi.org/10.1109/icici.2017.8365230\n\nBerkhout, A.J. (Guus), Duin, P.A.: New ways of innovation: An application of the cyclic innovation model to the Mobile Telecom Industry. Int. J. Technol. Manag. 40(4), 294 (2007). https://doi.org/10.1504/ijtm.2007.015754\n\nHota, L., Dash, P.K.: Prediction of customer churn in telecom industry: a machine learning perspective. Comput Intell Mach Learn 2(2), 1–9, (2021). https://doi.org/10.36647/ciml/02.02.a001\n\nKhder, M.A., Fujo, S.W., Sayfi, M.A.: A roadmap to data science: background, future, and trends. Int. J. Intell. Inf. Database Syst. 14(3), 277–293 (2021)\n\nGoogle Scholar\n\nPerez, M.J., Flannery, W.T.: A study of the relationships between service failures and customer churn in a telecommunications environment. In: PICMET ’09—2009 Portland international conference on management of engineering and technology (2009). https://doi.org/10.1109/picmet.2009.5262262\n\nWu S (2023) Customer churn prediction in the telecommunication industry. Adv. Econ. Manag. Political Sci. 4(1):41–50. https://doi.org/10.54254/2754-1169/4/20221017\n\nTsai, C-. F., Lu, Y-. H. (2010) Data mining techniques in customer churn prediction. Recent Patents Comput. Sci. 3(1), 28–32 https://doi.org/10.2174/1874479611003010028\n\nChabumba, DR, Jadhav, A, Ajoodha, R: Predicting telecommunication customer churn using machine learning techniques Interdisc. Res. Technol. Manag. 625–636 (2021) https://doi.org/10.1201/9781003202240-98\n\nJain, H., Khunteta, A., Srivastava, S.: Churn prediction in telecommunication using logistic regression and logit boost. Procedia Comput. Sci. 167, 101–112 (2020). https://doi.org/10.1016/j.procs.2020.03.187\n\nArticle\n  Google Scholar\n\nMahmood, I.N., Abdullah, H.S.: Telecom churn prediction based on Deep Learning Approach. Iraqi J. Sci. 2667–2675 (2022). https://doi.org/10.24996/ijs.2022.63.6.32\n\nRani, K.S., Thaslima, S., Prasanna, N.G. L., Vindhya, R., Srilakshmi, P.: Analysis of customer churn prediction in telecom industry using logistic regression. Int. J. Innovative Res. Comput. Sci. Technol. 9(4) (2021). https://doi.org/10.21276/ijircst.2021.9.4.6\n\nPamina, J., Beschi Raja, J., Sathya Bama, S., Soundarya, S., Sruthi, M.S., Kiruthika, S., Aiswaryadevi, V.J., Priyanka, G.: An effective classifier for predicting churn in telecommunication. J. Adv. Res. Dyn. Control Syst. 11(1 Special Issue), 221–229 (2019)\n\nGoogle Scholar\n\nAhmed,U., Khan, A., Khan, S.H., Basit, A., Haq, I.U., Lee, Y.S.: Transfer learning and meta classification based deep churn prediction system for telecom industry 1–10 (2019)\n\nGoogle Scholar\n\nSundararajan, A., Gursoy, K.: Telecom customer churn prediction. March (2020). . https://doi.org/10.7282/t3-76xmde75\n\nMomin, S., Bohra, T., Raut, P.: Prediction of customer churn using machine learning. In: EAI International Conference on Big Data Innovation for Sustainable Cognitive Computing, pp. 203–212, (2020) 2020. D.\n\nGoogle Scholar\n\nUmayaparvathi, V., Iyakutti, K.: Automated feature selection and churn prediction using deep learning models. Int Res J Eng Technol (IRJET) 4(3):1846–1854 (2017)\n\nGoogle Scholar\n\nAgrawal, S., Das, A., Gaikwad, A., Dhage, S.: Customer churn prediction modelling based on behavioural patterns analysis using Deep Learning. In: 2018 International Conference on Smart Computingand Electronic Enterprise (ICSCEE) (2018). https://doi.org/10.1109/icscee.2018.8538420\n\nAhmad, A.K., Jafar, A., Aljoumaa, K.: Customer churn prediction in telecom using machine learning in big data platform. J. Big Data 6(1) (2019). https://doi.org/10.1186/s40537-019-0191-6\n\nHassonah, M.A., Rodan, A., Al-Tamimi, A.-K., Alsakran, J.: Churn prediction: A comparative study using KNN and decision trees. 2019 Sixth HCT Inf. Technol. Trends (ITT) (2019). https://doi.org/10.1109/itt48889.2019.9075077\n\nKassem, E.A., Ali, S., Mostafa, A., Kamal, F.: Customer churn prediction model and identifying features to increase customer retention based on user generated content. Int. J. Adv. Comput. Sci. Appl. 11(5) (2020). https://doi.org/10.14569/ijacsa.2020.0110567\n\nXie, J., Li, Y., Wang, N., Xin, L., Fang, Y., Liu, J.: Feature selection and syndrome classification for rheumatoid arthritis patients with traditional chinese medicine treatment. Eur. J. Integr. Med. 34, 101059 (2020). https://doi.org/10.1016/j.eujim.2020.101059\n\nArticle\n  Google Scholar\n\nLi, Q., Xiong, Q., Ji, S., Wen, J., Gao, M., Yu, Y., Xu, R.: Using fine-tuned conditional probabilities for data transformation of nominal attributes. Pattern Recogn. Lett. 128, 107–114 (2019). https://doi.org/10.1016/j.patrec.2019.08.024\n\nArticle\n  Google Scholar\n\nDownload references\n\nAuthor information\n\nAuthors and Affiliations\n\nDepartment of Computer Science and Engineering, School of Engineering and Technology, Adamas University, Kolkata, India\n\nSrijan Sur\n\nDepartment of Computer Science, Kristu Jayanti College, Autonomous, Bengaluru, India\n\nRiya Sil\n\nDepartment of Computer Science and Engineering, School of Engineering and Technology, Sharda University, Greater Noida, India\n\nBharat Bhushan & Anuj Kumar\n\nDepartment of Computer Science and Engineering, Amity School of Engineering and Technology, Research and Innovation Cell, Amity University, Greater Noida, India\n\nPronaya Bhattacharya\n\nCorresponding author\n\nCorrespondence to Riya Sil .\n\nEditor information\n\nEditors and Affiliations\n\nFaculdade de Engenharia da Universidade do Porto (FEUP), Porto, Portugal\n\nJoão Manuel R. S. Tavares\n\nDepartment of Computer Science and Engineering, Sister Nivedita University (Techno India Group), Kolkata, India\n\nSouvik Pal\n\nDepartment of Digital Systems, University of Thessaly, Larissa, Greece\n\nVassilis C. Gerogiannis\n\nData Science Laboratory, Faculty of Information Technology, Industrial University of Ho Chi Minh City, Ho Chi Minh City, Vietnam\n\nBui Thanh Hung\n\nRights and permissions\n\nReprints and permissions\n\nCopyright information\n\n© 2024 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.\n\nAbout this paper\n\nCite this paper\n\nSur, S., Sil, R., Bhushan, B., Bhattacharya, P., Kumar, A. (2024). Customer Churn Prediction Model Using Deep Learning. In: Tavares, J.M.R.S., Pal, S., Gerogiannis, V.C., Hung, B.T. (eds) Proceedings of Second International Conference on Intelligent System. ICIS 2023. Algorithms for Intelligent Systems. Springer, Singapore. https://doi.org/10.1007/978-981-99-8976-8_26\n\nDownload citation\n\nDOI\nhttps://doi.org/10.1007/978-981-99-8976-8_26\n\nPublished\n12 April 2024\n\nPublisher Name\nSpringer, Singapore\n\nPrint ISBN\n978-981-99-8975-1\n\nOnline ISBN\n978-981-99-8976-8\n\neBook Packages\nIntelligent Technologies and Robotics\nIntelligent Technologies and Robotics (R0)\n\nShare this paper\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nPublish with us\n\nPolicies and ethics\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n212.98.144.16\n\nAmerican University of Beirut (8200692269) - Lebanese Academic Library Consortium (3002060333)\n\n© 2025 Springer Nature\n",
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates": "AI-Powered Loan Approvals: Faster Credit Scoring & Less Risk : \nAutomating Loan Approvals: How AI Reduces Risk & Speeds Up Credit Scoring by 70%\n\nIntroduction\n\nThe financial industry has traditionally used conventional credit risk models to determine a borrower’s credit-worthiness. These conventional models, however, do not capture high-potential borrowers because they use old risk models and narrow data points. Artificial Intelligence (AI) and Machine Learning (ML) innovations are transforming credit risk evaluation using alternative data sets and sophisticated predictive analysis.\n\nIn this blog, we will talk about how AI-based credit scoring models for FinTech are revolutionizing loan approvals, minimizing risk, and accelerating the credit scoring process by as much as 70%. We will also take a look at the technicalities of AI deployment and top algorithms like XGBoost and CatBoost. And how these technologies are enhancing loan efficiency while maintaining regulatory compliance.\n\nNow, let’s dive in-depth to know the role of AI in loan approvals.\n\nHow AI is Transforming Loan Approvals\n\nAI is transforming loan approval through credit risk analysis and real-time automated decision making. Machine learning algorithms process large data points, making it a quicker, more precise, and more equitable loan process.\n\n1. AI vs. Traditional Risk Models: A Comparison\n\nTraditional credit scoring models such as FICO and VantageScore rely primarily on structured data, including credit history, income details, and debt-to-income ratios. However, these models have limitations:\n\nAI-powered credit scoring addresses these gaps by:\n\nAdditionally, artificial intelligence models constantly learn from new data, ensuring that credit risk assessments remain dynamic and up-to-date, leading to more unbiased lending decisions.\n\n2. Automating Credit Scoring with Machine Learning\n\nMachine learning algorithms process large amounts of structured and unstructured data to generate dynamic credit scores. The most widely employed algorithms are:\n\nWith these cutting-edge AI models, lenders can evaluate risk more precisely, resulting in improved lending decisions and reduced default rates. To transform your fintech business, you can take the help of machine learning specialists, and they will help you to automate the whole process.\n\nAccelerate Loan Approvals with AI-Powered Automation!\n\nBoost efficiency & reduce risks with Amplework.\n\nUnderstanding AI-Powered Credit Scoring Models\n\nAI-based credit scoring models examine various financial information through machine learning to properly evaluate borrower risk. Such models enhance loan approval by identifying patterns, eliminating bias, and enhancing decision-making speed.\n\n1. Role of XGBoost & CatBoost in Credit Risk Prediction\n\nAI-based credit scoring systems utilize gradient boosting algorithms like XGBoost and CatBoost to enhance precision. Such models:\n\nXGBoost is especially effective at identifying non-linear patterns in credit information and is thus very good at loan default prediction. CatBoost, on the other hand, is able to deal with categorical features effectively without requiring a lot of preprocessing and is thus a favorite in fintech applications.\n\n2. How AI Uses Alternative Data for Better Scoring\n\nAI credit models integrate alternative data sources to refine risk assessment:\n\nBy incorporating alternative data, AI-powered scoring models enable lenders to evaluate borrowers with limited or no formal credit history, significantly increasing financial inclusion.\n\nKey Benefits of AI in Loan Approval Automation\n\nAI in loan approval automation enhances speed, accuracy, and risk assessment by analyzing huge datasets in real-time. It reduces manual processing, minimizes fraud, and ensures compliance with regulatory standards.\n\n1. 70% Faster Credit Decisions\n\nAI-powered loan approval systems significantly reduce processing time by automating key workflows:\n\n2. Reducing Default Risk with Predictive Analytics\n\nPredictive analytics enables lenders to:\n\n3. Expanding Access to High-Potential Borrowers\n\nTraditional credit scoring excludes millions of individuals without formal banking histories. AI allows fintechs to:\n\nBy opting for intelligent automation solutions, you can take your finance business to the next level of success.\n\nStep-by-Step Process to Build an AI-Based Loan Approval System\n\nDeveloping an AI-driven loan approval system means using machine learning models to predict credit risk, handle vast financial data, and automate the decision-making process. With the integration of AI-driven credit scoring models, lenders can enhance accuracy, minimize risk, and approve loans quickly. This part covers the technical aspects, tools, and deployment mechanisms needed for an effective AI-driven loan solution. Let’s start to know the process of developing an AI-driven loan approval system.\n\nA. Data Collection & Preprocessing\n\nWe use Apache Kafka for real-time data ingestion, AWS Glue for seamless data cleaning and transformation, and Pandas for efficient data manipulation and preprocessing.\n\nA robust AI-powered credit scoring model requires diverse data sources to ensure accuracy in loan approval and risk assessment.\n\n1. Structured Data:\n\nStructured data refers to well-organized datasets that can be easily processed by AI models. This includes:\n\n2. Unstructured Data:\n\nUnlike structured data, unstructured data provides deeper insights into a borrower’s financial behavior through:\n\nB. Real-Time Data Streaming\n\nReal-time data ingestion ensures that loan approvals happen instantly based on the latest borrower data.\n\nAlso Read : How AI Automates Repetitive Tasks and Frees Up Resources\n\n2. Model Selection: Why XGBoost & CatBoost?\n\nSelecting the appropriate machine learning model is essential in developing an AI-based credit scoring model for fintech. XGBoost and CatBoost are notable for their high accuracy, effective processing of structured and unstructured financial data, and capacity to optimize real-time loan approval.\n\nA. Why Use XGBoost for AI-Powered Credit Scoring Models?\n\nXGBoost stands out in AI-based credit scoring models for fintech because of its high predictive accuracy, quick processing, and capability to deal with missing or imbalanced financial information. Its boosted decision trees enhance risk assessment, making loan approvals more efficient and reliable. XGBoost is extensively used for credit risk modeling because:\n\nB. Why CatBoost for Credit Scoring?\n\nCatBoost is best suited for AI-driven credit scoring models for Fintech because it processes categorical data with ease and little preprocessing. Its high speed of training and excellent accuracy make it ideal for real-time loan risk assessment and approval automation.\n\nC. LightGBM for Real-Time Credit Scoring\n\nLightGBM improves real-time AI-driven credit scoring models for Fintech by offering low-latency predictions with high accuracy. Its capability to process large datasets efficiently makes it suitable for instant loan approval and risk assessment.\n\n3. Training & Deploying AI Models for Real-Time Credit Scoring\n\nTraining and deploying AI-powered credit scoring models for fintech involves processing huge data of borrowers to enhance predictive accuracy. Advanced machine learning frameworks ensure real-time risk assessment, enabling instant and reliable loan approvals.\n\nA. Model Training for AI-Powered Credit Scoring Models\n\nAI models are trained using large datasets consisting of:\n\nTraining involves:\n\nB. Hyperparameter Tuning for Optimized AI Performance\n\nHyperparameter tuning ensures that AI models perform optimally:\n\nC. Deployment Platforms for AI-Based Loan Approval Systems\n\nScalable deployment platforms like AWS SageMaker, Google Vertex AI, and Azure ML simplify the integration of AI-driven credit scoring models for fintech. These platforms facilitate real-time loan approval by ensuring effective model deployment, monitoring, and optimization.\n\nBy applying AI-driven credit scoring models to fintech, financial institutions and NBFCs can streamline loan approvals, minimize risk, and enhance access for high-value borrowers. Ampleworks excels at deploying these advanced AI models into fintech lending platforms, allowing real-time, data-driven decision-making.\n\nAlso Read : How to Integrate AI into Your Existing Systems and Stay Competitive\n\nAI Integration in Fintech & NBFC Platforms\n\nThe banking sector is quickly embracing AI to improve loan approval, reduce risks, and enhance customer experience. Fintech firms and non-banking financial firms (NBFCs) use AI to optimize lending operations by combining real-time credit scoring, fraud detection, and risk estimation. Decision-making systems powered by AI enable lenders to accept or decline loan applications in seconds, maximizing the disbursal of loans.\n\nReal-Time Decision Engines in Lending Apps\n\nAI-driven real-time decision engines are at the core of contemporary fintech lending platforms. These engines scan enormous amounts of structured and unstructured financial information in milliseconds and make instant loan decisions based on pre-specified risk parameters.\n\n1. Instant Approvals\n\nAI-powered APIs eliminate manual review delays by automatically processing loan applications based on creditworthiness.\n\n2. Fraud Detection\n\nAI ensures secure lending by detecting fraudulent applications and suspicious transactions.\n\n3. Scalability\n\nAI models dynamically scale according to the volume of loan requests.\n\nCompliance & Security Considerations\n\nEnsuring compliance with financial regulations and maintaining data security are top priorities for AI-powered lending platforms. AI must be designed with transparency, ethical considerations, and strict adherence to global lending regulations.\n\n1. Regulatory Compliance\n\nAI models must align with strict global financial regulations, including:\n\n2. Model Transparency\n\nExplainable AI (XAI) plays an important role in ethical lending.\n\n3. Data Security\n\nAI-driven fintech solutions implement advanced encryption and privacy-preserving techniques to secure data.\n\nBy integrating AI-powered credit scoring models for fintech, Amplework ensures secure, efficient, and scalable loan approval processes while maintaining compliance with financial regulations.\n\nAlso Read : Optimizing Financial Risk Analysis with AI Agents: Development Strategies and Tools\n\nCase Studies & Real-World Applications\n\nAI is transforming the lending industry by enabling faster, more accurate credit assessments. Here are real-world examples of companies leveraging AI to optimize loan approvals and minimize risks.\n\n1. ZestFinance: AI-Powered Credit Scoring\n\nZestFinance used machine learning algorithms to examine alternative data sources, including utility bills and online activities, for credit risk evaluation. This AI-based method enhanced loan approvals for underbanked consumers while minimizing default risks.\n\n2. Upstart: Automating Loan Approvals\n\nUpstart used AI and deep learning algorithms to automate the loan underwriting process, minimizing the use of traditional credit scores. By analyzing over 1,600 data points, Upstart’s AI model improved approval rates and lowered interest rates for borrowers with limited credit history.\n\n3. Kabbage: AI-Based Small Business Lending\n\nKabbage uses AI and real-time data analytics to instantly assess the creditworthiness of small businesses. By integrating AI-powered risk modeling, Kabbage streamlined loan disbursement, reducing approval times from weeks to just minutes.\n\nWhy Choose Amplework for AI Integration in Fintech & NBFC Platforms?\n\nAmplework is a top AI development company that has expertise in creating AI-driven loan approval systems that use real-time scoring engines for enhanced and quicker credit risk evaluation. Our AI-driven solution decreases manual intervention, allowing financial institutions, fintech startups, and NBFC platforms to process loan applications with greater efficiency and less risk.\n\nWe incorporate deep credit risk AI models into the lending platform through the use of technologies like XGBoost, CatBoost, and LightGBM to make precise predictions of loan eligibility. Our expertise with FastAPI, Flask, and Spring Boot enables us to develop scalable, high-performance APIs that facilitate instant loan approvals with a user-friendly experience.\n\nSecurity and compliance are built into the DNA of our AI solutions. We use IBM Trusteer to detect fraud, OpenAI Explainability to ensure transparency of AI, and differential privacy technology to safeguard borrower data. We also maintain compliance with international lending regulations like GDPR, CCPA, and FCRA and enable fintech platforms to remain compliant.\n\nWith Ampleworks, organizations can automate the approval of loans, increase anti-fraud features, and expand their AI-based lending with assurance. Want to integrate AI with your fintech platform? Let’s innovate!\n\nWant to Revolutionize Lending with AI-Powered Credit Scoring!\n\nAutomate credit scoring & streamline approvals today.\n\nFinal Words\n\nAI-powered credit scoring models are revolutionizing loan approval by making credit assessments faster, more accurate, and inclusive. By leveraging advanced machine learning techniques, fintechs can automate risk analysis, expand lending opportunities, and improve financial access. If you want to integrate AI-powered credit scoring into your lending platform, Amplework offers cutting-edge AI solutions to help you lead the fintech revolution.\n\nFrequently Asked Questions\n\nHow does AI improve loan approval processes in fintech and NBFC platforms?\n\nWhat technologies does Amplework use for AI-powered loan approvals?\n\nHow does Amplework ensure compliance and security in AI-driven lending solutions?\n\nHow does AI in loan approval enhance the credit evaluation process?\n\nIs AI in loan approval reliable for assessing credit risk?\n\nHow fast is AI in loan approval compared to traditional methods?\n\nPartner with Amplework Today\n\nAt Amplework, we offer tailored AI development and automation solutions to enhance your business. Our expert team helps streamline processes, integrate advanced technologies, and drive growth with custom AI models, low-code platforms, and data strategies. Fill out the form to get started on your path to success!\n\nOr Connect with us directly\n\nsales@amplework.com\n\n(+91) 9636-962-228\n\nLooking to leverage AI for your next project?\n\nOur expert AI development teams are here to solve your most complex challenges with cutting-edge solutions tailored to your needs. Whether you require part-time, full-time, or hourly AI experts, we’re ready to help you innovate.\n\nPart Time\n\nFull Time\n\nHourly\n\nAI Solutions for Startups to Disrupt the Market\n\nWe empower startups to leverage AI for rapid scaling and competitive advantage. From concept to market, our AI solutions help turn your vision into reality, driving innovation and growth.\n\nAccessibility\n\nAdaptability\n\nScalability\n\nISO-27001\nSoftware Security\n\nDun & Bradstreet\nVerified\n\nGoogle Cloud\nPartner\n\nAWS Consulting\nPartner\n\nCareer :\n\nAmplework © 2025 | All Right Reserved\n\nBuilding a Robust Credit Scoring Model: Machine Learning for Loan ... : \nThis site can’t be reached\n\nschintu.medium.com unexpectedly closed the connection.\n\nTry:\n\nArtificial Intelligence and Machine Learning in Credit Risk Assessment ... : \nAbstract\n\nCredit risk assessment has become one of the major concerns in modern finance regarding informed lending decisions. Although several studies have used traditional logistic regression and linear discriminant analysis techniques, these have increasingly become inadequate tools in today’s complex and data-rich environment. Such models often struggle with large datasets and nonlinear relationships, thus reducing their predictive power and adaptability. Artificial Intelligence (AI) and Machine Learning (ML) provide two of the most innovative approaches to credit risk modeling. This paper reviews a few ML models applied to improve the accuracy and efficiency of credit risk assessment, from Random Forests and Support Vector Machines to Neural Networks. Compared to the more traditional models, AI models can enhance predictive accuracy by using a wealth of structured and unstructured information, including alternative information sources such as social media activities and transaction history. However, despite noticeable advantages, there are some challenges concerning the use of AI in credit risk assessment, including model opaqueness, bias, and regulatory compliance. The nature of such a “black box”, especially for deep learning algorithms, can limit their interpretability and complicate regulatory compliance and decision rationalization. To solve problems induced by this “black box” nature, explainable AI techniques, namely Shapley values and LIME, have been implemented to enhance the transparency of models and raise stakeholder trust in support systems for decision-making. This review aims to evaluate the current applications of AI and ML in credit risk assessment, weigh the strengths and limitations of various models, and discuss the ethical considerations and regulatory challenges linked to their adoption by credit institutions.\n\nKeywords\n\nArtificial Intelligence, Machine Learning, Credit Risk Assessment, Data Bias\n\nShare and Cite:\n\n1. Introduction\n\nAccurate and fair assessment of credit risks is rudimentary in modern finance for informed lending decisions. Whereas the traditional models included logistic regression and linear discriminant analysis, these have recently become inadequate in today’s complex and data-rich environments. Though transparent and regulator-friendly, these methods usually cannot capture borrowers’ nuanced nonlinear behaviors, compromising predictive accuracy.\n\nThe change in credit risk modeling heralds AI and ML. Predictive power has radically improved by accessing large datasets and the ability to discern patterns not accessible to traditional analytics. However, interpretability, ethical issues, and regulatory compliance become issues with the translucent nature of “black box” models such as deep neural networks.\n\nThis review critically analyzes various state-of-the-art AI and ML models in credit risk assessment by comparing them with traditional methods and discussing the potential of such models to reshape financial decision-making. We analyze different models, such as Random Forests, Support Vector Machines, and Neural Networks, to discuss how these technologies can improve accuracy by considering ethical and regulatory considerations proper for widespread adoption. The paper will help achieve a balanced understanding of the responsible implementation of AI, hoping that technological advancements will translate into improvements in the consistency and transparency of credit risk assessment.\n\n1.1. Traditional Credit Risk Modeling\n\nFor years, traditional models such as logistic regression and linear discriminant analysis have formed the foundation of credit risk assessment, a core function of the financial industry. These models conventionally rely on structured data, including borrowers’ income, credit scores, and debt levels, to predict the likelihood of default (Breeden, 2020). Although these methods have served well for many decades, they are based on assumptions of linearity, which often fail to capture the complexity of borrowers’ behavior in today’s data-rich environment.\n\nFor instance, logistic regression assumes a linear relationship between the input variables and the outcome default or non-default, which can oversimplify real-world credit risk (Bussmann et al., 2020). Furthermore, traditional models often require significant human intervention to extract key features, potentially overlooking meaningful nonlinear interactions between variables (Addo et al., 2018).\n\nDespite these deficiencies, traditional models remain popular, because they are transparent and acceptable to many regulators. Their interpretability advantages over more complex model types, neural networks are a good example of them appealing to regulators explicitly required to explain credit decisions. However, as data becomes increasingly complex and voluminous, these models struggle to keep pace, highlighting the need for more advanced techniques such as AI and ML.\n\n1.2. Credit Risk Assessment Models Using Artificial Intelligence and Machine Learning\n\nRecently, AI and ML have emerged as powerful tools for credit risk assessment. These methods offer several advantages over traditional models, mainly due to their ability to process large and complex datasets and uncover nonlinear relationships. Examples of such models include Random Forests, Support Vector Machines (SVMs), and Neural Networks, which can learn from vast amounts of data and recognize patterns that traditional models might miss.\n\nRandom Forest, an ensemble learning model, builds multiple decision trees and then combines their outputs for better accuracy. It is highly effective in credit risk assessment because the relationship between borrower attributes and default probability is often complex and nonlinear. Research has shown that Random Forests outperform conventional models in predictive accuracy, especially when the model is enhanced with data sources such as social media activities and transaction history.\n\nNeural Networks have also been shown to reveal complex nonlinear relationships among variables. They are instrumental when managing vast amounts of unstructured data, such as transaction histories or mobile phone usage data (Zhang & Yu, 2024). However, interpretability is considered one of the main disadvantages of neural networks, referred to as the “black box” problem-which makes it difficult for financial institutions to justify their credit decisions.\n\nSVMs also show exciting potential for credit risk modeling, especially when dealing with unbalanced datasets, as is often the case where the number of defaulters is less than that of non-defaulters. Support Vector Machines seek an optimal hyperplane that separates the two classes: defaulters versus non-defaulters. In situations where generalizing from traditional models is difficult, SVMs are particularly robust.\n\n1.3. Incorporation of Alternative Information Providers\n\nOne of the most crucial advantages of AI and ML models in evaluating credit risk is their ability to incorporate alternative data. Traditional models have relied heavily on structured data such as credit scores and income levels, which are inaccessible to all borrowers, particularly in emerging markets or those with incomplete credit histories (Breeden, 2020). In contrast, AI models can use unstructured data from social media activity, mobile phone usage, and transaction history to better understand a borrower’s behavior.\n\nFor instance, Kou et al. (Kou et al., 2019) highlighted the contributions of social media data in enhancing credit risk assessment. AI models can better predict financial reliability based on factors like the size of borrowers’ social networks, posting frequencies, and sentiment analysis from their online activities. Mobile usage data, such as call logs and location information, can provide in-depth insights into a borrower’s stability and predict their likelihood of default.\n\nThese alternative data sources are particularly useful for assessing borrowers with minimal traditional credit records, making them an integral part of modern credit risk models. However, the use of alternative data raises ethical concerns regarding data privacy and potential discrimination. Financial organizations should ensure that their alternative data practices comply with data protection regulations, such as the GDPR in Europe, and that their models are not discriminatory across different demographic groups.\n\n1.4. Explainability and Transparency in Artificial Intelligence Models\n\nOne of the biggest challenges posed by AI and ML models in credit risk assessment is their lack of transparency. Traditional models, such as logistic regression, are comparatively easier to explain, allowing lending and regulatory bodies to understand how decisions are made. In contrast, many AI models, particularly neural networks, function as an impenetrable “black box”, making it difficult to explain how they derive their predictions.\n\nThis lack of transparency creates serious problems in regulated industries like finance, where institutions must demonstrate the reasonableness of their lending decisions. Consequently, researchers have developed techniques to make AI models more explainable. For example, Shapley values provide a method for quantifying the contribution of each feature to model predictions, thereby helping financial institutions more easily explain their decisions to regulatory bodies and customers.\n\nAnother explainability technique is LIME, which generates interpretable models to approximate complex AI models for individual predictions. These methods are paramount for enhancing trust in AI credit assessment models, especially when a borrower is denied credit and an explanation is required (Angelini et al., 2008).\n\n1.5. Bias and Fairness of Artificial Intelligence Models\n\nWhile credit risk estimates have improved, biases inherent in AI models remain a concern. AI models are trained on historical data, which can be biased and lead to discriminatory outcomes. For example, a model trained on data from discriminatory lending practices may still be biased against certain demographic groups, even if sensitive attributes like race or gender are excluded.\n\nSome methods incorporate fairness constraints during model training to prevent sensitive attributes from affecting predictions. Other biases may be reduced by approaches like “fairness through unawareness”, which removes sensitive variables from the model. However, Kou et al. (Kou et al., 2019) demonstrated that information like geographical location or employment status can introduce biases even without sensitive variables.\n\nFinancial institutions must ensure that their AI models comply with regulatory standards that prevent discrimination, such as the FCRA in the United States and the Equality Act in the United Kingdom. According to Fernandez (Fernandez, 2019), failure to meet these regulations can generate serious legal and reputational risks for financial institutions.\n\n1.6. Performance Metrics for Artificial Intelligence Models in Credit Risk\n\nBeyond accuracy, various performance metrics are used for AI models in credit risk assessment. In imbalanced datasets-with few defaulters compared to non-defaulters-metrics like precision, recall, their harmonic mean (F1 score), and the area under the ROC curve (AUC-ROC) are crucial.\n\nAccuracy is defined as the ratio of correctly classified instances to all instances. While accuracy is useful, it can be misleading in imbalanced datasets where one class (the majority class) dominates. Precision is the ratio of true positive predictions (correctly predicted defaulters) to all positive predictions made by the model. Precision is relevant in credit risk assessment because a false positive-classifying a borrower as a defaulter when they are not-can lead to lost lending opportunities.\n\nRecall measures the proportion of true positives out of all actual positive instances (defaulters). A high recall ensures that a high proportion of high-risk borrowers are correctly identified. The F1 score, the harmonic mean of precision and recall, provides a balanced metric that considers both false positives and false negatives.\n\nThe AUC-ROC is a measure indicating the model’s capability to differentiate between defaulters and non-defaulters. A higher AUC indicates better model performance. Utilizing these performance metrics helps in understanding the strengths and weaknesses of different AI models in credit risk assessment.\n\n2. Methodology\n\n2.1. Data Collection and Preprocessing\n\nThis research is based on a comprehensive dataset from various sources that integrates both structured and unstructured data to enhance the validity of models when assessing credit risk. The structured data includes conventional financial variables related to borrowers’ income, debt-to-income ratio, credit history, and loan amounts-definite elements in any traditional credit risk assessment model (Breeden, 2020; Addo et al., 2018). Additionally, it leverages data from social media activities, mobile transactions, and geolocation to capture more depth in borrower behavior than conventional data may miss.\n\nThe dataset consists of over 200,000 records of individual borrowers spanning from 2017 to 2022. This large sample size provides high statistical power and makes the model dependable. All personal information is anonymized in accordance with strict standards, such as the European Union’s General Data Protection Regulation (GDPR), which demands that organizations maintain stringent data handling and protection.\n\nData preprocessing involves several important steps. Missing values are imputed using mean and mode strategies for numerical and categorical variables, respectively. Outliers are detected and removed to avoid bias in the results. For unstructured data, natural language processing techniques such as sentiment analysis and keyword extraction are employed to transform text data into analyzable features. Sentiment analysis, also known as sentiment analysis or opinion mining, is the process of analyzing, processing, summarizing, and inferring subjective texts with emotional connotations. By utilizing the ability of sentiment analysis, it is possible to automatically determine the positive and negative emotional tendencies of natural language texts with subjective descriptions and provide corresponding results. Keyword extraction is the process of extracting the most relevant words from the text that are most relevant to the meaning of the article. It has important applications in literature search, automatic summarization, text clustering, and text classification. Transaction data is summarized to reveal financial behaviors, including transaction frequency and volume.\n\nReferring to the commonly used classification standards for training and testing sets in deep learning networks, this article divides the preprocessed data into an 80% training set and a 20% testing set. To avoid overfitting, 10-fold cross-validation is used to tune the models and measure their performance.\n\n2.2. Model Selection and Architecture\n\nIn this study, we will evaluate the effectiveness of several AI and ML models in credit risk assessment and compare them with the baseline logistic regression model:\n\nRandom Forests: As an integrated learning model, random forests improve prediction accuracy by constructing multiple decision trees and merging their outputs. They are excellent at preventing overfitting and can provide an ordering of the importance of features that affect prediction.\n\nSupport Vector Machines (SVMs): SVMs are excellent at handling unbalanced datasets and are able to find optimal hyperplanes in high-dimensional spaces to separate different classes. This makes SVMs particularly well suited to deal with class imbalance.\n\nNeural networks: These models can handle complex nonlinear relationships between input variables. Deep learning models with a multi-layer structure make neural networks particularly effective at handling structured and unstructured data. They are especially good at recognizing patterns that may not be detected by traditional models.\n\nLogistic regression: used as a benchmark model for performance comparisons, although logistic regression is widely used for credit risk modeling and its linear nature has limitations when dealing with interactions between variables.\n\n2.3. Feature Engineering and Selection\n\nThe predictive power of a model can be significantly enhanced through efficient feature engineering. New features derived from raw data can more accurately reflect borrower behavior and risk profile, such as calculating debt-to-income ratios, default rates, and loan maturities.\n\nSentiment analysis of social media posts can reveal borrower sentiment and risk tolerance, information hidden in unstructured data. Variables such as transaction frequency, average transaction amount, and stability derived from geolocation data further enrich the database for risk assessment models. These features demonstrate details of financial behavior that are difficult to capture in traditional data.\n\n2.4. Evaluation Metrics\n\nTherefore, a range of metrics was considered to comprehensively evaluate the model performance: accuracy, precision, recall, F1 score, and area under the receiver operator characteristic curve (AUC-ROC). Since credit risk datasets are often classically unbalanced, relying on accuracy alone may be misleading.\n\nAccuracy rate: The ratio of correct classifications to the total number of instances.\n\nPrecision rate: These are the number of instances correctly predicted as defaulters to all instances predicted as defaulters, with the importance of avoiding opportunity costs due to incorrect lending decisions.\n\nRecall rate: the ratio of correctly identified true defaulters to highlight high-risk borrowers accordingly.\n\nF1-score: Harmonic mean of the precision and recall rates; it allows for a balanced view to find the model’s score, considering the effect of false positives and false negatives.\n\nAUC-ROC: It describes the model performance in terms of the capability to distinguish between defaulter and non-defaulter classes where higher values are better.\n\n2.5. Cross-Validation and Hyperparameter Tuning\n\nIn this thesis, 10-fold cross-validation will be applied to make sure that the model is generalizable for unseen data. One dataset for this purpose is further divided into ten subsets and is, in turn, trained on nine subsets and evaluated on the remaining one. In this manner, overfitting can be avoided, and the model can be generalized well to various subsets of data. The hyperparameter tuning was done through a grid search in which different combinations of hyperparameters that yield an optimum configuration were systematically tried. It ranges from modifying the number of trees in a random forest and the maximum depth of each tree to adjusting the number of hidden layers, the number of neurons per layer, and the learning rate in a neural network, making fits of model performance.\n\n2.6. Explainability and Interpretability\n\nGiven the complexity of AI models, especially neural networks, interpretability is an important challenge for financial institutions. Shapley values and LIME are two techniques that help to solve the “black box” problem of AI models by clarifying the contribution of each feature to the model prediction.\n\nShapley value: inspired by cooperative game theory, it is used to quantify the marginal contribution of each feature to the model output.\n\nLIME: approximates the behavior of complex models by constructing simplified interpretable models that revolve around individual predictions.\n\nThe integration of these interpretable techniques not only improves the accuracy of the model, but also ensures that the model is transparent and interpretable, enabling GSEs to make lending decisions that are both informed and interpretable, with results that are easily understood by regulators and customers.\n\n3. Results and Analysis\n\n3.1. Model Performance Comparison\n\nThis section introduces the comparative performance of the Random Forest, Support Vector Machine, Neural Networks, and baseline Logistic Regression models for credit risk assessment. Each model was evaluated for accuracy, precision, recall, F1 score, and AUC-ROC, which refers to the Area Under the Receiver Operating Characteristic Curve.\n\nDuring the analysis, the Random Forest model yielded the best performance with 93% accuracy and a mean AUC-ROC score of 0.94. Its high capability to deal with nonlinear relationships in big, complicated datasets and its robustness when structured data, such as social media activities or transaction history, give it a special place in credit default prediction.\n\nThe neural network models fared equally well by achieving the accuracy of 91% with an AUC-ROC score of 0.92. The strong points of the methods are treating high volume data to capture nonlinear interactions among features and allowing both structured and unstructured data inclusion.\n\nSupport Vector Machine showed worse results when the accuracy was 89%, and the AUC-ROC score equaled 0.88. It could be useful in a case if there is a small portion of defaulters compared with the overall population, and its strong classification capabilities will be helpful to identify a high-risk class of borrowers (Goodell et al., 2021).\n\nWhile the traditional credit risk assessments applied the LR baseline model, it resulted in the poorest performance among the models, where its accuracy was 84% and AUC-ROC was 0.79. Some of the limitations of LRs include reliance on linear assumptions, making it not that effective to capture the complexity in borrower behaviors.\n\nTable 1 is the summary of key performance metrics in a tabular form.\n\nTable 1. Model performance comparison table.\n\nModel\n\nAccuracy\n\nAUC-ROC\n\nPrecision\n\nRecall\n\nF1 Score\n\nRandom Forest\n\n93%\n\n0.94\n\n0.91\n\n0.88\n\n0.90\n\nNeural Networks\n\n91%\n\n0.92\n\n0.89\n\n0.86\n\n0.87\n\nSupport Vector Machine\n\n89%\n\n0.88\n\n0.85\n\n0.82\n\n0.84\n\nLogistic Regression\n\n84%\n\n0.79\n\n0.78\n\n0.74\n\n0.76\n\nPerformance of Visualization:\n\nROC Curves: The ROC curve of each model has been plotted to visually compare the strength of each model in classifying between defaulters and non-defaulters. Of these, the Random Forest had the maximum AUC, thus becoming the most powerful predictive model.\n\nPrecision-Recall Curves: Since credit risk datasets are usually imbalanced, precision-recall curves will give insight into how each model will perform to minimize false positives and false negatives. Random Forests and Neural Networks performed particularly well in maintaining high precision while finding the right balance in terms of recall.\n\nConfusion Matrix: The confusion matrix for each model provides different values of true positives, those respective defaulters who were correctly identified as true negatives, false positives, and false negatives that give detailed insight into model performance in a real-world setting.\n\nThese analyses pinpoint the potential of AI models, especially Random Forest and Neural Networks, which, by treating complex borrower behavior and using sources of structured and unstructured data, can outperform traditional methods such as Logistic Regression.\n\n3.2. Alternative Data Sources—Effectiveness\n\nThe key insight from this research is the exceptional performance gain achieved by incorporating additional data sources, such as social media activity, transaction history, and geolocation data. These alternative data sources gave a better context on how borrowers behave; this was especially true for those who did not have any formal credit history in the first place (Fernandez, 2019). In the case of social media analysis, for instance, good sentiments reflected a minimal risk of default. Other indicative factors showed that borrowers with stable high-frequency transactions have a lower risk of default, again proving the predictive power of alternative data.\n\nThe Random Forest and Neural Network models benefited the most from all the alternative data sources (Zhang & Yu, 2024). Because both can manage structured and unstructured data, they can notice complex patterns in borrower behavior, which the Logistic Regression model couldn’t (Addo et al., 2018). In contrast, the RF model used the frequency and volume of transactions as a significant measure of stability, while NN models leveraged social media activities and transaction behavior to build far more accurate default predictions.\n\n3.3. Explainability and Transparency of Artificial Intelligence Models\n\nWhile the models Random Forest and Neural Network outperformed Logistic Regression on predictive performance, their opaqueness is yet a big concern in highly regulated sectors like finance. According to Fernandez (Fernandez, 2019), financial institutions should be able to explain the rationale behind credit decisions; therefore, explainability is a crucial factor when considering AI model adoption. This has been accomplished through Shapley values and LIME in this work, Local Interpretable Model-Agnostic Explanations, by Goodell et al. (Goodell et al., 2021).\n\nShapley values provided insights into the contributions of individual features, such as debt-to-income ratio, payment history, and transaction frequency, to model predictions. For instance, debt-to-income ratio was one of the most influential variables in predicting default with continuity in most applications. LIME was used in specific instances to explain, for instance, why a particular borrower had been classified as high-risk. This at least allows more insight into the decision-making process.\n\nWhile these explanation techniques brought much-needed transparency, they added even more complexity. The trade-off between model complexity and explainability remains among the most important challenges that financial institutions need to address to ensure that regulatory bodies are satisfied and trust is instilled in AI-driven credit assessments.\n\n3.4. Bias and Fairness in Artificial Intelligence Models\n\nHowever, their use in credit risk management has raised several concerns regarding algorithmic bias and fairness. In general, AI models are subject to learning potential biases from the data used for training to disadvantage one demographic group or another. For instance, even though discriminatory on lending issues, AI models could practice it if they had been biased in the beginning against some groups, even if sensitive attributes like race or gender are excluded from the dataset.\n\nThe principle of FTU compliance in this work eliminated the bias by not considering sensitive variables in the models. However, even with FTU, biases are still possible, like proxy variables of location or employment sector. In line with this, the calculations of fairness metrics were performed for demographic parity and equal opportunity, which are used to determine whether models make equitable predictions across different demographic groups. Fairness constraints increased equity but slightly reduced performance in the Neural Network model (Thakkar, & Chaudhari, 2021). This reveals that fairness is a competing factor against accuracy.\n\n3.5. Comparison of Artificial INTELLIGENCE and Traditional Model\n\nOverall, the findings of this study confirm that AI models outperform traditional logistic regression in credit risk evaluation. The performance of the Random Forest and Neural Network models was consistently better than logistic regression in all the individual metrics, especially when exploiting nonlinear relationships and alternative data sources.\n\nWhile logistic regression is still appreciated for its simplicity and transparency, its reliance on linear relationships limits its performance on more complex, real-world credit risk scenarios. The Support Vector Machine model balances accuracy and interpretability well without being as complex as Neural Networks, thus making it implementable for financial institutions that want better performance without going all the way to black-box models.\n\nThe findings reveal that it is high time financial institutions adopt more advanced AI-driven systems; otherwise, they would likely be excluded from contemporary finance. Nevertheless, the problems of model transparency and fairness, together with assurance of conformation with regulatory requirements, must be addressed so that AI can deploy its duties responsibly in credit risk assessment.\n\n4. Discussion\n\n4.1. Implication of the Findings for Credit Risk Assessment\n\nThese results indicate benefits of using AI and ML models in evaluating credit risk compared to traditional approaches. The two AI models, Forests and Neural Networks, are significantly better and can improve the accuracy of the predictions in identifying complex nonlinear relationships that are easily ignored by traditional techniques, such as those based on logistic regression. Indeed, prior studies have established that these AI models can use alternative data, such as how active a credit-seeker has been on social media, his transaction history, and geolocation, to arrive at more accurate risk assessments (Fernandez, 2019).\n\nOne key takeaway is that those financial institutions using AI-based credit risk models gain a much deeper understanding of the behavior of borrowers. Integrating unstructured data offers better predictions of creditworthiness for people who either have a thin or no formal credit history. This is particularly important in developing markets and among gig economy workers, who typically do not have access to traditional credit systems because of a lack of structured financial data.\n\n4.2. Challenges and Limitations of Artificial Intelligence Models\n\nConversely, several challenges dominate deploying AI and ML models to mainstream adoption in the financial industry. The most important of these is the inherent complexity of deep learning models, which requires several computational resources and technical skills for modeling, deployment, and maintenance substantial enough to make smaller institutions struggle to compete with large organizations able to invest more energy in state-of-the-art AI solutions.\n\nAnother critical issue with these AI models is their lack of transparency. That is why traditional models, like Logistic Regression, have remained preferable because they are easy to interpret and explain the decisions, both to a regulator and a customer who has been rejected. It is true that deep learning models have emerged as “black boxes”, and it becomes challenging to justify credit decisions, especially in cases of denial to applicants. This causes severe complications regarding regulatory issues, such as adhering to a framework that needs to be set by the General Data Protection Regulation for explainability when a decision is made.\n\nThis paper tried to overcome these challenges by incorporating Shapley values to explain model predictions in a transparent manner without sacrificing predictive power. Goodell et al. (Goodell et al., 2021) have proposed LIME as a method to derive locally interpretable models for individual predictions, increasing the potential for transparency such that institutions may give clear justifications for their credit decisions, as stated by (Bussmann et al., 2020). In any case, implementing these explainability techniques requires additional computational effort, adding to the complexity of adopting AI.\n\n4.3. Some Ethics to Consider: Bias and Fairness in Artificial Intelligence Models\n\nCredit risk assessment always faces the algorithmic bias issue from deployed AI models. The model can inherit biases leading to disparity in outcomes, as most models are trained with historical data. For instance, a model that has been trained on some discriminatory practices of the past might keep discriminating against certain groups, even when sensitive attributes like race or gender are removed. This will be especially detrimental in regulated industries where equity and fairness are core ends.\n\nThe researchers in this work have excluded sensitive variables from the dataset, using a principle called Fairness Through Unawareness. However, there is still a chance that proxy features might encode sensitive information; therefore, fairness constraints during model training were necessary to make the predictions equal among demographic groups.\n\nFairness constraints imposed to enforce equity came at some cost in predictive accuracy, especially for the Neural Network model. This depicts the challenge of trading off fairness against performance, an issue now at the forefront of research. Organizations must ensure that their AI models comply with the legislation enacted to eradicate discrimination, such as the U.S. Fair Credit Reporting Act and the Equality Act in the UK.\n\n4.4. Future Research Directions\n\nResults from this work have also pointed out some directions for future research and development concerning the applications of AI models in credit risk assessment. Hybrid model development will be one very promising avenue that combines the predictive power of AI with the interpretability of traditional models. By integrating AI techniques with Logistic Regression or decision trees, financial institutions can have the best of both worlds: accuracy from AI and transparency for regulatory compliance. Hybrid systems could allow real practicality for financial institutions that might be uneasy about thoroughly implementing black-box models like Neural Networks.\n\nAnother exciting direction is improving fair-aware AI models. Current approaches, such as Fairness Through Unawareness, are a good starting point for which much future research needs to develop sophisticated techniques that tend to reduce bias without performance compromise. Techniques such as adversarial de-biasing or representation learning for fairness might hold promise for mitigating bias with at least loss in model accuracy.\n\nThen, there is a valid ethical basis upon which this research into the effects of alternative data sources in credit risk assessment can be pursued. Although social media activities and transaction histories present quite valuable advantages concerning improvement in credit risk predictions, data privacy and regulatory compliance issues implicate essential considerations, especially under rigid data protection laws such as the GDPR, as identified by (Goodell et al., 2021). Projects soon must be considered considering the impact of more privacy-preserving AI techniques, federated learning, and differential private ways to train AI models from decentralized data without touching borrower privacy.\n\nFinally, future studies need to be directed at enhancing the explainability of AI models, since they are still opaque despite the high accuracy of deep learning models. New developments in explainability techniques which can make complex models more interpretable would thus be crucial in developing AI-driven credit assessments that are transparent, trustworthy, and adherent to regulatory standards.\n\n5. Conclusions\n\nThe present study has explored the transformative power of AI and ML in credit risk assessment by comparing various state-of-the-art techniques with conventional models like Logistic Regression. The results show more significant improvements in predictive accuracy and identification of risk features for AI models, particularly Random Forest and Neural Networks. These models outperform conventional ones by processing and integrating a large volume of structured and unstructured information to attain much deeper insights into the behavior of borrowers (Breeden, 2020; Addo et al., 2018).\n\nAnother key takeaway is that incorporating alternative data sources, such as social media activity, transaction histories, and geolocation data, improves the performance of AI models. Moreover, a better breakthrough of predictive power is achieved that assists institutions in making more accurate estimates of credit risk, especially for people who have limited formal credit records. These AI models synthesize these sources of data in a way that would not otherwise be available for evaluating borrower risk, hence making the models valuable for underrepresented or emerging market borrowers.\n\nHowever, there are some issues that AI models need to go through before they can earn inconspicuous acceptance. First, transparency in the decision-making process, such as Neural Networks, is imperative due to its lack. Their black-box nature antagonizes explanations of how the model derived a particular decision, which is inappropriate for regulated industries. This research used Explainable AI techniques, including Shapley values and LIME, for enhanced interpretability according to feature importance and the building of overall trust in AI-driven decisions.\n\nIt is also important to consider bias and equity. Models built on historical data run the risk of perpetuating bias, leading to discriminatory outcomes. Fairness Through Unawareness and fairness constraints are among the various techniques applied in this study, but more research needs to be done to develop methods that reduce bias with minimal impact on performance. Showing compliance with regulatory standards goes hand in hand with responsible AI adoption.\n\n5.1. Future of AI in Credit Risk Assessment\n\nWhile the future of AI in credit risk assessment has a promising direction, several key considerations must be met to ensure that this technology is used responsibly and effectively. One auspicious direction is the development of hybrid models that take advantage of the predictive power of AI but combine such powers with the interpretability of traditional models. Adding AI techniques to the models with Logistic Regression or decision trees may give some accuracy of AI but retain the transparency to satisfy regulatory compliance. Hybrid models can thus provide a practical alternative for financial institutions that may be quite apprehensive about their use.\n\nAnother interesting path might be investigating the development of eventual fairness-aware AI models that can reduce bias without sacrificing accuracy. Techniques like adversarial de-biasing and fair representation learning are promising methods for mitigating bias with minimal model performance compromise. Future research should investigate such techniques within the context of credit risk assessment, especially regarding their effectiveness in reducing bias in real-world applications.\n\nAnother important direction of research is related to the ethical impact of using alternative data sources in credit risk assessment. While alternative data offers advantages in making credit risk predictions, it also raises major concerns about data privacy and regulatory issues, especially with strict data protection laws such as GDPR. Further research is warranted, which would apply newly developed AI privacy-preserving techniques, including federated learning and differential privacy, to enable the training of an AI model on decentralized data without violation of the borrower’s privacy.\n\nExplainability in AI models is a key element of any responsible use of AI within credit risk evaluation. While the Shapley values and LIME are indeed especially useful tools to explain the decisions of a model, much remains to be undertaken regarding the development of more sophisticated techniques of explainability that may render complex models, such as Neural Networks, more transparent. As AI continues to develop, it will be relevant to ensure that models remain interpretable, trustworthy, and comply with regulatory standards (Breeden, 2020).\n\n5.2. Realistic Recommendations to Financial Institutions\n\nBased on the findings of this study, the following are some practical recommendations that any financial institution willing to adopt AI in its credit risk assessment processes can consider. First, institutions should consider the adoption of hybrid models that provide a balancing act between the accuracy of AI and the interpretability of traditional models. These models can be a workable solution for organizations that have to comply with regulatory requirements while leveraging the predictive power of AI.\n\nSecond, fairness and equity should be inherent concerns of any AI model within financial institutions. This means periodic auditing of models for bias elimination, constraining models at development for fairness, and adherence to relevant regulations on the subject. Institutions should also be open towards borrowers regarding data considered during credit assessment and give meaningful reasons if adverse decisions are being passed.\n\nThirdly, institutions should consider the use of alternative data sources to populate their credit risk profiles. This will be particularly important in the case of borrowers with limited or no formal credit history. Nonetheless, this must be pursued while giving full respect to borrowers’ privacy and keeping in mind the needs of data protection regulations such as the GDPR. Privacy-preserving AI techniques, such as federated learning, should be considered to mitigate the risks associated with using alternative data.\n\nThe second is that it must make investments in explainable AI techniques, which shall help and work towards more transparency of the AI models that one uses. These techniques can include Shapley values and LIME in enabling institutions to explain their credit decisions to regulators and customers and therefore further build trust and ensure that regulatory standards are met. With increasing complexity, AI models would require the clear and interpretable development of institutions that can be trusted by all stakeholders.\n\n5.3. Future Directions and Considerations\n\nAI and ML represent the future of credit risk assessment: more enabling correct decisions, lower default rates, and increased credit accessibility (Milojević & Redzepagic, 2021). Minimizing transparency, bias, and fairness challenges, along with compliance issues, paves the path for successful adoption. Successful implementation of hybrid models, fairness-aware approaches, and techniques for explainable AI will enable institutions to harness all the benefits of AI while never compromising on ethical grounds or regulatory compliance.\n\nWhile much fair, private, and explainable AI models are developed today, with the rapid development of AI, future research in this area should be channeled to make AI models fairer, more private, more explainable for a truly more equitable financial system that exists between borrowers and lenders.\n\nConflicts of Interest\n\nThe author declares no conflicts of interest regarding the publication of this paper.\n\nReferences\n\nCopyright © 2025 by authors and Scientific Research Publishing Inc.\n\nThis work and the related PDF file are licensed under a Creative Commons Attribution 4.0 International License.\n",
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI": "Personalized Marketing: Effective Strategies Utilizing Machine Learning : \nPersonalized Marketing: Effective Strategies Utilizing Machine Learning\n\nIntroduction\n\nIn the fast-paced realm of modern marketing, personalization has emerged as a transformative strategy that resonates with consumers on a deeper level. By leveraging cutting-edge technologies, particularly Machine Learning (ML), marketers are now able to dissect vast troves of data to deliver tailored experiences that meet individual preferences. As audiences increasingly demand content that feels relevant and engaging, the incorporation of ML into marketing strategies is not just an option but a necessity for brands striving to stand out in a crowded marketplace.\n\nThis article aims to delve into the world of personalized marketing and explore effective strategies that harness the power of machine learning. We will discuss various approaches brands can take to implement personalized marketing campaigns, the role of ML in customer segmentation, predictive analytics, and enhance consumer interactions. By understanding these strategies and the technologies driving them, businesses can create marketing efforts that are more effective, relevant, and ultimately successful.\n\nUnderstanding Personalized Marketing and Its Importance\n\nPersonalized marketing is a strategy that tailors marketing messages, content, and product offerings to the preferences and behaviors of individual consumers. By leveraging data from various sources such as purchase history, browsing behavior, and social media interactions, brands can create a more engaging and relevant user experience. This approach has shown significant improvements in customer satisfaction, loyalty, and conversion rates.\n\nOne of the primary benefits of personalized marketing is the ability to create a strong connection with customers. When consumers feel like a brand understands their needs and preferences, they are more likely to engage with that brand. Additionally, personalized marketing allows for more effective targeting, reducing the wastage of marketing budgets by ensuring that the right messages reach the right audiences at the right time. Such precision can lead to improved customer acquisition and retention rates, making it a crucial aspect of contemporary marketing strategies.\n\nMachine learning is at the forefront of this personalization revolution. It enables marketers to analyze vast datasets quickly and effectively, uncovering hidden patterns and insights about consumer behavior. As ML algorithms become increasingly sophisticated, they provide the ability to not only predict what customers may want but also to automate and optimize marketing efforts in real-time. This combination makes machine learning an indispensable tool for any brand looking to enhance its personalized marketing initiatives.\n\nKey Strategies for Implementing Personalized Marketing with ML\n\nTo effectively harness machine learning for personalized marketing, brands can utilize several strategies. Each of these strategies taps into the capabilities of ML to deliver tailored experiences and improve customer engagement.\n\nCustomer Segmentation\n\nCustomer segmentation is the process of dividing a customer base into distinct groups based on shared characteristics. ML algorithms enhance this process by analyzing extensive datasets to uncover nuanced insights that traditional segmentation methods may overlook. By employing clustering techniques, brands can categorize customers based on demographics, purchasing behavior, preferences, and even psychographics.\n\nFor instance, a fashion retailer might use machine learning to segment its customers into groups such as “frequent buyers,” “occasional shoppers,” and “bargain hunters.” Each group would respond differently to marketing messages, and through targeted campaigns, the retailer can customize its approach. Frequent buyers might receive exclusive previews of new collections, while bargain hunters might receive notifications on discounts and special sales.\n\nFurthermore, this segmentation can be dynamic, allowing businesses to adjust their strategies according to changes in consumer behavior. With ML algorithms continuously learning from new data, customer segments can be updated in real-time, ensuring that brands are always aligned with their audiences' current preferences.\n\nPredictive Analytics\n\nOne of the most powerful applications of machine learning in personalized marketing is predictive analytics. This involves using historical data and machine learning models to forecast future consumer behavior. By understanding trends and predicting what products or services a customer may be interested in, brands can tailor their marketing strategy accordingly.\n\nFor example, an e-commerce platform can analyze customers’ past purchases and browsing history to identify potential future purchases. The platform may find that a customer who frequently buys hiking gear is likely to be interested in camping equipment or outdoor apparel. Machine learning models can be used to compel recommendations, improve upselling and cross-selling efforts, and create timely email reminders about relevant products.\n\nMoreover, predictive analytics allows brands to proactively address customer needs. By anticipating when a customer might need a refill of a product or when they might be looking for complementary items, brands can reach out to customers with personalized messages that drive engagement. This not only improves the customer experience but also encourages repeat business.\n\nReal-Time Personalization\n\nIn today’s digital landscape, real-time personalization has become crucial. With machine learning, companies can tailor experiences in real-time based on immediate customer interactions. For instance, personalization can occur on web pages, in emails, or during live chat interactions. This ensures that customers receive information that is relevant not only based on their previous behaviors but also on their current activities.\n\nA classic example can be found on streaming platforms like Netflix. When a user logs in, machine learning algorithms analyze their viewing history, preferences, and even the time of day to display recommended content tailored to that moment. If a user has recently watched sci-fi movies, the platform might prioritize recommending similar genres during their next visit, effectively maximizing their engagement.\n\nMoreover, companies can implement real-time adjustments to promotions and offers based on a user’s on-site behavior. If a user lingers on a product page or adds an item to their cart without completing the purchase, a targeted ad or notification can pop up, offering a discount or highlighting complementary products. This responsiveness powered by machine learning not only improves conversion rates but also enhances customer satisfaction.\n\nChallenges of Implementing Personalized Marketing with ML\n\nWhile the advantages of personalized marketing using machine learning are substantial, several challenges can arise during implementation. Understanding these obstacles is crucial for brands looking to optimize their marketing efforts.\n\nData Privacy Concerns\n\nAs brands gather vast amounts of data to fuel personalized marketing efforts, concerns regarding data privacy and consumer trust come to the forefront. Customers are increasingly aware of data collection practices, and any perceived misuse can lead to a loss of trust. Marketers must navigate regulations such as the General Data Protection Regulation (GDPR) or California Consumer Privacy Act (CCPA), which dictate how personal data can be collected, stored, and used.\n\nTo combat these concerns, brands must prioritize transparency in their data practices. Clear communication regarding what data is collected, how it is used, and the benefits to the customer is essential. By giving consumers control over their data and allowing them to opt in or out of data collection, brands can foster trust and build lasting relationships with their customers.\n\nAlgorithm Bias\n\nAnother significant challenge in the realm of machine learning is algorithm bias. Machine learning models can inadvertently perpetuate existing biases present in historical datasets. If a dataset reflects stereotypical behaviors or assumptions, the resulting predictions and recommendations will carry those biases, potentially alienating certain consumer groups or diminishing their experiences.\n\nTo mitigate algorithm bias, brands need to ensure that their datasets are diverse and representative. Regular audits of ML models can help identify and correct biases, leading to more equitable personalization strategies. This not only leads to improved customer satisfaction but also enhances a brand’s reputation for fair practices.\n\nIntegration and Resource Allocation\n\nFinally, the integration of machine learning into existing marketing strategies can be complex. Organizations need to ensure that they have the appropriate technology infrastructure, skilled personnel, and budgets to implement ML effectively. The integration process often requires significant investment in tools and training, which can be intimidating for businesses.\n\nBrands should consider a phased approach to integration, starting with smaller projects or pilot programs to test the efficacy of machine learning-driven personalized marketing. Successful case studies can validate the investment required while building momentum for broader implementation. Additionally, allocating resources toward training employees about machine learning and data analysis is vital to ensure that teams can fully utilize the technology available.\n\nConclusion\n\nAs consumer expectations continue to evolve, the need for effective, personalized marketing strategies powered by machine learning is undeniable. Brands that successfully implement personalized marketing strategies utilizing ML can foster greater customer engagement, loyalty, and satisfaction. By employing techniques such as customer segmentation, predictive analytics, and real-time personalization, companies can provide tailored experiences that resonate deeply with their audiences.\n\nHowever, it is essential to navigate the challenges that accompany this newfound power. Prioritizing data privacy, addressing algorithm biases, and securing the necessary resources for successful implementation are crucial steps in achieving personalized marketing success. By embracing these strategies responsibly, brands can build stronger connections with their customers, enhancing their overall marketing efforts.\n\nIn the end, as technology continues to advance and reshape the marketing landscape, those who adapt and innovate will thrive. By honing in on personalization through machine learning, businesses position themselves at the forefront of customer engagement, ensuring sustainable growth in an ever-competitive market.\n\nIf you want to read more articles similar to Personalized Marketing: Effective Strategies Utilizing Machine Learning, you can visit the Personalization Algorithms category.\n\nYou Must Read\n\nApplying Regression Models for Social Media Performance Metrics\n\nRule-based vs. Machine Learning for NLP: Which Approach Is Superior?\n\nSetting up SQL Server Machine Learning Services\n\nCategories\n\nRelated Posts\n\nThe Use of Gradient Boosting for Investment Risk Assessment\n\nHarnessing Wearable Technology: Machine Learning for Health Insights\n\nExploring Effective Data Augmentation Techniques for Machine Learning\n\nTerms and Conditions\n\nPrivacy Policy\n\nCookie Policy\n\nAbout Us\n\nContact Us\n\nAI-Driven Advertising: The Role of Machine Learning in Ads (2025 ... : \nAI-Driven Advertising: The Role of Machine Learning in Optimizing Ad Campaigns\n\nArtificial intelligence is reshaping digital marketing.\n\nDid you know that by 2025, AI technology in marketing is expected to reach $190 billion?\n\nThis surge underscores AI's growing role in advertising. Here are just the first three changes that pop to mind:\n\nMachine learning algorithms analyze vast amounts of data, providing valuable insights into consumer behavior. These insights enable advertisers to make informed decisions, enhancing campaign performance.\n\nReal-time analytics allow for immediate adjustments, optimizing return on investment.\n\nAI-driven tools personalize user experiences, tailoring content to individual preferences. This personalization increases user engagement and conversion rates.\n\nAnd as AI continues to evolve, its impact on advertising strategies will expand, offering numerous benefits to marketing professionals.\n\nBut more on that in a second.\n\nContinue reading to discover how AI-driven advertising is transforming campaign optimization, targeting, and the future of digital marketing.\n\nP.S. Ready to see the power of AI-driven advertising in action? Choose one of the best AI marketing agencies today, and let their experts show you how machine learning can optimize your ad campaigns for maximum impact and ROI.\n\nWhat Is Machine Learning in Advertising?\n\nMachine learning in advertising leverages advanced algorithms to analyze massive amounts of data, detecting patterns that reveal consumer preferences and behaviors.\n\nThis data-driven approach enables predictive analytics, a powerful tool that forecasts user behavior and purchase intent.\n\nBasically, machine learning in advertising helps you go beyond traditional guesswork.\n\nPro tip: Research indicates that businesses using AI can boost conversion rates by up to 20% and can increase click-through rates by up to 50%.\n\nWhy Do You Need AI in Advertising?\n\nAI is reshaping the advertising industry, offering a wide range of benefits that traditional methods can't match.\n\nHere's why incorporating AI into your advertising strategy is a game changer:\n\nEnhanced targeting for potential customers: AI-driven advertising leverages advanced algorithms to analyze vast amounts of data, identifying target audiences with remarkable precision. For instance, Coca-Cola uses artificial intelligence to dissect consumer data, leading to more effective marketing efforts and improved customer engagement.\n\nImproved conversion rates through personalized experiences: AI tailors content to individual preferences, increasing the likelihood of conversion. Amazon's recommendation engine, powered by AI and neural networks, contributes to approximately 35% of its sales, showcasing the impact of personalized experiences on conversion rates.\n\nReal-time adjustments and actionable insights: AI provides real-time analytics, allowing advertisers to adapt to market conditions instantly. The North Face employs AI-powered tools to offer personalized product recommendations in real time, enhancing user engagement and optimizing campaign performance.\n\nAccurate predictions of consumer behavior: AI predicts future trends and consumer preferences using predictive analytics. Netflix utilizes advanced algorithms to forecast viewer preferences, which drives content creation and improves user retention.\n\nOptimized bidding strategies and advertising budgets: AI enhances bidding strategies on platforms like Google Ads, maximizing return on investment. Advertisers using Google's AI-driven bidding strategies have reported increased conversion values, according to Google's case studies.\n\n7 Use Cases for AI and Machine Learning in Advertising\n\nNow that we’ve seen the benefits, here’s how to leverage AI and machine learning in advertising:\n\n1. Optimize Ad Creative and Testing\n\nAI-powered tools enhance ad creatives by generating dynamic ad content and enabling rapid testing.\n\nUsing natural language generation, AI can create numerous ad variations in real time, analyzing engagement rates and identifying high-performing ads.\n\nThis data-driven approach ensures more effective advertising strategies and faster campaign optimization.\n\nThere are plenty of examples to prove this, but let’s review the three for now:\n\nHeadway's AI Integration: The Ukrainian edtech startup Headway incorporated AI technologies like ChatGPT, Midjourney, and HeyGen into its marketing strategy, resulting in 3.3 billion ad impressions in the first half of 2024.\n\nTikTok's Symphony Creative Studio: TikTok launched Symphony Creative Studio, an AI-powered video creation platform that allows advertisers to automatically generate video content based on product descriptions, create multiple ad versions, and remix existing ones. This tool enables real-time optimization and enhanced targeting, improving user engagement and conversion rates.\n\nSource\n\nMemorable's AI-Based Co-Pilot: Memorable offers an AI-based co-pilot that guides advertisers through every creative decision, from conceptualizing ideas to assembling the final ad. It provides data-driven feedback for each decision, allowing for real-time adjustments and optimization of campaign performance.\n\nPro tip: We’ve found that pairing AI-driven testing with precise audience segments makes ad creative truly resonate. Don’t just test variations—align each version with distinct audience insights. Target your creative tests to specific user behaviors or preferences, and you’ll see clearer patterns in engagement rates and conversions.\n\n2. Better Targeting and Personalization\n\nAI-driven advertising enhances audience segmentation and targeting by analyzing vast amounts of data to identify potential customers.\n\nFor example, 75% of advertisers use AI for audience segmentation and targeting, leading to a 20% increase in ad campaign performance.\n\nBy leveraging machine learning algorithms, advertisers can deliver personalized experiences, improving user engagement and conversion rates.\n\nThis data-driven approach enables real-time adjustments to advertising strategies, optimizing return on investment and aligning with business goals.\n\nPro tip: We advise integrating real-time analytics into your advertising strategy. By doing so, you harness the power of immediate data to make informed decisions. The point is to ensure that your ads resonate with your audience segments dynamically, reflecting current market trends and user engagement effectively.\n\n3. Budget Optimization and Cost Efficiency\n\nMachine learning algorithms enhance advertising budgets by analyzing real-time data to allocate resources effectively.\n\nFor example, Klarna implemented AI-driven tools to manage its marketing campaigns, resulting in annual cost savings of approximately $10 million.\n\nHere’s why that works.\n\nFirst, when you leverage AI-powered analytics, you can make data-driven decisions, reducing cost per acquisition and maximizing return on investment.\n\nBesides, you can use these tools for budget optimization. AI takes the guesswork out of ad spend, using real-time data and advanced analytics to ensure every dollar works smarter:\n\nPro tip: We recommend setting up automated bid adjustments based on real-time performance data. By doing this, you can allocate budget precisely where it's driving results, scaling down on underperforming segments and avoiding unnecessary costs, all while maintaining strong engagement rates and maximizing returns on investment.\n\n4. Cross-Channel and Cross-Screen Advertising\n\nAI enables advertisers to create cohesive advertising strategies across platforms, ensuring consistent messaging for individual consumers. AI-driven algorithms analyze user behavior to coordinate ads across devices, creating consistency without redundancy.\n\nStudies show that brands using cross-channel strategies see a 24% increase in ROI, while 73% of consumers prefer ads tailored to their experiences.\n\nMachine learning tools like Google’s Display & Video 360 allow advertisers to manage multi-platform campaigns in real-time, adjusting bids and messages based on audience insights.\n\nSource\n\n5. Ad Bidding and Management\n\nWe already pointed to the fact that AI and machine learning algorithms enhance ad bidding strategies by analyzing real-time data to optimize bids for conversions.\n\nHere’s a more in-depth look at how this works.\n\nFor example, Google's Smart Bidding uses machine learning to adjust bids, resulting in a 20% increase in conversion rates for advertisers.\n\nBasically, AI-driven tools enable real-time adjustments to bidding strategies, ensuring optimal use of campaign budgets and aligning with business goals. This approach allows advertisers to make data-driven decisions, reducing cost per acquisition and maximizing return on investment.\n\nBasically, these AI-powered analytics allow you to improve campaign performance and achieve better results.\n\n6. Ad Placement\n\nAI-driven tools determine optimal ad placement by analyzing user interactions, time of day, and digital platforms with the highest engagement rates.\n\nFacebook’s AI-based ad placement system, for example, uses advanced algorithms to match ads with audience segments most likely to convert.\n\nAI can also place ads in real time, maximizing visibility across a wide range of digital destinations.\n\n7. Competitor Research\n\nAI and machine learning drive competitive insights by analyzing ad trends, keywords, and testing strategies in real time.\n\nBesides, AI reveals patterns in ad frequency and placement, pinpointing what works and where gaps lie. Use this info to adjust your own campaigns for higher performance based on proven competitor results.\n\nAnd you have a wide array of options to include in your tool stack.\n\nTools like Adbeat and SpyFu track competitors’ ad structures and ad variations across platforms, offering data on which ads perform best and identifying keyword shifts.\n\nSource\n\nChallenges and Limitations\n\nEmerging technologies like quantum computing and generative AI are poised to transform AI-driven advertising.\n\nQuantum computing could enable real-time optimization of complex bidding strategies, enhancing campaign performance.\n\nHowever, integrating these technologies presents challenges, including ethical considerations and the need for advanced algorithms to handle vast amounts of data.\n\nHere are our future predictions for machine learning advancements in advertising:\n\nEnhanced personalization: Machine learning algorithms will analyze user behavior and preferences more deeply. They can even become so good as to make data-informed predictions about customer behavior enabling advertisers to deliver highly personalized content. Imagine Minority Report but for the advertising context.\n\nReal-time optimization: Advancements in real-time analytics will allow for immediate adjustments to ad campaigns, improving return on investment even more. AI-driven tools can become so good as to suggest real-time manual bidding strategies, not just automatic ones. They could even suggest multiple ad placements and budgets, with possible outcomes for each. This would be like Sheldon Cooper’s tri-dimensional chess.\n\nPredictive analytics: Future machine learning models will predict consumer behavior with greater accuracy, enabling advertisers to anticipate market trends and adjust strategies accordingly. This predictive capability will lead to more effective advertising efforts and higher engagement rates.\n\nIntegration with customer-facing technologies: The integration of machine learning with technologies like virtual assistants and speech recognition will open new avenues for customer interactions. Advertisers will leverage these platforms to deliver targeted messages, enhancing user experience and engagement.\n\nEthical considerations: As machine learning becomes more integral to advertising, ethical implications regarding data privacy and user consent will gain prominence. Advertisers will need to navigate these challenges to maintain consumer trust and comply with regulations.\n\nMaxwell Finn on Twitter - digital marketing expert in Facebook and TikTok ads\n\nConclusion: The Transformative Impact of AI in Advertising\n\nAI is reshaping advertising through data-driven insights and real-time optimization.\n\nMachine learning analyzes massive data sets, revealing patterns in customer behavior that drive smarter campaigns.\n\nPredictive analytics allow advertisers to adjust strategies instantly, improving conversion rates and return on investment.\n\nFor those looking to adopt AI-driven strategies, start by exploring AI-powered tools that enhance targeting and personalization.\n\nLeverage real-time analytics to make informed decisions, ensuring ads reach relevant audiences effectively.\n\nEmbrace competitor research through machine learning to stay agile and competitive.\n\nAI-driven advertising is evolving quickly. Adopting these tools and insights now positions advertisers to lead in the future.\n\nReady to elevate your advertising strategy with AI? Choose one of the best AI marketing agencies today, and let their experts guide you in maximizing your campaign's impact and ROI.\n\n🎯 Ready to learn more? Recommended reads:\n\nTop Full-Service Advertising Agencies\n\nBest Advertising Agencies\n\nTop B2B Advertising Agencies\n\nTop International Ad Agencies\n\nTop Creative Ad Agencies\n\nTop Paid Media Agencies\n\nFrequently Asked Questions\n\nHow is AI used for advertising?\n\nAI in advertising analyzes large data sets to target audiences, personalize ads, and optimize budgets in real time. It predicts user behavior, adjusts ad placements instantly, and increases engagement and ROI with data-driven insights.\n\nWhat is an example of AI advertising?\n\nNetflix’s recommendation system is a top example. Netflix personalizes content suggestions, uses AI to tailor recommendations for specific audience segments, increasing engagement.\n\nWhat is AI-driven marketing?\n\nAI-driven marketing uses machine learning to automate targeting, personalize messages, and optimize ad spend. It processes real-time data to adjust campaigns, boosting engagement and ROI.\n\nHow to use AI to create ads?\n\nTools like Google’s Responsive Search Ads and TikTok’s Symphony Creative Studio use AI to generate, test, and optimize ad content automatically, improving relevance and performance.\n\nWhat is the #1 most used AI tool for advertising?\n\nGoogle Ads’ Smart Bidding is widely used, leveraging AI to adjust bids in real time, boosting conversion rates and ROI.\n\nWhich AI can create ads for free?\n\nCanva and Lumen5 offer free tools that use AI for ad creation, ideal for social media and basic video ads, with premium options available for advanced features.\n\nIs it legal to use AI art for commercial use?\n\nYes, depending on the platform. Most AI art tools allow commercial use but may have specific terms. Always check guidelines to ensure compliance.\n\nHow to use AI to make Facebook ads?\n\nFacebook Ads Manager uses AI to optimize ad variations, target lookalike audiences, and segment users by behavior, improving ad relevance and performance.\n\nHow does Google use AI for ads?\n\nGoogle’s AI adjusts bids, personalizes ad content, and analyzes keywords in real time. This optimizes targeting and ROI for advertisers on its search and display networks.\n\nAdditional Reading on Marketing\n\nThoughts on B2B, Entrepreneurship, & Thinking Different…\n\nI promise to only send good stuff :)\n\nCopyright 2024 — Waving Banner LLC dba Jake-Jorgovan.com\nSponsorships Disclaimer\n\nML-Powered Personalization: How Machine Learning Drives Targeted ... : \nML-POWERED PERSONALIZATION: HOW MACHINE LEARNING DRIVES TARGETED MARKETING CAMPAIGNS\n\nIn the era of digital-first marketing, consumers expect brands to understand their unique preferences and provide tailored experiences. Unfortunately, traditional marketing methods often fall short of meeting these expectations, leading to irrelevant ads and wasted budgets. Enter machine learning in Digital Marketing, a game-changer that enables brands to deliver hyper-personalized and impactful campaigns.\n\nThe AI-powered campaign delivered a 275% ROI on ad spending, a 23% average click rate uplift and a 15% average open rate uplift.\n\nWith the ability to process massive datasets and uncover actionable insights, machine learning (ML) is revolutionizing how businesses connect with their audiences. From optimizing ad placements to creating personalized content, ML ensures every aspect of marketing is more efficient, effective, and engaging. Let’s explore how machine learning for ads and targeted campaigns are reshaping the digital marketing landscape.\n\nUNDERSTANDING MACHINE LEARNING IN MARKETING\n\nMachine learning is a subset of artificial intelligence (AI) designed to learn and improve from data without explicit programming. In marketing, ML leverages algorithms to analyze customer data, predict behaviors, and optimize campaigns.\n\nUnlike traditional approaches, machine learning in digital marketing replaces guesswork with precision. It enables marketers to:\n\nThis blend of automation and intelligence allows businesses to create highly targeted marketing campaigns that resonate with their audience.\n\nROLE OF MACHINE LEARNING IN ADVERTISING\n\n1. Smarter Audience Segmentation\n\nOne-size-fits-all marketing is no longer effective. Today’s consumers demand relevance, and segmentation is the key to achieving it.\n\nMachine learning excels at analyzing vast datasets to group audiences based on shared characteristics such as:\n\nBy identifying distinct audience segments, ML helps marketers craft tailored messages that are more likely to engage and convert.\n\nImagine receiving an ad that feels like it was created just for you. That’s the power of AI-driven marketing personalization.\n\nML tailors every aspect of the customer experience, from product recommendations to ad copy. This level of personalization:\n\nGone are the days of setting and forgetting ad campaigns. Machine learning continuously monitors performance and makes adjustments in real-time.\n\nFor instance, ML can:\n\nThis ensures your machine learning for ads campaigns delivers the best possible results.\n\nWhy Personalization Is a Game-Changer\n\nPersonalization isn’t just a buzzword—it’s a necessity. Modern consumers expect brands to understand their needs and preferences. Machine learning in advertising makes this possible by analyzing data to create relevant and meaningful interactions.\n\nBenefits of Personalization in Marketing\n\nHOW MACHINE LEARNING POWERS TARGETED MARKETING CAMPAIGNS\n\n1. Data Collection and Analysis\n\nMachine learning thrives on data. It collects and analyzes information from various sources, such as:\n\nWith this data, ML identifies patterns and trends that inform marketing strategies.\n\n2. Predictive Analytics\n\nPredictive analytics is one of the most innovative machine learning applications in digital marketing. By analyzing historical data, ML can forecast:\n\n3. Customer Journey Optimization\n\nUnderstanding the customer journey is critical for effective marketing. ML tracks every touchpoint, from the first ad click to the final purchase, and identifies areas for improvement.\n\nFor example:\n\n4. Dynamic Content Creation\n\nMachine learning doesn’t just analyze data; it acts on it. With dynamic content creation, ML adapts marketing materials to suit individual preferences.\n\nThis includes:\n\nAPPLICATIONS OF MACHINE LEARNING IN DIGITAL MARKETING\n\n1. Social Media Marketing\n\nSocial platforms generate massive amounts of data daily. Machine learning analyzes this data to:\n\n2. Email Campaigns\n\nML enhances email marketing by:\n\n3. Chatbots and Virtual Assistants\n\nPowered by machine learning, chatbots provide personalized customer support 24/7. They can:\n\n4. Product Recommendations\n\nE-commerce platforms use ML to suggest products based on browsing and purchase history. This not only increases sales but also improves customer satisfaction.\n\nOVERCOMING CHALLENGES IN MACHINE LEARNING FOR MARKETING\n\nWhile the potential of machine learning is immense, implementing it isn’t without challenges.\n\nWith regulations like GDPR, marketers must ensure they handle customer data ethically and transparently.\n\nDeploying machine learning requires advanced tools, skilled professionals, and significant investment.\n\nIf not carefully managed, ML algorithms can reinforce existing biases in the data, leading to skewed results.\n\nDespite these hurdles, the rewards of machine learning in marketing analytics far outweigh the risks.\n\nFUTURE TRENDS: WHAT’S NEXT FOR MACHINE LEARNING IN MARKETING?\n\n1. Hyper-Personalization\n\nAs algorithms become more sophisticated, brands can deliver experiences tailored to individual customers.\n\nMachine learning will play a crucial role in refining voice and visual search capabilities, transforming how customers find products.\n\nAI tools will generate personalized content at scale, reducing the time and effort required for manual creation.\n\nML will help brands understand consumer sentiment and adapt their strategies by analyzing customer feedback, social media posts, and reviews.\n\nCONCLUSION: PARTNER WITH EXPERTS TO UNLOCK ML’S POTENTIAL\n\nMachine learning is revolutionizing the marketing landscape, offering unparalleled personalization, efficiency, and growth opportunities. From predictive analytics to real-time ad optimization, the potential of machine learning in advertising is limitless.\n\nLooking to elevate your marketing efforts? Riithink, a leading digital marketing company in the USA, specializes in harnessing machine learning to create impactful, targeted campaigns. Whether it’s optimizing ad performance or enhancing customer experiences, we’ll help you stay ahead in the ever-evolving digital world.\n\nReady to transform your marketing strategies? Contact Riithink today and unlock the full potential of machine learning for your business!\n\nRECOMMENDED FOR YOU\n\nTop Innovative Digital Marketing Trends You Need to Know For 2025\n\nHow to Use Google Ads AI Image Editor to Boost Campaigns\n\nWhy It’s Important to Curate Stories and Reels Ads Into Your Social Media Strategy\n\nSCHEDULE YOUR COMPLIMENTARY DIGITAL MARKETING CONSULTATION\n\nGet Started\n\nCOME VISIT\n\nRiithink Digital Marketing\n\n103 Mosaic Blvd\nSuite 120\nPittsboro, NC 27312\n\nCONTACT US\n\n+1 (919) 766-9180\nhello@riithink.com\n\nSTAY IN THE LOOP WITH OUR WEEKLY NEWSLETTER\n\n© Copyright Riithink Digital Marketing 2025 | Disclaimer Policy | End User License Agreement | Privacy Policy | Terms And Conditions\n\nCookie Policy: We use cookies on this site to improve your experience as explained in our Cookie Policy. You can reject cookies by changing your browser settings.\n",
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions": "Machine learning forecasting in the macroeconomic environment: the case ... : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nMachine learning forecasting in the macroeconomic environment: the case of the US output gap\n\nAccess provided by American University of Beirut\n\n193 Accesses\n\nExplore all metrics\n\nAbstract\n\nThis paper aims to forecast deviations of the US output measured by the industrial production index (IPI), from its long-run potential output, known as output gaps. These gaps are important for policymakers when designing relevant economic policies, especially when a negative output gap may show economic slack or underperformance, often associated with higher unemployment and low inflation. We use a dataset that includes 32 explanatory economic and financial variables and 18 lags of the IPI, spanning the period from 2000:1 to 2022:12, resulting in 50 variables and 276 monthly observations. The dataset is fed to five well-established machine learning (ML) methods, namely decision trees, random forests, XGBoost, long short-term memory (LSTM) and support vector machines (SVMs), coupled with the linear, the RBF and the polynomial kernel. Moreover, we use the standard elastic net logit method from the area of econometrics as a benchmark. Our results indicate that the tree-based ML techniques perform better in-sample, and the best overall forecasting model is the XGBoost achieving an out-of-sample accuracy of 91.67%.\n\nSimilar content being viewed by others\n\nMachine Learning in Macroeconomics: Application to DSGE Models\n\nBenchmark Analysis of Machine Learning Methods to Forecast the U.S. Annual Inflation Rate During a High-Decile Inflation Period\n\nOpening the Black Box: Machine Learning Interpretability and Inference Tools with an Application to Economic Forecasting\n\nExplore related subjects\n\nAvoid common mistakes on your manuscript.\n\n1 Introduction\n\nOutput gaps are deviations of real GDP from its long-run trend or as it are called “potential output” (Kiley 2013). In the case of a positive gap, when the GDP is higher than the long-term trend, there is an aggregate demand greater than the supply and may lead to higher inflation and possibly lower unemployment. In the opposite case, that of a negative gap or recessionary gap, the economy is producing less than the long-term trend and that may cause unemployment and low growth rates (Blanchard et al. 2010). Governments and central bank policymakers are keen to be able to predict these deviations from the long-term trend, accurately and in time, as the effects of the output gaps on the real economy and the wellbeing of the society are obvious (Orphanides and Van Norden 2002). The significant impact of output gaps on society and the need for accurate and timely output gap forecasts by policymakers motivate us to test alternative machine learning (ML) models for successfully predicting these output gaps on a monthly basis. In addition, our motivation is based on the poor performance of the output gap estimates by standard econometric models like VAR models, making related government policies less efficient.\n\nIn this paper, we use several well-established machine learning methodologies and more than 30 economic and financial explanatory variables plus a relatively long lag structure of the output gap, to capture possible dynamics. Machine learning methodologies are suitable for forecasting sets of data with many independent variables, when possible, where relationships are usually nonlinear. In this case, ML models can uncover patterns and relationships in the data that cannot be unveiled with classic econometrics (Araujo et al. 2020). In addition, these machine learning algorithms are “agnostic” in the sense that they do not have a hypothesis-based analytic solution like econometrics, but they are rather based on a fast and efficient trial and error process to produce the best classification or regression model for the datasets used.\n\nIn this respect, following the studies of Gogas et al. (2014, 2015) and Sofianos et al. (2021), where the authors forecasted the output gaps for the US and the euro area using only short- and long-term interest rates, we attempt to forecast the output gap in the US using a wide range of macroeconomic and financial variables which may help to improve the accuracy of the predictions and consequently the efficiency of the policies applied. The quarterly sampling of real GDP makes the series suitable for econometrics, but not dense enough for ML methods. The availability of quarterly data is subject to a certain delay following the conclusion of the quarter. In such cases, the utility of forecasts of the output gap based on these data may be limited, particularly in the context of sudden external shocks to the economic system, as it was the case with the recent outbreak of the coronavirus (Berger et al. 2023). To manage this shortcoming, in this paper, we use the monthly US industrial production index (IPI), a measure that is widely used as a proxy for the GDP (Sofianos et al. 2021).\n\nThis paper adds to the existing literature in several ways. First, we use several explanatory variables from the areas of economics and finance; recent research evidence suggests that multivariate models can be used to produce reasonable estimates of the output gap (Barigozzi and Luciani 2023; Morley and Wong 2020). Second, we use the more accurate machine learning models for prediction. These methods outperformed, in most cases, standard econometric analysis (Camba-Mendez and Rodriguez-Palenzuela 2003; Berger and Kempa 2011; Araujo et al. 2020; Granados and Parra-Amado 2024; Dubbert and Kempa 2024). Specifically, in our effort to forecast US output gaps, we use five machine learning algorithms (XGBoost, SVM, random forests, decision trees and long short-term memory) and the elastic net logistic Regression (logit) methodology from the field of econometrics as our benchmark method. Third, out paper can serve as a stepping stone to apply our methodology to emerging markets where there is a substantial volatility in the trend growth rate relative to developed markets (Aguiar and Gopinath 2007).\n\n2 Literature review\n\nOutput gaps may be defined as deviations of real GDP from its long-run stochastic trend, Kiley (2013).\nFootnote\n1 These gaps have been the subject of academic and institutional research (e.g., by the ECB and the OECD), since it has been recognized that the ability to predict them accurately can contribute to the development of effective policy initiatives by governments aimed at enhancing social welfare. Based on mainstream macroeconomic theory, a positive gap may lead to high levels of inflation and possibly lower unemployment, and a negative gap is related to unemployment and low growth rates (Phillips 1958; Okun 1963).\nFootnote\n2\n\nPolicymakers can reduce these deviations from the potential (trend) output by adopting the appropriate fiscal and/or monetary policies and stabilizing the economy. For example, with the appropriate fiscal policy with increased spending and/or low taxes, demand can be boosted, closing a negative output gap. In the opposite case, a tight fiscal policy may be adopted as a measure against inflation. We must mention here that output gaps may affect the financial sector as well. For example, a switch in the correlation between inflation and output gap may affect real bond and equity returns (Campbell et al. 2020).\n\nThe serious effects of output gaps on society and the accurate and timely forecasts of the output gap by the policymakers have been studied in many research papers. In general, with univariate and multivariate statistical analysis, the evidence indicated imprecise output gaps estimates (Dupasquier et al. 1999). Early works with the use of VAR models obtained results that showed limited forecasting accuracy; these models underperformed when compared with an arbitrary autoregressive model regarding inflation, under the assumption that the output gap estimate should have a forecasting power over inflation (Camba-Mendez and Rodriguez-Palenzuela 2003). In the framework of VAR modeling, Morley and Wong (2020), with quarterly data for the period 1959–2016, performed an empirical analysis with up to 138 variables covering various aspects of the US economy. According to their results, the unemployment rate, inflation, aggregate consumption, stock prices and the federal funds rate contain more or less relevant information for estimating the output gap than output growth.\n\nFor the policy implications of the output gap, the “real time” data are of paramount importance, as for example, in the cases of fiscal measures or interest rate changes. The term “real time” data includes all data that are available or reported at the time of the observation contrary to the term “revised data” that includes adjusted information after the initial release. Obviously, real-time data are even more precious in case of a response needed when a shock takes place, or an exogenous factor affects the economic system like in the recent case of COVID-19. In the above framework, monthly data are closer to “real time” data since a more precise picture of the economy is captured due to the higher frequency of observations. The higher data frequency gives better responsiveness to changes in the economy and consequently better policies applied by governments. For example, in terms of monetary policy, which is often associated with output gaps, it has been demonstrated that policy directions can be significantly different when based on “real time” data than revised data and that the use of “real time” data is important for targeted and successful monetary policies (Orphanides 2001).\n\nIn addition to the data frequency, many estimation methods, used in previous studies, failed to accurately forecast the output gaps. Orphanides and Norden (2002) examined the detrending methods for “real time” data for the output gap and concluded that the quality of the estimates was quite low. Analytically, estimates of the output gap in real time often do not even agree on the sign of the gap and measurement error would pose a serious policy problem (Orphanides and Norden 2002). Additional research confirmed the estimation difficulties for real-time output gaps with standard statistical methods. For example, and for the case of Norway, Bernhardsen et al. (2005) obtained evidence for the poor performance in predicting real-time output gaps. Cayen and van Norden, (2005) using quarterly data for the Canadian economy tested several univariate and multivariate models and obtained poor estimation results. In their study, even the transformation of the dependent variable, i.e., from levels to changes, reduced the measurement error only slightly. In addition, for the euro area, Marcellino and Musso (2011) used a multivariate unobserved component model (MUCM), a multivariate statistical analysis which considers correlations between the used variables and concluded that there were problems in the magnitude and the sign of the euro area output gap.\n\nAs mentioned before, the estimation of the output gap is more challenging for developing economies due to the economic fluctuations in these economies, plus the fact that these economies are more vulnerable to exogenous shocks. In a relative study for Pakistan, Haider and Safdar Ullah (2008) used an array of techniques, including linear time trends, filters like Hodrick–Prescott, VAR models and unobserved component models (UCMs) to calculate benchmark output gap as a combination of the outcomes of the different methods used. In terms of policy implications, they observed a correlation between inflation and their combined measure of the output gap. Their statistical results lead them to suggest that there must be “considerable caution when constructing output gaps and using them for policy analysis in developing countries.”\n\nNevertheless, recently in the field of statistical analysis, there have been some research papers with positive results for the estimate of the output gap. For example, Quast and Walters (2022), using US quarterly data, argued that the use of a modified Hamilton filter yielded reliable real-time output gap estimates. Assunção and Fernandes (2024) argue that nonlinear techniques may give better results by treating better sharp and important changes in the growth trend giving attention only to the relevant information. In their study, the robustness of nonlinear filters was confirmed in forecasting downtrends and uptrends (recessions and recoveries) in the economic output.\n\nThe reported superiority of nonlinear filters hints that the use of machine learning techniques may add significantly to the correct estimation of the output gap since machine learning models can capture nonlinear relationships and interactions between economic variables that standard statistical models cannot. In Pérez-Pons et al. (2021), a comparative survey between the econometrics and the ML applications in economics, the authors concluded that in most cases ML outperformed econometric models. Regarding forecasting output gaps with ML techniques, the literature is very limited. To the best of our knowledge, there are only three papers on the subject: one for the euro area and two for the US. In all three, the ML outperforms the traditional econometric models, namely elastic net logit (Sofianos et al. 2024), probit (Gogas et al. 2014) and logit/probit (Gogas et al. 2015).\n\n3 Methodology and data\n\n3.1 Methodology and hyperparameter optimization\n\nMachine learning was developed in the 1950s to provide artificial intelligence (AI) systems with the ability to learn. The essence of machine learning is the automated construction of analytical models, based on the idea that computers can learn from historical data, identify complex and nonlinear patterns and make decisions with minimal human intervention (Gogas and Papadimitriou 2021). In our research, out of a large number of existing ML techniques, we selected five, based on their advantages compared to possible limitations, as these are described as follows.\n\nThe support vector machine (SVM) family of ML algorithms is used quite successfully for both classification and regression problems. The fundamental idea behind the SVM is to identify the best linear separator for classifying data points into two classes. In order to achieve this in cases of datasets contaminated by outliers, noise or produced by nonlinear systems, SVMs are coupled with the so-called \"kernel trick:\" a kernel function that projects the original data from the data space to a higher dimensional space, called feature space, where the dataset might classify more accurately (Cortes and Vapnik 1995). An important advantage of the SVM is that they exhibit robustness to extreme values, and they can achieve particularly high prediction accuracy, while maintaining high generalization properties. In addition, they are easy to implement and train without many hyperparameters. A possible disadvantage is that they may exhibit low performance when the number of features exceeds the number of samples of the training data (Russell and Norvig 2016; Mouchtaris et al. 2021).\n\nWe also use the classification and regression trees (CART) algorithm, also known as decision trees (Breiman et al. 1984), which uses the Gini impurity index (the probability that a randomly chosen observation of the data set is misclassified) to create optimal splits on the dataset and generate decision trees.\n\nAlthough decision trees are simple to understand and train, they may suffer from limited generalization ability, the well-known low bias-high variance problem, also known as overfit. Breiman developed the random forest algorithm in Breiman (2001), to overcome this problem. This learning technique blends the idea of decision trees with a bootstrapping and aggregating process, known as “bagging” (Breiman 1996). Many different classification trees are created during the training process of the random forest models. For every single tree, we use a randomly selected, with replacement (bootstrapped), sample of the same size as the original data set. To further reduce the model's dependency on the training set data and avoid overfitting, a randomly chosen subset of features (independent variables) is used from the original set of n features at each split in each tree. The advantages of random forests are the reduced risk of overfitting to the data and the ease of determining the significance of variables due to the built-in variable importance measure technique (VIM). As possible disadvantages, we may include the resources required, due to the existence of multiple trees (time consuming process), and the complexity of the models both in structuring and interpretation.\n\neXtreme Gradient Boosting (XGBoost, Chen & Guestrin 2016) is a machine learning algorithm that belongs to the family of ensemble learning methods. Unlike random forest, it builds an ensemble of decision trees sequentially, with each subsequent tree aiming to correct the errors made by the previous ones (boosting). The algorithm employs a gradient boosting framework, where each tree is grown to minimize a specific objective function, typically a combination of a loss function and a regularization term. The advantages of XGBoost are the ease of implementation and the reduction of the bias. Possible disadvantages are that the XGBoost models are prone to overfitting, and they have high computational requirements.\n\nLong short-term memory (LSTM), proposed by Hochreiter and Schmidhuber (1997), is a type of recurrent neural network (RNN) architecture that addresses the vanishing and exploding gradient problems (during backpropagation, gradients should not be too small or too large) commonly encountered in traditional RNNs. This enables effective modeling of long-range dependencies in sequential data. LSTM incorporates gated units, including input, forget and output gates, which regulate the flow of information within the network. This allows it to selectively remember or forget information over time. LSTMs are capable of capturing and retaining crucial information over long sequences, making them ideal for tasks such as time-series prediction. The main advantages of the LSTM methodology are the lack of need of preprocessing the data, the adaptive structure and the time-series specific structure. Possible disadvantages are the high computing equipment (hardware) requirements and the amenable to little control by the researcher.\n\nFinally, the elastic net (ΕΝ) model is a penalized linear regression model that incorporates both the ridge (L2) and lasso (L1) penalties during training, where the lambda (λ) parameter controls the penalty terms (Zou and Hastie 2005). The weight assigned to each penalty, L1 and L2, is determined by a parameter α ∈ (0, 1]. The EN reduces to a simple lasso at α = 1, and it approaches the ridge regression as α (alpha) tends toward 0. It can be trained with cross-validation, as with the rest of the machine learning methodologies, and it is more advanced than a simple logistic regression, rendering EN an ideal benchmark for the more complex machine learning methodologies.\n\nOur dataset is split in two parts: the first 18 years from 2000 to 2017, almost 80% of the total sample are used as the training set (in-sample), and the last 5 years (2018–2022), the rest 20% of the total sample are used as our validation (out-of-sample) dataset to test the generalization ability of the models in unknown data. The split ratio, close to 80–20, is a common practice in machine learning models.\n\nFor every model, we employ a stratified k-fold cross-validation process (Fig. 1), to avoid overfitting. The training portion of the dataset (in-sample) is divided into k equal-sized folds (subsets) for cross-validation ensuring that each fold contains a nearly equal proportion of samples from each class (stratified). Therefore, for each different set of hyperparameters to be tested, the training and testing process are repeated k times. The testing is performed on a different fold at each iteration, with the remaining k-1 folds used for the training. The overall performance of each set of hyperparameters is calculated as the average performance over all k folds tested. The best hyperparameters for the optimal model of each methodology are presented in Table 3 in the Appendix.\n\nAn example of a threefold cross-validation procedure (Sofianos et al. 2024)\n\nStratified cross-validation is employed due to the time-series nature of the macroeconomic dataset. This ensures representative sampling across time segments, as there are periods of consistent binary outcomes.\n\n3.2 The dataset\n\nIn our study, we use the monthly US industrial production index (IPI), a measure that is widely used as a proxy for the GDP (Sofianos et al. 2021). The IPI, especially in highly industrial countries, exhibits a strong correlation with the overall economic activity as it is measured by the GDP, making IPI a good indicator of economic activity and trends. Moreover, as mentioned before, the IPI is measured in a monthly frequency instead of the quarterly observations of the GDP. Thus, the IPI is often used in empirical work as it is a good proxy of the GDP and provides larger datasets for empirical work.\n\nThe dataset consists of 32 economic and financial variables, widely recognized for their importance in the economic framework, at a monthly frequency from 2000:1 to 2022:12, and up to 18 lags of the US industrial production index (IPI), for a set of 50 explanatory variables and 276 observations. Analytically, we used the following economic and financial variables: IPI, cycle and trend components calculated with HP filter for the IPI, inflation, unemployment rate and market yield on U.S. Treasury Securities at 1, 5 and 10 years constant maturity, exchange rates of US dollar to euro, Japanese Yen and UK pound, gold spot price, Eurocoin index, exports and imports price indices, money supply as M1 and M2, Nasdaq composite, S&P 500, Dow Jones Industrial, Russel 2000, VIX index, Divisia M1/M2M/MZM/M2/ALL/M4/M3 levels, Baltic Dry index, WTI and Henry Hub Natural Gas Spot Price (Table 4). Every variable is transformed into a natural logarithm, except for those expressed in percentages. Furthermore, all data, except for cycle, trend and IPI lags, are converted into first differences. The three aforementioned variables were not transformed since they are used to calculate the target variable, thus, they may contain useful information in their original form.\n\nNext, the IPI is decomposed using a Hodrick–Prescott (1997) filter (λ = 14,400) to extract its trend and cyclical component. Our binary dependent variable (target) takes the value 1 when the cyclical component is positive, indicating an inflationary gap, and the value 0 if it is negative, indicating an unemployment gap. The forecasting horizon is one month.\n\nThe forecasting metric used for selecting the best models during the cross-validation process (training) is the accuracy = \\(\\frac{{{\\text{True}}\\;{\\text{ Positive}} + {\\text{True }}\\;{\\text{Negative}}}}{{{\\text{All}}\\;{\\text{ Positive}} + {\\text{All }}\\;{\\text{Negative}}}}\\), since the data are balanced between the two classes (53.6% and 46.4%).\n\nTime-series macroeconomic data are usually of low frequency: annual, quarterly, or at best, monthly. Traditional machine learning algorithms, when they are trained with relatively few observations, may lead to overfitting problems (Dou et al. 2023). In fact, that was the case until recently. The topic has been under scrutiny for quite some time (see Althnian et al. 2021; Chen & Ishwaran 2012; Zhou 2022). Currently, the ML methods used in this paper are considered a good choice for small datasets (see Dou et al. 2023; Xu et al. 2023; Gogas & Papadimitriou 2021; Kokol et al. 2022). This is demonstrated by numerous studies that effectively use machine learning algorithms for macroeconomic datasets to forecast economic recessions (Cicceri et al. 2020), real GDP growth (Yoon 2020; Gogas et al. 2019), unemployment rate (Katris 2019; Gogas et al. 2021), inflation rate (Hauzenberger et al. 2023), output gaps (Sofianos et al. 2021), etc.\n\n4 Empirical results\n\n4.1 Forecasting results\n\nA stratified five-fold cross-validation approach was used to optimize the hyperparameters of each model. Moreover, the SVM models were coupled with three different kernels: the linear, the radial basis function (RBF) and the polynomial kernel.\n\nIn Table 1, we present eight metrics, both for the in-sample and the out-of-sample (OOS) parts of the dataset, namely accuracy, F1-score, precision, recall, log loss, Jaccard, ROC-AUC and Kappa. We use the in-sample part to identify the best model and the OOS to test the generalization ability, thus if the models overfit. For the in-sample part, we observe that the XGBoost outperforms the competition in five, out of 8, metrics tested. For the competition, LSTM achieves higher recall, elastic net achieves lower log loss and random forest achieves higher Kappa. Thus, we can conclude that the XGBoost is the best performing model, in terms of the US output gaps forecasting. In addition to the in-sample, we also present the OOS metrics for the out-of-sample part of the dataset. We observe (a) that for the majority of metrics, the XGBoost and the elastic net models outperform the competition and (b) the XGBoost model does not overfit, thus, it has a good generalization ability. Finally in Table 2, we can observe that the XGBoost model achieves high accuracy for both classes accurately forecasting 14 out of 17 cases where there is an unemployment gap (class 0) and 41 out of 43 when there is an inflationary gap (class 1). Also, in addition to the ROC-AUC metric, the ROC plots are included in the “Appendix” for all the models as well (Figs. 3, 4, 5, 6, 7, 8, 9, 10).\n\n4.2 Variable importance measure (VIM) results\n\nIn order to shed some light on the so-called black box of the machine learning models, we produce the variable importance measure (VIM) based on the best model, the XGBoost. The VIM produces a ranking of the importance of all independent variables in forecasting the target variable. In Fig. 2, we present the 15 most important independent variables according to the VIM. These include the technical indices cycle (cyclical components) and trend, as they are calculated from the Hodrick–Prescott filter, and various lags of the industrial production index. The fact that various lags of the dependent variable appear in the top 15 independent variables according to the variable importance measure (VIM) indicates that the past values of the target variable are highly predictive of its future values. This is a common finding when trying to predict time-series data due to the inherent complex temporal structure and time dependencies. Thus, the model can learn from historical data and provide better forecasts. Moreover, the inclusion in position 2, 4 and 6 of the 18th, 17th and 16th lags, respectively, shows that there are medium-term dependencies. These lags capture significant trends and seasonalities.\n\nVariable importance measure for the top 15 variables based on XGBoost gain\n\nInflation (Inf) is the only variable, besides technical indices and lags of the industrial production index, which is amongst the 15 most informative variables. As a summary indicator of the relative demand and supply, inflation is a macroeconomic indicator that affects various macroeconomic variables including the IPI. This happens through various channels: (a) changes in the inflation rate and the corresponding prices for investment goods, services and future prices, increases the overall uncertainty in the economy which can lead to reduced investment, production and employment, (b) inflation creates pressure for wages to increase as workers try to compensate for their lost purchasing power, thus increasing productions costs and reducing the supply and (c) an active monetary policy that focuses on keeping the inflation low, will result in higher interest rates, increasing the cost of capital, reducing the overall investment demand and production.\n\nThis finding highlights a known and important link between the so-called real economy (production of goods and services), as expressed in our paper by the IPI, and the nominal side that, in our case, is expressed by the inflation feature, at least in the short run (1–18 months). It is important to clarify here that this is a forecasting model, thus, all independent variables are at period t, while the dependent variable, the production gaps are in period t + 1. The detected evidence on the interrelation and connectedness between the IPI and inflation is expected as an increase in the inflation rate at period t increases the production costs and reduces the supply of all goods at time t + 1, including the industrial goods that are measured by the IPI. These results highlight the importance of changes in the inflation rate on the deviations of the IPI from its long-run trend. As a result, we find evidence of short-term non-neutrality of inflation and in extension of money on the real side of the economy.\n\n5 Conclusion and policy implications\n\nIn this paper, we attempt to forecast unemployment and inflation gaps for the US IPI index, using a large set of economic and financial independent variables. To do so, we train five machine learning algorithms. More specifically, we used support vector machines, decision trees, random forests, XGBoost, LSTM and the elastic net logit methodology from the area of econometrics. According to the empirical results, the best overall forecasting model is the XGBoost, achieving a training accuracy of 88.45% and an out-of-sample accuracy of 91.67%.\n\nOur findings can help policymakers to establish a methodological framework for timely predictions of the output gaps. This can provide the necessary time for the designing and implementation of the appropriate mitigating policies as early as possible. The obtained results suggest that machine learning approaches, especially the tree-based models, can help to forecast the US IPI output gaps in a timely and accurate manner. Nevertheless, the econometric technique we used in this study performed well for the out-of-sample period as well. Compared to previous studies that used quarterly data and only a few explanatory variables, our results indicate that the forecasting tools, that are available to policymakers, are more efficient in terms of fit and accuracy when a large number of explanatory variables and higher data frequency (monthly instead of quarterly) is used.\n\nIn terms of variable importance, inflation proved very informative, in the short run, in addition to technical indices and past lags of the dependent variable. Inflation affects the economy through various channels, including increased uncertainty, which reduces investment and output; wage pressures, which increase production costs; and monetary policy, which raises interest rates and reduces investment demand. This underscores the strong link between inflation (a nominal indicator) and real activity, as deviations in the inflation rate have a direct impact on the IPI's deviation from its long-term trend. Inflation is of paramount importance for the global economy these days. As noted in a recent European Parliament study, “Inflation as a Global Challenge” (November 2022), “the inflation challenge today is a global phenomenon and because of the integrated global economy, domestic monetary policy can have spillover effects to other economies.” (page 3). The relationship of the output gaps with inflation and unemployment in the framework of a short-run Phillips curve macroeconomic analysis has been indicated by other studies as well (Jašová et al 2020). The above are important findings for policymakers especially for the central bank when designing and implementing the appropriate monetary policy; changes in the inflation rate can produce and forecast deviations of the real economic activity from the long-run trend.\n\nFinally, the good performance of our optimal model, based on high frequency data and the use of several explanatory variables, for the case of the US economy is encouraging for applying it in developing economies which are characterized by higher volatility and bigger exposure to external shocks. We leave this for future research.\n\nNotes\n\nKiley (2013) discusses three alternative definitions of the output gap: “the deviation of output from its long-run stochastic trend (i.e., the “Beveridge–Nelson cycle”); the deviation of output from the level consistent with current technologies and normal utilization of capital and labor input (i.e., the “production-function approach”); and the deviation of output from “flexible-price” output (i.e., its “natural rate”), page 1.\n\nOkun's law is an empirically observed relationship between unemployment and negative changes in output. According to this empirical relationship, a 1% increase in unemployment can be related to a 2% decrease in GDP, Ball et al (2017). Nevertheless, there are doubts for this empirical relationship, Lee (2000).\n\nReferences\n\nAguiar M, Gopinath G (2007) Emerging market business cycles: the cycle is the trend. J Polit Econ 115(1):69–102\n\nArticle\n  Google Scholar\n\nAlthnian A, AlSaeed D, Al-Baity H, Samha A, Dris AB, Alzakari N, Abou Elwafa A, Kurdi H (2021) Impact of dataset size on classification performance: an empirical evaluation in the medical domain. Appl Sci 11(2):796\n\nArticle\n  Google Scholar\n\nAraujo D, Pereira AM, Simões M (2020) Predicting the output gap: a comparison of machine learning models and economic techniques. J Appl Econ 23(1):47–64\n\nGoogle Scholar\n\nAssunção JB, Fernandes PA (2024) A robust method to date recessions and compute output gaps: the Portuguese case. Port Econ. https://doi.org/10.1007/s10258-024-00259-4\n\nArticle\n  Google Scholar\n\nBall L, Leigh D, Loungani P (2017) Okun’s law: Fit at 50? J Money Credit Bank 49(7):1413–1441\n\nArticle\n  Google Scholar\n\nBarigozzi M, Luciani M (2023) Measuring the output gap using large datasets. Rev Econ Stat 105(6):1500–1514\n\nArticle\n  Google Scholar\n\nBerger T, Kempa B (2011) Bayesian estimation of the output gap for a small open economy: The case of Canada. Econ Lett 112(1):107–112\n\nArticle\n  Google Scholar\n\nBerger T, Morley J, Wong B (2023) Nowcasting the output gap. J Econom 232(1):18–34\n\nArticle\n  Google Scholar\n\nBernhardsen T, Eitrheim Ø, Jore AS, Røisland Ø (2005) Real-time data for Norway: challenges for monetary policy. N Am J Econ Finance 16(3):333–349\n\nArticle\n  Google Scholar\n\nBlanchard O, Amighini A, Giavazzi F (2010) Macroeconomics: a European PERSPECTIVE. Pearson Education\n\nGoogle Scholar\n\nBreiman L (1996) Bagging predictors. Mach Learn 24(2):123–140\n\nArticle\n  Google Scholar\n\nBreiman L (2001) Random forests. Mach Learn 45:5–32\n\nArticle\n  Google Scholar\n\nBreiman L, Friedman JH, Olshen RA, Stone CJ (1984) Classification and regression trees. Wadsworth. Inc., Monterey, California, USA\n\nGoogle Scholar\n\nCamba-Mendez G, Rodriguez-Palenzuela D (2003) Assessment criteria for output gap estimates. Econ Model 20(3):529–562\n\nArticle\n  Google Scholar\n\nCampbell JY, Pflueger C, Viceira LM (2020) Macroeconomic drivers of bond and equity risks. J Polit Econ 128(8):3148–3185\n\nArticle\n  Google Scholar\n\nCayen JP, Van Norden S (2005) The reliability of Canadian output-gap estimates. N Am J Econ Finance 16(3):373–393\n\nArticle\n  Google Scholar\n\nChen T, Guestrin C (2016) Xgboost: a scalable tree boosting system. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp 785–794\n\nChen X, Ishwaran H (2012) Random forests for genomic data analysis. Genomics 99(6):323–329\n\nArticle\n  Google Scholar\n\nCicceri G, Inserra G, Limosani M (2020) A machine learning approach to forecast economic recessions—an Italian case study. Mathematics 8(2):241\n\nArticle\n  Google Scholar\n\nCortes C, Vapnik V (1995) Support-vector networks. Mach Learn 20(3):273–297\n\nArticle\n  Google Scholar\n\nDou B, Zhu Z, Merkurjev E, Ke L, Chen L, Jiang J, Zhu Y, Liu J, Zhang B, Wei GW (2023) Machine learning methods for small data challenges in molecular science. Chem Rev 123(13):8736–8780\n\nArticle\n  Google Scholar\n\nDubbert T, Kempa B (2024) Nowcasting the output gap with shadow rates. Econ Lett 236:111583\n\nArticle\n  Google Scholar\n\nDupasquier C, Guay A, St-Amant P (1999) A survey of alternative methodologies for estimating potential output and the output gap. J Macroecon 21(3):577–595\n\nArticle\n  Google Scholar\n\nGogas P, Papadimitriou T (2021) Machine learning in economics and finance. Comput Econ 57:1–4\n\nArticle\n  Google Scholar\n\nGogas P, Papadimitriou T, Chrysanthidou E (2015) Yield curve point triplets in recession forecasting. Int Finance 18(2):207–226\n\nArticle\n  Google Scholar\n\nGogas P, Papadimitriou T, Matthaiou M, Chrysanthidou E (2014) Yield curve and recession forecasting in a machine learning framework. Comput Econ 45(4):635–645\n\nArticle\n  Google Scholar\n\nGogas P, Papadimitriou T, Sofianos E (2019) Money neutrality, monetary aggregates and machine learning. Algorithms 12(7):137\n\nArticle\n  Google Scholar\n\nGogas P, Papadimitriou T, Sofianos E (2021) Forecasting unemployment in the euro area with machine learning. J Forecast 41(3):551–566\n\nArticle\n  Google Scholar\n\nGranados C, Parra-Amado D (2024) Estimating the output gap after COVID: how to address unprecedented macroeconomic variations. Econ Model 135:106711\n\nArticle\n  Google Scholar\n\nHaider A, Safdar Ullah K (2008) Estimating output gap for Pakistan economy: structural and statistical approaches (working paper)\n\nHauzenberger N, Huber F, Klieber K (2023) Real-time inflation forecasting using non-linear dimension reduction techniques. Int J Forecast 39(2):901–921\n\nArticle\n  Google Scholar\n\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780\n\nArticle\n  Google Scholar\n\nHodrick R, Prescott E (1997) Postwar US business cycles: an empirical investigation. J Money Credit Bank 29(1):1\n\nArticle\n  Google Scholar\n\nJašová M, Moessner R, Takáts E (2020) Domestic and global output gaps as inflation drivers: What does the Phillips curve tell? Econ Model 87:238–253\n\nArticle\n  Google Scholar\n\nKatris C (2019) Prediction of unemployment rates with time series and Machine Learning Techniques. Comput Econ 55(2):673–706\n\nArticle\n  Google Scholar\n\nKiley MT (2013) Output gaps. J Macroecon 37:1–18\n\nArticle\n  Google Scholar\n\nKokol P, Kokol M, Zagoranski S (2022) Machine learning on small size samples: a synthetic knowledge synthesis. Sci Prog. https://doi.org/10.1177/00368504211029777\n\nArticle\n  Google Scholar\n\nLee J (2000) The robustness of Okun’s law: Evidence from OECD countries. J Macroecon 22(2):331–356\n\nArticle\n  Google Scholar\n\nMarcellino M, Musso A (2011) The reliability of real-time estimates of the euro area output gap. Econ Model 28(4):1842–1856\n\nArticle\n  Google Scholar\n\nMorley J, Wong B (2020) Estimating and accounting for the output gap with large Bayesian vector autoregressions. J Appl Economet 35(1):1–18\n\nArticle\n  Google Scholar\n\nMouchtaris D, Sofianos E, Gogas P, Papadimitriou T (2021) Forecasting natural gas spot prices with machine learning. Energies 14(18):5782\n\nArticle\n  Google Scholar\n\nOkun AM (1963) Potential GNP: its measurement and significance. Cowles Foundation for Research in Economics at Yale University\n\nOrphanides A (2001) Monetary policy rules based on real-time data. Am Econ Rev 91(4):964–985\n\nArticle\n  Google Scholar\n\nOrphanides A, Van Norden S (2002) The unreliability of output-gap estimates in real time. Rev Econ Stat 84(4):569–583\n\nArticle\n  Google Scholar\n\nPérez-Pons M, Parra-Dominguez J, Omatu S, Herrera-Viedma E, Corchado J (2021) Machine learning and traditional econometric models: a systematic mapping study. J Artif Intell Soft Comput Res 12(2):79–100\n\nArticle\n  Google Scholar\n\nPhillips AW (1958) The relation between unemployment and the rate of change of money wage rates in the United Kingdom, 1861–1957. Economica 25(100):283–299\n\nGoogle Scholar\n\nQuast J, Wolters MH (2022) Reliable real-time output gap estimates based on a modified Hamilton filter. J Bus Econ Stat 40(1):152–168\n\nArticle\n  Google Scholar\n\nRussell SJ, Norvig P (2016) Artificial intelligence: a modern approach. Pearson\n\nSofianos E, Gogas P, Papadimitriou T (2021) Mind the gap: Forecasting euro-area output gaps with machine learning. Appl Econ Lett 29(19):1824–1828\n\nArticle\n  Google Scholar\n\nSofianos E, Zaganidis E, Papadimitriou T, Gogas P (2024) Forecasting east and west coast gasoline prices with tree-based machine learning algorithms. Energies 17(6):1296\n\nArticle\n  Google Scholar\n\nXu P, Ji X, Li M, Lu W (2023) Small data machine learning in materials science. Npj Comput Mater. https://doi.org/10.1038/s41524-023-01000-z\n\nArticle\n  Google Scholar\n\nYoon J (2020) Forecasting of real GDP growth using machine learning models: gradient boosting and Random Forest approach. Comput Econ 57(1):247–265\n\nArticle\n  Google Scholar\n\nZou H, Hastie T (2005) Regularization and variable selection via the elastic net. J R Stat Soc Ser B (Stat Methodol) 67(2):301–320\n\nArticle\n  Google Scholar\n\nZhou S (2022) An analysis of the small sample datasets based on machine learning. In: Proceedings of the 2022 6th international conference on electronic information technology and computer engineering, vol 11, pp 1654–1658\n\nDownload references\n\nFunding\n\nThe first author would like to acknowledge that this work is part of the Interdisciplinary Thematic Institute MAKErS of the ITI 2021–2028 program of the University of Strasbourg, CNRS and INSERM. It has received financial support from the IdEx Unistra (ANR-10-IDEX-0002), and from the “Programme Investissement d'Avenir” as part of the SFRI-STRAT'US project(s) (ANR-20-SFRI-0012).\n\nAuthor information\n\nAuthors and Affiliations\n\nBureau d’Economie Théorique et Appliquée (ΒΕΤΑ), University of Strasbourg, University of Lorraine, CNRS, 67000, Strasbourg, France\n\nEmmanouil Sofianos\n\nDepartment of Finance and Accounting, Rennes School of Business, 2 Rue Robert d’Arbrissel, 35065, Rennes, France\n\nChristos Alexakis\n\nDepartment of Economics, Democritus University of Thrace, Komotini, Greece\n\nPeriklis Gogas & Theophilos Papadimitriou\n\nCorresponding author\n\nCorrespondence to Christos Alexakis.\n\nEthics declarations\n\nConflict of interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nAppendix\n\nAppendix\n\nTables 3 and 4 and Figs. 3, 4, 5, 6, 7, 8, 9 and 10.\n\nROC curve for the optimal SVM model coupled with the linear kernel (out of sample)\n\nROC curve for the optimal SVM model coupled with the RBF kernel (out of sample)\n\nROC curve for the optimal SVM model coupled with the polynomial kernel (out-of-sample)\n\nROC curve for the optimal decision tree model (out of sample)\n\nROC curve for the optimal random forest model (out of sample)\n\nROC curve for the optimal XGBoost model (out of sample)\n\nROC curve for the optimal LSTM model (out-of-sample)\n\nROC curve for the optimal elastic net logit model (out-of-sample)\n\nRights and permissions\n\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nSofianos, E., Alexakis, C., Gogas, P. et al. Machine learning forecasting in the macroeconomic environment: the case of the US output gap. Econ Change Restruct 58, 9 (2025). https://doi.org/10.1007/s10644-024-09849-w\n\nDownload citation\n\nReceived\n23 April 2024\n\nAccepted\n26 December 2024\n\nPublished\n09 January 2025\n\nDOI\nhttps://doi.org/10.1007/s10644-024-09849-w\n\nShare this article\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nKeywords\n\nJEL Classification\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n212.98.144.16\n\nAmerican University of Beirut (8200692269) - Lebanese Academic Library Consortium (3002060333)\n\n© 2025 Springer Nature\n\nPDF Predictive Modeling of Financial Market Trends Using Advanced Machine ... : \nThis site can’t be reached\n\nijrpr.com refused to connect.\n\nTry:\n\n(Pdf) Machine Learning in Market Forecasting: Trends, Challenges, and ... : \nMACHINE LEARNING IN MARKET FORECASTING: TRENDS, CHALLENGES, AND FUTURE PATHWAYS IN PREDICTION MODELS\n\n2024, Journal of the Oriental Institute\n\nAbstract\n\nMachine learning (ML) techniques have emerged as promising tools for enhancing market forecasting compared to traditional methods. This research conducts a systematic literature review to delineate current trends and future trajectories in ML-driven stock market prediction studies. Peer-reviewed journal articles spanning the last twenty years were classified into four primary categories: artificial neural networks, support vector machines, genetic algorithms in conjunction with other methods, and hybrid or alternative AI approaches. Each category underwent scrutiny to unveil commonalities, distinctive perspectives, constraints, and areas necessitating further exploration. The outcomes furnish valuable insights and suggestions for forthcoming research endeavours in this field.\n\nRelated papers\n\n2021\n\nFinancial stocks values are non-linear, volatile, and chaotic, making them one of the most challenging financial time series to predict. The incentive of financial gain has led many researchers and academia to devise methods to predict the stock market, despite copious uncertainty. Because of their ability to recognize complex patterns in several applications, machine learning models are extensively researched among the most recent methods. In this paper, Support Vector Machine, Artificial Neural Networks and Case-based Reasoning for stock market prediction is surveyed. This paper also reviews sentiment analysis to highlight the behavioral trends of the stock market and its investors with the advent of technology.A generalised modelling methodology for applying machine learning techniques to the stock market is proposed in this paper .\n\nJournal of AppliedMath\n\nThis research paper investigates the use of machine learning techniques in financial markets. The paper provides a comprehensive literature review of recent research on machine learning applications in finance, including stock price prediction, financial time series forecasting, and portfolio optimization. Various machine learning techniques, such as regression analysis, decision trees, support vector machines, and deep learning, are discussed in detail, with a focus on their strengths, weaknesses, and potential applications. The paper also highlights the challenges associated with machine learning in finance, such as data quality, model interpretability, and ethical considerations. Overall, the paper demonstrates that machine learning has significant potential in finance but calls for further research to address these challenges and fully explore its potential in financial markets.\n\nElectronics\n\nWith the advent of technological marvels like global digitization, the prediction of the stock market has entered a technologically advanced era, revamping the old model of trading. With the ceaseless increase in market capitalization, stock trading has become a center of investment for many financial investors. Many analysts and researchers have developed tools and techniques that predict stock price movements and help investors in proper decision-making. Advanced trading models enable researchers to predict the market using non-traditional textual data from social platforms. The application of advanced machine learning approaches such as text data analytics and ensemble methods have greatly increased the prediction accuracies. Meanwhile, the analysis and prediction of stock markets continue to be one of the most challenging research areas due to dynamic, erratic, and chaotic data. This study explains the systematics of machine learning-based approaches for stock market prediction ...\n\nIn the USA, one of the world's largest and most liquid financial markets, the ability to anticipate market trends has deep economic implications. Precise stock market forecasting is highly instrumental for investors and analysts in making better asset allocation decisions, managing risks, and setting investment strategies. This research aimed to analyze the efficiency of some machine learning models in stock market forecast evaluation. This research project concentrated on stock market data from the USA, exploring historical price patterns, trading volumes, and relevant economic indicators to assess the performance of various machine learning models. The dataset used for this research work about predicting stock market trends is an exhaustive collection of historical stock prices, some fundamental financial indicators, and relevant news about the market, gathered from various dependable sources. Historical stock prices are retrieved from financial market databases like Yahoo Finance and Google Finance. These sources have daily records of open, high, low, and close prices, and trading volumes for thousands of publicly traded companies for extended periods, normally running into several years. The analyst selected several strategic models, namely, Random Forest, Gradient Boosting, and Logistic regression. Logistic Regression outperformed the other two models with relatively higher accuracy, while the others are just a little below. The findings of this study have implications well beyond academic curiosity into investment strategies and financial analysis. By leveraging the strengths of the best models, investors can create better-informed trading strategies. These models can also include predictive insights that give a serious edge to the risk management strategy in an investment portfolio. By using the predictive power of models like Random Forest, investors can foresee market fluctuations and adjust their portfolios to reduce potential losses.\n\nIn this study, it is aimed to compare the performances of the algorithms by predicting the movement directions of stock market indexes in developed countries by employing machine learning algorithms (MLMs) and determining the best estimation algorithm. For this purpose, the movement directions of indexes such as the NYSE 100 (the USA), NIKKEI 225 (Japan), FTSE 100 (the UK), CAC 40 (France), DAX 30 (Germany), FTSE MIB (Italy), and TSX (Canada) were estimated by employing the decision tree, random forest k-nearest neighbor, naive Bayes, logistic regression, support vector machines and artificial neural network algorithms. According to the results obtained, artificial neural networks were found to be the best algorithm for NYSE 100, FTSE 100, DAX 30 and FTSE MIB indices, while logistic regression was determined to be the best algorithm for the NIKKEI 225, CAC 40, and TSX indices. The artificial neural networks, which exhibited the highest average prediction performance, have been determined as the best prediction algorithm for the stock market indices of developed countries. It was also noted that artificial neural networks, logistic regression, and support vector machines algorithms were capable of predicting the directional movements of all indices with an accuracy rate of over 70 %.\n\nFinancial time series forecasting is a popular application of machine learning methods. Previous studies report that advanced forecasting methods predict price changes in financial markets with high accuracy and that profit can be made trading on these predictions. However, financial economists point to the informational efficiency of financial markets, which questions price predictability and opportunities for profitable trading. The objective of the paper is to resolve this contradiction. To this end, we undertake an extensive forecasting simulation, based on data from thirty-four financial indices over six years. These simulations confirm that the best machine learning methods produce more accurate forecasts than the best econometric methods. We also examine the methodological factors that impact the predictive accuracy of machine learning forecasting experiments. The results suggest that the predictability of a financial market and the feasibility of profitable model-based trading are significantly influenced by the maturity of the market, the forecasting method employed, the horizon for which it generates predictions and the methodology used to assess the model and simulate model-based trading. We also find evidence against the informational value of indicators from the field of technical analysis. Overall, we confirm that advanced forecasting methods can be used to predict price changes in some financial markets and we discuss whether these results question the prevailing view in the financial economics literature that financial markets are efficient.\n\nStock index forecasting is vital for making informed investment decisions. This paper surveys recent literature in the domain of machine learning techniques and artificial intelligence used to forecast stock market movements. The publications are categorised according to the machine learning technique used, the forecasting timeframe, the input variables used, and the evaluation techniques employed. It is found that there is a consensus between researchers stressing the importance of stock index forecasting. Artificial Neural Networks (ANNs) are identified to be the dominant machine learning technique in this area. We conclude with possible future research directions.\n\nJurnal Ilmu Komputer dan Informasi, 2021\n\nThis literature review identifies and analyzes research topic trends, types of data sets, learning algorithm, methods improvements, and frameworks used in stock exchange prediction. A total of 81 studies were investigated, which were published regarding stock predictions in the period January 2015 to June 2020 which took into account the inclusion and exclusion criteria. The literature review methodology is carried out in three major phases: review planning, implementation, and report preparation, in nine steps from defining systematic review requirements to presentation of results. Estimation or regression, clustering, association, classification, and preprocessing analysis of data sets are the five main focuses revealed in the main study of stock prediction research. The classification method gets a share of 35.80% from related studies, the estimation method is 56.79%, data analytics is 4.94%, the rest is clustering and association is 1.23%. Furthermore, the use of the technical i...\n\nVikalpa: The Journal for Decision Makers, 2021\n\nExecutive Summary Predicting stock trends in the financial market is always demanding but satisfying as well. With the growing power of computing and the recent development of graphics processing unit and tensor processing unit, analysts and researchers are applying advanced techniques such as machine learning techniques more and more to predict stock price trends. In recent years, researchers have developed several algorithms to predict stock trends. To assist investors interested in investing in the stock market, preferably for a short period, it has become necessary to review research papers dealing on machine learning and analyse the importance of their findings in the context of how stock price trends generate trading signals. In this article, to achieve the stated task, authors scrutinized more than 50 research papers focusing on various machine learning algorithms with varied levels of input variables and found that though the performance of models measured by root-mean-squar...\n\nThe paper focuses on predicting the Nifty 50 Index by using 8 Supervised Machine Learning Models. The techniques used for empirical study are Adaptive Boost (AdaBoost), k-Nearest Neighbors (kNN), Linear Regression (LR), Artificial Neural Network (ANN), Random Forest (RF), Stochastic Gradient Descent (SGD), Support Vector Machine (SVM) and Decision Trees (DT). Experiments are based on historical data of Nifty 50 Index of Indian Stock Market from 22nd April, 1996 to 16th April, 2021, which is time series data of around 25 years. During the period there were 6220 trading days excluding all the non trading days. The entire trading dataset was divided into 4 subsets of different size-25% of entire data, 50% of entire data, 75% of entire data and entire data. Each subset was further divided into 2 parts-training data and testing data. After applying 3 tests-Test on Training Data, Test on Testing Data and Cross Validation Test on each subset, the prediction performance of the used models were compared and after comparison, very interesting results were found. The evaluation results indicate that Adaptive Boost, k-Nearest Neighbors, Random Forest and Decision Trees under performed with increase in the size of data set. Linear Regression and Artificial Neural Network shown almost similar prediction results among all the models but Artificial Neural Network took more time in training and validating the model. Thereafter Support Vector Machine performed better among rest of the models but with increase in the size of data set, Stochastic Gradient Descent performed better than Support Vector Machine.\n\nInternational Journal of Computer Application, 2021\n\nThis paper tries to address the problem of stock market prediction leveraging artificial intelligence (AI) strategies. The stock market prediction can be modeled based on two principal analyses called technical and fundamental. In the technical analysis approach, the regression machine learning (ML) algorithms are employed to predict the stock price trend at the end of a business day based on the historical price data. In contrast, in the fundamental analysis, the classification ML algorithms are applied to classify the public sentiment based on news and social media. In the technical analysis, the historical price data is exploited from Yahoo Finance, and in fundamental analysis, public tweets on Twitter associated with the stock market are investigated to assess the impact of sentiments on the stock market's forecast. The results show a median performance, implying that with the current technology of AI, it is too soon to claim AI can beat the stock markets.\n\nInternational Journal of Creative Research Thoughts (IJCRT), 2023\n\nThe stock market is a complex and dynamic environment where investors and traders strive to make informed decisions to maximize profits or minimize losses. Predicting stock prices accurately has long been a challenging task due to the multitude of factors influencing the market. With the advent of machine learning techniques, researchers have attempted to leverage these methods to forecast stock prices more effectively. This thesis aims to explore the application of machine learning algorithms for stock price prediction, comparing various models and features, and assessing their performance on historical data. The research intends to contribute valuable insights into the viability and effectiveness of machine learning in the financial domain.\n\nINTERNATIONAL JOURNAL OF ADVANCES IN ENGINEERING AND MANAGEMENT, 2021\n\nToday it is globally known that there is no point in investing your money into a new venture or a start-up business. It is being observed that in Today’s time, the market has turned out to be financially volatile due to aggressive competition between the vendors for the same products in the market. Because of the various financial and economic crises in the industry, today, every person smells it risky to put the money in any ongoing business. Thus, we need such ideas and techniques that can help the market and economy grow. To secure the investments we need to move to a new Entrepreneur Gateway- “Stock Market”. The current globalization and distribution of markets have meant that companies and organizations frequently need to adapt their internal structure and operating processes, to demonstrate efficiency and effectiveness for every customer and user, especially when considering aspects that are beyond logical definitions, such as emotions. To cope with that, this work proposes an alternative to model the distributed Stock Exchange Scenario with ontologies and their futuristic predictions. The proposed model considers that each investor can invest using information obtained by communication with different traders or investors. Each investor has its knowledge represented by ontologies, which is composed of technical knowledge together with internal training states of the data to present the graph. Our preliminary results show the possibility to use ontologies as knowledge representation mechanisms for domains that consider the human emotional dimension for decision-making processes. Our objective is to identify the best possible algorithm for predicting future stock market performances. The successful prediction of the stock market will have a very positive impact on the stock market institutions and the investors also. In other words, we can say that Stock Market is the way out for every investor in Today’s time to make their money by scoring profits in the market itself. Therefore, the correct identification of algorithms for the stock market prediction model is needed so that an investor can successfully raise profits.\n\nInternational Journal of Financial Studies\n\nThe financial sector has greatly impacted the monetary well-being of consumers, traders, and financial institutions. In the current era, artificial intelligence is redefining the limits of the financial markets based on state-of-the-art machine learning and deep learning algorithms. There is extensive use of these techniques in financial instrument price prediction, market trend analysis, establishing investment opportunities, portfolio optimization, etc. Investors and traders are using machine learning and deep learning models for forecasting financial instrument movements. With the widespread adoption of AI in finance, it is imperative to summarize the recent machine learning and deep learning models, which motivated us to present this comprehensive review of the practical applications of machine learning in the financial industry. This article examines algorithms such as supervised and unsupervised machine learning algorithms, ensemble algorithms, time series analysis algorithms,...\n\nManager-TheBritishJournalof AdministrativeManagement, 2022\n\nIndustry Qualifications, The Institute of Administrative Management, UK | 141 predicted intelligently with the collaboration of the software-programmed algorithm. In order to understand this factor in more detail way, this research paper is going to investigate the role of artificial intelligence in stock market prediction by taking diversified secondary research data into the account.\n\nInternational Journal for Research in Applied Science and Engineering Technology (IJRASET), 2022\n\nStock is a curve with a lot of unknowns. The stock market has a lot of intricacy and turbulence, which makes it difficult to predict what will happen. The primary goal of the topic's argument is to forecast future market stock stability. Many researchers have looked at the future market's evolution. Data is a vital source of efficiency because stock is made up of shifting data. The efficiency of the forecast has an impact on the same probability. Machine learning has been incorporated into the picture for the deployment and prediction of training sets and data models in the latest trend of Stock Market Prediction Technologies. Machine Learning uses a variety of predictive models and algorithms to forecast and automate tasks. The focus of the paper is on the application of regression and LSTM to forecast stock prices.\n\nFrontiers of Computer Science in China, 2009\n\nForecasting is an important activity in finance. Traditionally, forecasting has been done with in-depth knowledge in finance and the market. Advances in computational intelligence have created opportunities that were never there before. Computational finance techniques, machine learning in particular, can dramatically enhance our ability to forecast. They can help us to forecast ahead of our competitors and pick out scarce opportunities. This paper explains some of the opportunities offered by computational intelligence and some of the achievements so far. It also explains the underlying technologies and explores the research horizon.\n\nInternational Journal of Advance Research, Ideas and Innovations in Technology, 2019\n\nThe goal of this review is to describe the various methods used to predict the stock market, gold price and fuel price. The following paper describes the work that was done on investigating the application of regression, SVM, ELM, ANFIS techniques on the stock market price prediction. The report describes the various technologies with their accuracy level and efficiency in the test phase. It was found that support vector regression was the most effective out of the models used, although there are opportunities to expand this research further using additional techniques to incorporate the current affairs into the prediction features.\n\nIRJET, 2022\n\nStock Market Prediction is a challenging and trending topic for researchers in recent years. Although it contains significant risk, it is frequently utilized in investment schemes that promise big returns. The returns on stocks are quite erratic. They are influenced by a number of variables, including prior stock prices, current market trends, financial news, social media, etc. There are many methods used to forecast stock value, including technical analysis, fundamental analysis, time series analysis, and statistical analysis, however none of these methods has been demonstrated to be a reliable forecasting method. In order to improve the accuracy of stock price prediction, a variety of machine learning approaches and algorithms are examined in this study.\n\nMachine Learning (ML) has grown significantly in recent years as a result of new computer technologies, but Artificial Intelligence (AI) still requires significant innovation from data scientists and engineers to advance. Artificial Intelligence (AI) is expected to become a dominant technology in the 2020s. As a result, in this work, We want to infer the intellectual growth of AI and ML in finance research by pursuing and examining the services provided by these concepts using a scoping review and an embedded review. We goose-step the five stages of the scoping review technique and Donthu. Bibliometric review method for a technical literature review. This article examines developments in AI and ML applications in the financial sector of industrialized and developing nations between 1989 and 2022. The major goal is to highlight the specifics of various research kinds that clarify the application of AI and ML in finance sector. Our research's conclusions are distilled into seven categories: Portfolio management and robot advisory are the first two, risk management and financial distress are the third, financial fraud detection and anti-money laundering are the fourth, sentiment analysis and investor behavior are the fifth, algorithmic stock market prediction and high-frequency trading are the sixth, data protection and cyber-security are the seventh, and big data analytics, blockchain, and fintech are the eighth. We also show how AI and ML research improves the financial sector now in each of these fields, as well as how these fields can offer opportunities and solutions to a wide range of financial institutions and businesses. A review of dozens of documents organized into the seven categories of AI and ML application serves as our conclusion.\n\nRelated topics\n"
    },
    "AnalyzedArticles": {
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies": {
            "Article_Summary": "The articles focus on customer churn prediction using advanced machine learning and deep learning techniques, particularly in the telecommunications, banking, insurance, and media industries. The research aims to develop more accurate predictive models that can identify customers likely to stop using a service, enabling proactive retention strategies.",
            "ML_Models": "Multi-Head Self-Attention Neural Networks, Hybrid Neural Networks (BiLSTM-CNN), Transformer-based Models"
        },
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates": {
            "Article_Summary": "The research explores advanced machine learning models for credit risk assessment, focusing on how AI and ML techniques can improve predictive accuracy by incorporating alternative data sources like social media activity, transaction history, and geolocation data. The study highlights the potential of AI models to provide more comprehensive credit risk evaluations, especially for individuals with limited traditional credit histories.",
            "ML_Models": "Random Forest, Neural Networks, LightGBM"
        },
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI": {
            "Article_Summary": "The articles discuss the transformative role of machine learning in personalized marketing, focusing on how ML enables precise customer targeting, predictive analytics, and real-time campaign optimization across digital platforms. The key theme is leveraging data-driven insights to create highly tailored consumer experiences that improve engagement and conversion rates.",
            "ML_Models": "Neural Networks, Random Forest, Support Vector Machines"
        },
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions": {
            "Article_Summary": "The research focuses on forecasting US output gaps using machine learning techniques, specifically examining deviations of industrial production index from its long-run potential output. The study uses a comprehensive dataset of 32 economic and financial variables across 276 monthly observations from 2000-2022, demonstrating the potential of machine learning in macroeconomic forecasting.",
            "ML_Models": "XGBoost, Support Vector Machines (SVM), Long Short-Term Memory (LSTM)"
        }
    },
    "Relationship": {
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies": [
            "The banking dataset contains valuable customer demographic and interaction data that can be directly used for churn prediction. The target variable 'y' likely indicates whether a customer has churned or not. The proposed ML models (Random Forest, Gradient Boosting, Logistic Regression, Neural Networks) are well-suited for this classification task as they can identify patterns in customer behavior and characteristics that precede churn. Additionally, the advanced models mentioned (Multi-Head Self-Attention Neural Networks, Hybrid Neural Networks, Transformer-based Models) can capture complex temporal patterns in customer interactions that may signal impending churn."
        ],
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates": [
            "The banking dataset contains crucial financial and demographic information that directly supports credit risk assessment models. Features like 'default', 'housing', 'loan', along with economic indicators such as 'emp_var_rate' and 'euribor3m' provide strong predictive signals for loan default probability. The ML models mentioned (XGBoost, SVM, Decision Trees, Ensemble Methods, Random Forest, Neural Networks, LightGBM) are all well-suited for classification problems like predicting the binary outcome of loan default."
        ],
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI": [
            "The banking dataset contains customer demographic information and campaign response data that directly supports personalized marketing optimization. K-Means Clustering can segment customers based on demographic attributes, Collaborative Filtering can identify similar customer response patterns, Decision Trees can determine which factors most influence campaign success, and Naive Bayes can predict conversion probability based on customer characteristics and previous campaign outcomes."
        ],
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions": [
            "The economic trend analysis models (ARIMA, LSTM, Prophet, Random Forest) can leverage the banking dataset's economic indicators to forecast market conditions. These indicators track employment rates, consumer confidence, price indices, and interest rates, which are crucial for predicting economic cycles and market trends that impact banking decisions."
        ]
    },
    "Needs": {
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies": [
            "For churn prediction, we need both categorical features (job, marital, education) and numerical features (age, duration, campaign, previous) to train classification models. The target variable 'y' would need to be binary (churned/not churned). The demographic data helps identify customer segments prone to churn, while interaction metrics (duration, campaign, previous) reveal engagement patterns. The ML models will perform binary classification to predict the probability of churn for each customer, allowing the bank to proactively implement retention strategies for high-risk clients."
        ],
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates": [
            "For credit risk assessment, we need categorical features (job, education, marital status) and numerical features (age, duration, economic indicators) to train classification models that predict the probability of default. The target variable would be 'default' or 'y' (if representing loan outcome). The models require properly encoded categorical variables, normalized numerical features, and possibly engineered features that capture interaction effects between economic indicators and personal financial status. The end goal is a binary classification model that outputs default probability, enabling risk-based pricing and approval decisions."
        ],
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI": [
            "For personalized marketing campaign optimization, we need categorical data (job, education, marital status) for customer segmentation using K-Means Clustering, binary response data ('y' column) for classification models like Decision Trees and Naive Bayes, and campaign history data (campaign, poutcome, previous) for pattern recognition. The models will require preprocessing categorical variables through one-hot encoding, and numerical features may need normalization. The primary goal is classification (predicting whether a customer will respond positively to a campaign), with secondary clustering objectives to create meaningful customer segments for targeted marketing approaches."
        ],
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions": [
            "For economic trend analysis and forecasting, we need time-series data from the economic indicators. The emp_var_rate (employment variation rate), cons_price_idx (consumer price index), cons_conf_idx (consumer confidence index), euribor3m (3-month interest rate), and nr_employed (number of employees) will serve as features, while 'month' provides the time dimension. The 'y' column could serve as a target variable or additional context. These will be used in time-series regression models like ARIMA for short-term forecasting, LSTM networks for capturing long-term dependencies, Prophet for detecting seasonality patterns, and Random Forest Regressors for handling non-linear relationships between economic indicators."
        ]
    },
    "ModelsPerTopic": {
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies": "Multi-Head Self-Attention Neural Networks, Hybrid Neural Networks (BiLSTM-CNN), Transformer-based Models",
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates": "Random Forest, Neural Networks, LightGBM",
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI": "Neural Networks, Random Forest, Support Vector Machines",
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions": "XGBoost, Support Vector Machines (SVM), Long Short-Term Memory (LSTM)"
    },
    "ML_Models1": [
        "Random Forest, Gradient Boosting, Logistic Regression, Neural Networks",
        "XGBoost, Support Vector Machines, Decision Trees, Ensemble Methods",
        "K-Means Clustering, Collaborative Filtering, Decision Trees, Naive Bayes",
        "ARIMA, LSTM Networks, Prophet, Random Forest Regressors"
    ],
    "GPT_Columns": {
        "Machine learning models employed in customer churn prediction to identify at-risk clients and improve retention strategies": [
            [
                {
                    "banking": [
                        "age",
                        "job",
                        "marital",
                        "education",
                        "duration",
                        "campaign",
                        "previous",
                        "y"
                    ]
                }
            ]
        ],
        "Machine learning models employed in credit risk assessment to optimize loan approval processes and minimize default rates": [
            [
                {
                    "banking": [
                        "default",
                        "housing",
                        "loan",
                        "age",
                        "job",
                        "education",
                        "income",
                        "credit_history"
                    ]
                }
            ]
        ],
        "Machine learning models employed in personalized marketing campaign optimization to increase conversion rates and ROI": [
            [
                {
                    "banking": [
                        "age",
                        "job",
                        "education",
                        "marital",
                        "housing",
                        "campaign",
                        "poutcome"
                    ]
                }
            ]
        ],
        "Machine learning models employed in economic trend analysis to forecast market conditions and guide strategic banking decisions": [
            [
                {
                    "banking": [
                        "emp_var_rate",
                        "cons_price_idx",
                        "cons_conf_idx",
                        "euribor3m",
                        "nr_employed",
                        "month",
                        "y"
                    ]
                }
            ]
        ]
    }
}