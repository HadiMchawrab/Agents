{
    "tables": [
        {
            "banking": [
                "age",
                "job",
                "marital",
                "education",
                "default",
                "housing",
                "loan",
                "contact",
                "month",
                "day_of_week",
                "duration",
                "campaign",
                "pdays",
                "previous",
                "poutcome",
                "emp_var_rate",
                "cons_price_idx",
                "cons_conf_idx",
                "euribor3m",
                "nr_employed",
                "y"
            ]
        },
        {
            "data": [
                "Bankrupt?",
                " ROA(C) before interest and depreciation before interest",
                " ROA(A) before interest and % after tax",
                " ROA(B) before interest and depreciation after tax",
                " Operating Gross Margin",
                " Realized Sales Gross Margin",
                " Operating Profit Rate",
                " Pre-tax net Interest Rate",
                " After-tax net Interest Rate",
                " Non-industry income and expenditure/revenue",
                " Continuous interest rate (after tax)",
                " Operating Expense Rate",
                " Research and development expense rate",
                " Cash flow rate",
                " Interest-bearing debt interest rate",
                " Tax rate (A)",
                " Net Value Per Share (B)",
                " Net Value Per Share (A)",
                " Net Value Per Share (C)",
                " Persistent EPS in the Last Four Seasons",
                " Cash Flow Per Share",
                " Revenue Per Share (Yuan ¥)",
                " Operating Profit Per Share (Yuan ¥)",
                " Per Share Net profit before tax (Yuan ¥)",
                " Realized Sales Gross Profit Growth Rate",
                " Operating Profit Growth Rate",
                " After-tax Net Profit Growth Rate",
                " Regular Net Profit Growth Rate",
                " Continuous Net Profit Growth Rate",
                " Total Asset Growth Rate",
                " Net Value Growth Rate",
                " Total Asset Return Growth Rate Ratio",
                " Cash Reinvestment %",
                " Current Ratio",
                " Quick Ratio",
                " Interest Expense Ratio",
                " Total debt/Total net worth",
                " Debt ratio %",
                " Net worth/Assets",
                " Long-term fund suitability ratio (A)",
                " Borrowing dependency",
                " Contingent liabilities/Net worth",
                " Operating profit/Paid-in capital",
                " Net profit before tax/Paid-in capital",
                " Inventory and accounts receivable/Net value",
                " Total Asset Turnover",
                " Accounts Receivable Turnover",
                " Average Collection Days",
                " Inventory Turnover Rate (times)",
                " Fixed Assets Turnover Frequency",
                " Net Worth Turnover Rate (times)",
                " Revenue per person",
                " Operating profit per person",
                " Allocation rate per person",
                " Working Capital to Total Assets",
                " Quick Assets/Total Assets",
                " Current Assets/Total Assets",
                " Cash/Total Assets",
                " Quick Assets/Current Liability",
                " Cash/Current Liability",
                " Current Liability to Assets",
                " Operating Funds to Liability",
                " Inventory/Working Capital",
                " Inventory/Current Liability",
                " Current Liabilities/Liability",
                " Working Capital/Equity",
                " Current Liabilities/Equity",
                " Long-term Liability to Current Assets",
                " Retained Earnings to Total Assets",
                " Total income/Total expense",
                " Total expense/Assets",
                " Current Asset Turnover Rate",
                " Quick Asset Turnover Rate",
                " Working capitcal Turnover Rate",
                " Cash Turnover Rate",
                " Cash Flow to Sales",
                " Fixed Assets to Assets",
                " Current Liability to Liability",
                " Current Liability to Equity",
                " Equity to Long-term Liability",
                " Cash Flow to Total Assets",
                " Cash Flow to Liability",
                " CFO to Assets",
                " Cash Flow to Equity",
                " Current Liability to Current Assets",
                " Liability-Assets Flag",
                " Net Income to Total Assets",
                " Total assets to GNP price",
                " No-credit Interval",
                " Gross Profit to Sales",
                " Net Income to Stockholder's Equity",
                " Liability to Equity",
                " Degree of Financial Leverage (DFL)",
                " Interest Coverage Ratio (Interest expense to EBIT)",
                " Net Income Flag",
                " Equity to Liability"
            ]
        }
    ],
    "analyzed_topics": [
        {
            "topic": "Machine learning models employed in Credit Risk Assessment",
            "ML_Models": "Logistic Regression, Random Forest, Gradient Boosting, Neural Networks",
            "reasoning": "The banking dataset contains features like 'default', 'loan', 'housing', while the data table has bankruptcy indicators and financial ratios. These can be used to predict credit risk, loan defaults, and bankruptcy probability using classification models that analyze financial health metrics, debt ratios, and payment history."
        },
        {
            "topic": "Machine learning models employed in Customer Segmentation and Targeting",
            "ML_Models": "K-means Clustering, Hierarchical Clustering, DBSCAN, Decision Trees",
            "reasoning": "The banking dataset includes demographic information ('age', 'job', 'marital', 'education') and contact history ('contact', 'campaign', 'previous'). These features can be used to segment customers into groups with similar characteristics and behaviors, enabling personalized marketing strategies and product recommendations."
        },
        {
            "topic": "Machine learning models employed in Financial Performance Prediction",
            "ML_Models": "XGBoost, LSTM Networks, Support Vector Regression, Random Forest Regression",
            "reasoning": "The data table contains numerous financial ratios and performance indicators like ROA, operating margins, growth rates, and asset turnover. These metrics can be used to forecast future financial performance, predict stock prices, and identify trends in company valuation using time-series forecasting and regression models."
        },
        {
            "topic": "Machine learning models employed in Fraud Detection and Anomaly Detection",
            "ML_Models": "Isolation Forest, One-Class SVM, Autoencoders, Ensemble Methods",
            "reasoning": "Both datasets contain financial indicators and transaction-related features that can be used to identify unusual patterns or outliers. Models can analyze cash flow patterns, debt ratios, and account activities to flag potential fraudulent activities, accounting irregularities, or suspicious transactions that deviate from normal behavior."
        }
    ],
    "csv_files": [
        "csv_test/banking.csv",
        "csv_test/data.csv"
    ],
    "topic": [
        "Machine learning models employed in Credit Risk Assessment",
        "Machine learning models employed in Customer Segmentation and Targeting",
        "Machine learning models employed in Financial Performance Prediction",
        "Machine learning models employed in Fraud Detection and Anomaly Detection"
    ],
    "ScrapedArticles": {
        "Machine learning models employed in Credit Risk Assessment": "Machine learning-driven credit risk: a systemic review : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nMachine learning-driven credit risk: a systemic review\n\nYou have full access to this\nopen access\narticle\n\n41k Accesses\n\n62 Citations\n\n3 Altmetric\n\nExplore all metrics\n\nAbstract\n\nCredit risk assessment is at the core of modern economies. Traditionally, it is measured by statistical methods and manual auditing. Recent advances in financial artificial intelligence stemmed from a new wave of machine learning (ML)-driven credit risk models that gained tremendous attention from both industry and academia. In this paper, we systematically review a series of major research contributions (76 papers) over the past eight years using statistical, machine learning and deep learning techniques to address the problems of credit risk. Specifically, we propose a novel classification methodology for ML-driven credit risk algorithms and their performance ranking using public datasets. We further discuss the challenges including data imbalance, dataset inconsistency, model transparency, and inadequate utilization of deep learning models. The results of our review show that: 1) most deep learning models outperform classic machine learning and statistical algorithms in credit risk estimation, and 2) ensemble methods provide higher accuracy compared with single models. Finally, we present summary tables in terms of datasets and proposed models.\n\nSimilar content being viewed by others\n\nMachine learning techniques for credit risk evaluation: a systematic literature review\n\nHow Can Credit Scoring Benefit from Machine Learning? SWOT Analysis\n\nDeep Learning and Machine Learning Techniques for Credit Scoring: A Review\n\nExplore related subjects\n\nAvoid common mistakes on your manuscript.\n\n1 Introduction\n\nMachine learning advances heavily affected industry and academia in the past decades, ultimately transforming people’s daily life. Artificial Intelligence (AI) has been applied to almost every human activity, including pattern recognition, image classification, business, agriculture, transportation, and finance. This paper focuses on machine learning applied to finance and credit risk estimation. Modern financial systems rely on credit and trust. Credit risk is a fundamental parameter that measures and predicts the default probabilities of a debtor. The correct estimation of credit risk is paramount for the entire system. Failing in the credit risk estimation can lead to systemic failures such as the sub-prime crisis of 2008. Consequently, lenders devote large amounts of resources to predict the creditworthiness of consumers and companies to develop appropriate lending strategies that minimize their risks. Historically, credit risk approaches use statistical methods such as Linear Discriminant Analysis [1] and Logistic Regression [2]. These methods, however, do not easily handle large datasets.\n\nAdvances in computing power and availability of large credit datasets paved the way to AI-Driven credit risk estimation algorithms such as traditional machine learning and deep learning. Conventional machine learning techniques, e.g., k-Nearest Neighbor [3], Random Forest [4] and Support Vector Machines [5], are more effective and flexible than statistical methods. In particular, the vital branch of machine learning-deep learning techniques [6] applied to large credit risk data lake outperform their predecessors both in accuracy and efficiency.\n\nThis paper presents a systemic review of credit risk estimation algorithms. It analyzes both the major statistical approaches and AI-based techniques with a critical spirit. The aim is to provide a comprehensive overview of the current leading credit risk estimation technology, providing justification and connections between past and present works. This work proposes a novel taxonomy combining finance with machine learning techniques. In addition, this work ranks their performance in terms of accuracy and costs. This paper also discusses the challenges and possible solutions in terms of four aspects: data imbalance, dataset inconsistency, model transparency, and inadequate utilization of deep learning methods.\n\nThe remainder of the paper is organized as follows: the survey methodology will be discussed in Sect. 2. Section 3 introduces the principles of statistical learning, machine learning and deep learning. Section 4 analyzes credit risk-related applications in detail. In Sect. 5, presented algorithms are discussed and ranked by their performance against public datasets. Finally, results and current challenges are summarized in Sect. 6; while Sect. 7 concludes this work.\n\n2 Survey methodology\n\n2.1 Methodology\n\nWe applied PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Fig. 1) reviewing methodology in our paper. First, we adopted five searching platforms for our investigation: Google Scholar, ACM, IEEEXplore, Springerlink, and ScienceDirect. We used the keywords “machine learning” or “deep learning” combined with “credit risk” while searching. We got 2400 articles in total. Then, we applied a filtering algorithm considering the trade-off between publication year and citations to proceed. After removing 1400 duplicate records, 800 ineligible records, and 76 incomplete articles, we obtained 124 screened records. Based on the relevance, we excluded 24 articles less related to the topic. After manually checking whether the paper has clear evaluation metrics, we further excluded another 24 papers. Finally, we kept 76 studies in terms of the relevancy to the research topic, precision of evaluation metrics, publication time, and number of citations as our source of reviewing.\n\nFigure1 depicts the PRISMA flow diagram\n\nThe PRISMA flow diagram. From: Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 2021;372:n71. doi:https://doi.org/10.1136/bmj.n71\n\n2.2 Inclusion and exclusion criteria\n\nIn this paper, we select three inclusion criteria: (1) the relevance of research topic, (2) the precision of evaluation metrics, (3) the publication year and citations. Moreover, the papers will be excluded if they are duplicated, incomplete, too early, low-related with the topic, having no clear metrics or comparatively low citations.\n\nWe show the whole workflow of the selection process in Fig. 2.\n\nThe workflow of selecting papers\n\n2.3 The datasets and approaches of the reviewed articles\n\nThe mainly used datasets by the papers under review are German and Australian public credit data from the UCI Machine Learning repository [7,8,9,10,11]. In addition, there exist some researches that discover and mine their own data. For example, Chee Kian Leong (2015) uses data from a firm in Singapore [12]. Authors in [13,14,15,16,17,18,19,20,21,22,23,24,25,26] all employ their unique dataset. Those articles mainly emphasize the significance and the veracity of the original data.\n\nWe discuss the principles and application of the overall machine learning approaches. The traditional machine learning models for credit risk contain Support Vector Machines (SVMs) [5], k-Nearest Neighbor (k-NN) [3], Random Forests (RFs) [4], Decision Trees (DTs) [27,28,29], AdaBoost [30], Extreme Gradient Boost (XGBoost) [31], Stochastic Gradient Boosting (SGB) [32], Bagging [33], Extreme Learning Machine (ELM) [34] and GA (Genetic Algorithm) [35]. Neural network models generally belong to deep learning methods. Most of them include Convolutional Neural Networks (CNNs) [36], Deep Belief Neural Networks (DBNs) [37], Artificial Neural Networks (ANNs) [38], LSTM (Long Short-Term Memory) [39], Restricted Boltzmann Machines (RBMs) [40], Deep Multi-Layer Perceptron (DMLP) [41], and Recurrent Neural Networks (RNNs) [42].\n\nSummary tables and bar charts regarding all the methods of the reviewed papers are provided.\n\n2.4 Taxonomy\n\nThe taxonomy is shown as Fig. 3. We can divide it into two parts: the first is regarding computing technology and the second is credit risk application domain. The two parts are further categorized into subsections. These two parts are connected and fused with each other. All the right-side sub-domains include the left-side techniques, and all the techniques can be applied in the financial domains.\n\nThe taxonomy of selecting paper\n\n3 Computing approaches\n\nThis section briefly introduces three main computing techniques used for credit analysis, i.e., statistical learning, machine learning and deep learning, each of which has its own characteristics and similar principles. Statistical approaches are traditional ways to classify a customer’s or enterprise’s credit behavior. However, with the rapid development of artificial intelligence, machine learning and deep learning gradually took the place of statistical analysis.\n\n3.1 Statistical learning approaches\n\nWe divide the statistical approaches into three subsections—discriminant analysis, logistic regression and Bayesian related model.\n\nLDA (Linear Discriminant Analysis) is a classic technique for predicting groups of samples [1]. It aims at generating characteristics that can separate binary variables.\n\nLogistic regression is a classification algorithm which uses the logistic sigmoid function to squash the output of the linear function into the interval (0, 1) and interpret that value as a probability [6].\n\nNaïve Bayes methods are statistical learning algorithms that apply Bayes’ theorem with the “naïve” assumption of conditional independence between every pair of features if the class variable is given [43]. A Bayesian network is a probabilistic model based on graphs. It measures the conditional dependence structure of a series of random variables that comply with the Bayes theorem [44].\n\n3.2 Machine learning methods\n\nWe review a series of conventional machine learning algorithms that can be applied well in credit risk area.\n\nk-NN [3] belongs to classification methods that appoint the class of the majority of the k nearest neighbors of an input variable x to it in a dataset [3].\n\nTree-related methods show their effects in credit risk domain. Typical examples include DTs [27,28,29], Random Forests (RFs) [4], Classification and Regression Trees (CART) [45], C4.5 [46], and Diverse Ensemble Creation by Oppositional Relabeling of Artificial Training Examples (DECORATE) [47].\n\nSupport Vector Machine [5] implements a hyperplane (a decision boundary) which can separate classes in a high dimensional feature space. It outputs a class identity according to whether \\(w^{T} + b\\) is positive or not [6]. Here, w stands for the margin between the negative and positive hyperplane while b means the bias.\n\nBoosting is an ensemble method that combines the individual models to gain higher capacity [6]. Adaptive Boosting (AdaBoost) belongs to the most popular boosting algorithms as the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances [48]. SGB (Stochastic Gradient Boosting) [32] can add incorporate randomness as an integral part when created from Gradient Boosting algorithm. This family of algorithms includes Extreme Gradient Boost (XGBoost) [31] similar to Gradient boosting. However, it includes the decision trees built in parallel rather than in a series manners.\n\nBagging is an ensemble method which contains the same kind of model, training algorithm, and objective function in recycling [6]. It is also known as bootstrap aggregation [33].\n\nExtreme learning machine [34] was developed by Guang-Bin Huang in 2006. It targets at building single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and outputs the weights of SLFNs logically [34].\n\nGenetic algorithm (GA) [35] is a heuristic search algorithm to solve searching and optimization problems. It first generates an initial population, then obtains a fitness score for all individuals in it. Individuals are selected for the reproduction of offspring [49].\n\n3.3 Deep learning methods\n\nDeep learning has deeper layers and more units within a layer compared with traditional machine learning. It can represent functions of increasing complexity [6]. In this section, we review some crucial deep learning methods used in credit risk.\n\nArtificial Neural Networks [38] were inspired by a biological neural network system. It has three layers generally: an input, hidden and output layers. Given a feature vector x, the ANN outputs \\(\\hat{y}\\) through the following formula [50]:\n\nwhere \\(\\alpha _{0}^{(1)}, \\alpha ^{(1)}, \\alpha _{0}^{(2)}, \\alpha ^{(2)}\\) are weights and \\(a_{1},a_{2}\\) are activation functions.\n\nRecurrent neural networks (RNNs) are a family of neural networks for processing sequential data [6]. They can better handle sequential information rather than the spatial data which Convolutional Neural Networks (CNNs) can effectively process. RNNs introduce state variables to store past information as well as the current inputs, both determining the current outputs [51].\n\nLSTM [39] was first developed to produce paths in which the gradient flows for long durations [6]. It is the variant of Recurrent Neural Networks (RNNs). Compared with traditional RNNs, it can solve gradient disappearance and explosion in the long-term sequence process.\n\nDMLP is a Multi-Layer Perceptron with multiple hidden layers. It is a directed neural network. In order to update the weights, the loss function for DMLP uses Softmax and Cross-Entropy. [50].\n\nLeCun et al. first introduced CNNs [36] which were widely applied in image processing, voice recognition, automatic QA systems, and many other computing fields. CNNs consist of an input layer, convolutional layers, pooling layers and fully connected layers.\n\nThe convolution function is following [6]:\n\nwhere w(a) is a weighing function where a is the age of a measurement.\n\nHinton et al. introduced DBNs [37] which are a class of deep neural networks. A typical DBN consists of several hidden layers of Restricted Boltzmann Machines. An output of a lower level RBM can be regarded as input of the higher level RBM [50].\n\nRBMs are some of the most common building blocks of deep probabilistic models. They are undirected probabilistic graphical models containing a layer of observable variables and a single layer of latent variables [6].\n\nIt has the similar energy function like Boltzmann Machine. The function is as follows [50]:\n\nwhere \\(a_{i}, b_{j}\\) are biases for binary variables \\(v_{i}, h_{j}\\), and \\(\\alpha _{ij}\\) are weights between j and i.\n\n4 Credit risk application with computing algorithms\n\nIn the past decades, a lot of scholars have employed various computing algorithms and models to solve credit risk prediction and assessment. Binary classification problem is the most fundamental and essential computing technique in credit risk scenarios. In this section, we divide the related studies into two groups from the perspective of finance: consumer and corporate.\n\n4.1 Consumer credit risk\n\nConsumer credit scoring is one of the main parts of credit risk management. It is a kind of system which determines the creditworthiness of a customer based on his/her past credit situation. In [52], the Bayesian network method is improved to find out whether there is a change in credit risk profiles. Numerous approaches have been implemented in this domain. Typical examples include Extreme learning machines (ELM) [7], Ensemble of classifiers [8], Bayesian networks [12], Deep Genetic Cascade Ensembles [11], a hybrid model with convolutional neural networks and Relief algorithm [22], Genetic Programming [53], feature selection [54], RNN [55], ensemble of supervised learning and statistical learning [56], Radial Basis Function [57], TreeSHAP method for Stochastic Gradient Boosting [58], a real-time binary classification model [59], CNN [60], MLP [61], etc. The authors in [62] compared the traditional and machine learning models in the credit score evaluation area.\n\nPredicting a consumer’s future credit condition is also valuable for credit risk and quantitative analysis. The authors in [19] conduct a comparison between deep learning techniques and other machine learning methods. It proves that XGBoost overperforms traditional machine learning techniques like Logistic regression, SVM and Random Forest. It turns out that a hybrid model is capable of predicting credit risk. In [63], a unique model named TRUST (Trainable Undersampling with SELF Training) was proved to be decisive.\n\nCCF (Credit card fraud) is a specific crime in the banking system and becoming a substantially growing problem worldwide [64]. Detection of it helps to control the credit risk in banking security issue. A novel framework called DEAL (Deep Ensemble Algorithm) is employed [64]. Recurrent Neural Network (RNN) [65], Boosted Decision Tree [66,67,68], a deep learning structure with an advanced feature engineering [69] display a satisfactory performance. The authors in [70] conduct a comparison among Deep Learning, Logistic Regression and Gradient Boosted Tree. In [71,72,73], the authors implemented LR, SVM, k-NN, NB, RF, DT, MLP methods and found that they were all robust while tree-related models have the best performance. By using an auto-encoder, the authors in [70] create features with domain expertise. It is proved to be an improvement in predictive power. In [74], Visual Analytics were used to help reduce the incidence of false positives.\n\n4.2 Corporate credit risk\n\nCredit risk in corporate aspect also demands the necessity of machine learning and deep learning.\n\nDeep learning plays a significant role in corporate credit rating and assessment. Two-layer additive risk model [13], Artificial Neural Network [15], LSTM and AdaBoost [9], denoising-based neural network [21], deep belief network [14, 75], probabilistic neural network (PNN) [76], Genetic algorithm with neural network [77], CNN [78] all show their great competency in estimation and assessing.\n\nOnline supply chain financial risk can be controlled by proper estimation and assessment. The authors in [24] construct a deep belief network based on Restricted Boltzmann Machine and classifier SOFTMAX. The dataset came from annual financial reports of Chinese notable companies. The model shows an accuracy which is far beyond SVM and Logistic Regression. In [79, 80], SVM and XGBoost were more accurate than LR and NB in supply chain fraud detection.\n\nBecause of the remaining effect of Global Financial Crisis in 2008, a large number of corporations are under the threats of bankruptcy. Neural networks can help those in danger detect the early signals of collapsing. A series of machine learning methods are enforced to predict bankruptcy [16, 81]. Bagging, boosting and random forest have the best performance. In [10], random forest trees are proven to outperform most of the other machine learning models.\n\nIn [26], statistical methods—probit models and CART (Classification And Regression Trees), machine learning methods—Neural Networks and k-NN are applied and compared to make a prediction in financial intermediary domain.\n\nInternational finance, which has an important branch peer-to-peer lending, once flourished in the past decades. Normally, it has greater credit risk than common financial industry. Neural Networks [17, 82, 83], Attention Mechanism LSTM [20], word embedding models [84], Ensemble Learning Method [85, 86], Restricted Boltzmann Machine (RBM) [87] all exert their impact on predicting the risk of P2P industry.\n\nMortgage credit and prepayment risk are vital issues for measuring a borrower’s behavior in real estate financial industry. In [88], the authors find a highly nonlinear relationship between a borrower’s behavior and risk factors with deep neural networks. Deep learning is proved to be effective in measuring mortgage risks.\n\nBig data technology triggered the massive transformation of finance. According to Denis Ostapchenya, a financial expert, big data in banking can be deployed to assess risks in the procedure of trading stocks or checking the creditworthiness of a loan applicant. Big Data analysis also accelerates and ensures the processes which require compliance verification, auditing, and reporting [89]. In the credit risk domain, the combination of machine learning, big data and specific financial techniques has achieved satisfactory results. BP neural networks, genetic algorithm [90], logistic regression with XGBoost and AdaBoost [91, 92], Synthetic Minority Oversampling Technique algorithm [93], integrated and mixed models [94, 95] all play a vital role in predicting and classifying credit risk assessment.\n\n5 Performance ranking of machine learning techniques\n\n5.1 Data imbalance\n\nGenerally, data imbalance often occurs in the credit risk classification due to the huge differences of the number of good borrowers and bad borrowers. SMOTE [93] is one of the most widely used approach to address this problem. In addition, over-sampling and under-sampling techniques are also employed. Nevertheless, data imbalance has been severely underestimated in many credit risk researches.\n\n5.2 Evaluation metrics\n\nIn this review, we select ACC (accuracy) and AUC as main metrics for performance evaluation. The metric accuracy (ACC) is calculated through correctly classified values divided by the total number of samples while the metric AUC is the area under ROC curve which is also a measurement of precision of classification.\n\nACC is calculated as follows:\n\nwhere TP denotes true positive, TN stands for true negative, FP means false positive, FN denotes false negative.\n\nAUC [96] can be expressed as the following formula:\n\n5.3 Ranking of techniques\n\nThere hasn’t been consensus on the specific ranking of each machine learning technique. In this section, we propose our own thoughts that is based on a thorough and objective investigation. Because the open-source databases of German and Australian credit risk have uniform judging criteria, we select the common techniques appearing in the related literature to compare their performances. We use the mean of each metric of the methods. The bar charts are shown in Fig. 4.\n\nThe accuracy from German credit data\n\nThe graph shows that machine learning methods have a higher accuracy universally than statistical methods. Bagging has the highest AUC and Random Forest (RF) has the highest ACC. Logistic Regression is the most powerful tool among the statistical methods in the credit risk classification. Naïve Bayes (NB), k-Nearest Neighbor (k-NN) and Classification and Regression Trees (CART) have comparatively low rankings regarding German credit dataset. The detailed ranking results are shown in Table 1.\n\nSimilarly, we sort and calculate the mean ACC and AUC appearing in the Australian Credit Risk dataset. The result is shown in Fig. 5. It turns out that the accuracy in the Australian dataset exceeds the one in the German dataset because the imbalanced ratio of German dataset is comparatively higher. The best AUC is contributed by ANN. The best ACC belongs to ELM method.\n\nThe accuracy from Australian credit data\n\nFrom Fig. 5, we can conclude that deep learning methods are more potent than traditional machine learning and statistical methods from the above graph. The specific ranking is shown in Table 2.\n\nIn short, deep learning techniques have better performance regarding public credit risk data sets compared with machine learning and statistical learning methods based on ACC and AUC values.\n\n6 Discussions\n\n6.1 Existing survey papers\n\nIn this section, we review several typical surveys published recently. In [97], the majority of machine learning methods and data imbalance are discussed, but the discussion only focuses on the card defraud domain and the authors didn’t consider the synergetic effects of models. Xolani Dastile, Turgay Celik et al. [50] had a thorough investigation of systematic machine learning and its application in credit risk. Nevertheless, the role of deep learning models in credit risk hasn’t been fully expressed. In [98], principles of machine learning methods are not clearly displayed. In [99], abundant bibliography is shown. However, the structure of the paper is not balanced. Siddharth Bhatore et al. [100] displayed an intact review of machine learning in credit risk and showed clear graphs, but they ignored the limitation of datasets in some sense. In [101], similar problems with [98] occurred.\n\nIn our work, we give a comprehensive analysis and provide detailed comparison among methods, hoping to improve existing results.\n\n6.2 The summary tables\n\nWe summarize our survey in the following four tables. A whole summary table is shown in S1 Table.\n\nTable 3 shows that LR and Bayesian models are the mostly used ones among the statistical learning techniques.\n\nAs shown in Table 4, we find that AdaBoost, SVM, Tree-related, k-NN and Bagging are the primarily implemented models among the machine learning techniques while SGB (Stochastic Gradient Boosting) and ELM (Extreme Learning Machine) have a relatively low citation.\n\nTable 5 shows that ANN and MLP are the widely used deep learning models. Moreover, nearly all of the listed deep learning methods have a balanced citation distribution.\n\nWe list several important works containing unique datasets as Table 6. Almost all of them deploy their own computing models that improve the original algorithms. The results show that the models are effective.\n\n6.3 Challenges\n\nWe summarize four major challenges in the research of machine learning-driven credit risk. First, data imbalance in credit risk is quite severe. Although several approaches such as over-sampling and under-sampling (usually chosen to under-sample the majority) have been proposed to solve this problem, the results are still unsatisfactory in terms of both effectiveness and efficiency. Second, the shortage of benchmark datasets is serious. Most existing works use private datasets, thus the results of performance comparison cannot be fair enough. Third, most machine learning models are black boxes since they are generally not transparent. Information transparency should be noticed. Fourth, the application of deep learning models is still limited in credit risk.\n\nThese four challenges are what we are supposed to overcome in future work. We hope more and more deep advanced models will emerge in this area.\n\n7 Conclusions\n\nIn conclusion, we have witnessed an overall application of machine learning as well as deep learning methods in credit risk area. We build a taxonomy which links computing algorithms and finance. We also briefly introduce the principles of statistical and machine learning approaches. As for public datasets, we rank them according to their accuracy. In addition, we list some of the accuracy for the private and unique datasets. A checklist is provided in S2 Table.\n\nThe results show that deep learning methods are more powerful than the traditional machine learning and statistical approaches although they haven’t been fully employed. Also, the conclusion that ensembles of several methods outperform a single one has been proved in some of the related researches [9, 11, 75, 81, 103, 104].\n\nIn the future, we are supposed to find proper solutions to the challenges mentioned above. First, we should find new ways to tackle the problem of imbalanced data. Second, we will find a comprehensive judging criterion to make up for the default of specific methods and the inconsistency of datasets. Third, we should seek improvements in machine learning methods in tackling data transparency. Fourth, we should try our new and improved deep learning models in credit risk classification problem.\n\nMoreover, in recent years, some authors proposed a series of representative nature-inspired metaheuristic algorithms such as (monarch butterfly optimization) MBO [105], (earthworm optimization algorithm) EOA [106], (elephant herding optimization) EHO [107], (moth search algorithm) MS [108], (Slime mould algorithm) SMA [109], (hunger games search) HGS [110], (colony predation algorithm) CPA [111] and (Harris hawks optimization) HHO [112]. They can also be applied in credit risk prediction. Besides, (Runge Kutta optimizer) RUN [113] is an algorithm that excludes the general characteristics of metaphor among other metaheuristic algorithms. Generally, those novel intelligent computational algorithms haven’t been sufficiently applied in finance due to the complexity and instability of risk related problems. However, they may have promising results when the analysis tools become more mature.\n\nLast but not least, big data technology and its application in credit risk is a newly booming area. We will explore them and utilize the vast amounts and efficiency of big data tools like MapReduce and Hadoop platform to get better results.\n\nAvailability of data and materials\n\nNot applicable.\n\nReferences\n\nMoo-Young M (2019) Comprehensive biotechnology. Elsevier, Amsterdam\n\nGoogle Scholar\n\nCox DR (1958) The regression analysis of binary sequences. J R Stat Soc Ser B 20(2):215–232\n\nMathSciNet\n  MATH\n  Google Scholar\n\nCover T, Hart P (1967) Nearest neighbor pattern classification. IEEE Trans Inf Theory 13(1):21–27\n\nArticle\n  MATH\n  Google Scholar\n\nBreiman L (2001) Random forests. Mach Learn 45(1):5–32\n\nArticle\n  MATH\n  Google Scholar\n\nCortes C, Vapnik V (1995) Support-vector networks. Mach Learn 20(3):273–297\n\nArticle\n  MATH\n  Google Scholar\n\nGoodfellow I, Bengio Y, Courville A (2016) Deep Learn. MIT press, Cambridge\n\nMATH\n  Google Scholar\n\nBequé A, Lessmann S (2017) Extreme learning machines for credit scoring: An empirical evaluation. Expert Syst Appl 86:42–53\n\nArticle\n  Google Scholar\n\nAbellán J, Castellano JG (2017) A comparative study on base classifiers in ensemble methods for credit scoring. Expert Syst Appl 73:1–10\n\nArticle\n  Google Scholar\n\nShen F, Zhao X, Kou G et al (2021) A new deep learning ensemble credit risk evaluation model with an improved synthetic minority oversampling technique. Appl Soft Comput 98(106):852\n\nGoogle Scholar\n\nGhatasheh N (2014) Business analytics using random forest trees for credit risk prediction: a comparison study. Int J Adv Sci Technol 72(2014):19–30\n\nArticle\n  Google Scholar\n\nPławiak P, Abdar M, Acharya UR (2019) Application of new deep genetic cascade ensemble of svm classifiers to predict the australian credit scoring. Appl Soft Comput 84(105):740\n\nGoogle Scholar\n\nLeong CK (2016) Credit risk scoring with bayesian network models. Comput Econ 47(3):423–446\n\nArticle\n  Google Scholar\n\nChen C, Lin K, Rudin C, et al (2018) An interpretable model with globally consistent explanations for credit risk. arXiv preprint arXiv:1811.12615\n\nLuo C, Wu D, Wu D (2017) A deep learning approach for credit scoring using credit default swaps. Eng Appl Artif Intell 65:465–470\n\nArticle\n  Google Scholar\n\nAngelini E, Di Tollo G, Roli A (2008) A neural network approach for credit risk evaluation. Quarte Rev Econ Finan 48(4):733–755\n\nArticle\n  Google Scholar\n\nBarboza F, Kimura H, Altman E (2017) Machine learning models and bankruptcy prediction. Expert Syst Appl 83:405–417\n\nArticle\n  Google Scholar\n\nByanjankar A, Heikkilä M, Mezei J (2015) Predicting credit risk in peer-to-peer lending: A neural network approach. In: 2015 IEEE symposium series on computational intelligence, IEEE, pp 719–725\n\nArora N, Kaur PD (2020) A bolasso based consistent feature selection enabled random forest classification algorithm: an application to credit risk assessment. Appl Soft Comput 86(105):936\n\nGoogle Scholar\n\nMarceau L, Qiu L, Vandewiele N, et al (2019) A comparison of deep learning performances with other machine learning algorithms on credit scoring unbalanced data. arXiv preprint arXiv:1907.12363\n\nWang C, Han D, Liu Q et al (2018) A deep learning approach for credit scoring of peer-to-peer lending using attention mechanism lstm. IEEE Access 7:2161–2168\n\nArticle\n  Google Scholar\n\nFan Q, Yang J (2018) A denoising autoencoder approach for credit risk analysis. In: Proceedings of the 2018 international conference on computing and artificial intelligence, pp 62–65\n\nZhu B, Yang W, Wang H, et al (2018) A hybrid deep learning model for consumer credit scoring. In: 2018 International Conference on Artificial Intelligence and Big Data (ICAIBD), IEEE, pp 205–208\n\nZhang Q, Wang J, Lu A et al (2018) An improved smo algorithm for financial credit risk assessment-evidence from china’s banking. Neurocomputing 272:314–325\n\nArticle\n  Google Scholar\n\nXu RZ, He MK (2020) Application of deep learning neural network in online supply chain financial credit risk assessment. In: 2020 international conference on computer information and big data applications (CIBDA), IEEE, pp 224–232\n\nGolbayani P, Wang D, Florescu I (2020) Application of deep neural networks to assess corporate credit rating. arXiv preprint arXiv:2003.02334\n\nGalindo J, Tamayo P (2000) Credit risk assessment using statistical and machine learning: basic methodology and risk modeling applications. Comput Econ 15(1):107–143\n\nArticle\n  MATH\n  Google Scholar\n\nQuinlan JR (1993) C4. 5: Programming for machine learning. Morgan Kauffmann 38(48):49\n\nBreimann L, Friedman JH, Olshen RA et al (1984) Classif Regres Trees. Wadsworth, Pacific Grove\n\nGoogle Scholar\n\nQuinlan JR (1986) Induction of decision trees. Mach Learn 1(1):81–106\n\nArticle\n  Google Scholar\n\nFreund Y, Schapire RE (1997) A decision-theoretic generalization of on-line learning and an application to boosting. J Comput Syst Sci 55(1):119–139\n\nArticle\n  MathSciNet\n  MATH\n  Google Scholar\n\nChen T, Guestrin C (2016) Xgboost: A scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp 785–794\n\nFriedman JH (2002) Stochastic gradient boosting. Comput Statis Data Anal 38(4):367–378\n\nArticle\n  MathSciNet\n  MATH\n  Google Scholar\n\nBreiman L (1996) Bagging predictors. Mach Learn 24(2):123–140\n\nArticle\n  MATH\n  Google Scholar\n\nHuang GB, Zhu QY, Siew CK (2006) Extreme learning machine: theory and applications. Neurocomputing 70(1–3):489–501\n\nArticle\n  Google Scholar\n\nHolland JH (1975) Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. U Michigan Press\n\nLeCun Y, Boser B, Denker JS et al (1989) Backpropagation applied to handwritten zip code recognition. Neural Comput 1(4):541–551\n\nArticle\n  Google Scholar\n\nHinton GE, Osindero S, Teh YW (2006) A fast learning algorithm for deep belief nets. Neural Comput 18(7):1527–1554\n\nArticle\n  MathSciNet\n  MATH\n  Google Scholar\n\nMcCulloch WS, Pitts W (1943) A logical calculus of the ideas immanent in nervous activity. Bull Math Biophys 5(4):115–133\n\nArticle\n  MathSciNet\n  MATH\n  Google Scholar\n\nHochreiter S, Schmidhuber J (1997) Lstm can solve hard long time lag problems. Advances in neural information processing systems pp 473–479\n\nSmolensky P (1986) Information processing in dynamical systems: foundations of harmony theory. Colorado Univ at Boulder Dept of Computer Science, Tech. rep\n\nWan S, Liang Y, Zhang Y, et al (2018) Deep multi-layer perceptron classifier for behavior analysis to estimate parkinson’s disease severity using smartphones. IEEE Access 6:36,825–36,833\n\nElman JL (1990) Finding structure in time. Cogn Sci 14(2):179–211\n\nArticle\n  Google Scholar\n\nBuitinck L, Louppe G, Blondel M, et al (2013) API design for machine learning software: experiences from the scikit-learn project. In: ECML PKDD Workshop: languages for data mining and machine learning, pp 108–122\n\nLiu S, McGree J, Ge Z et al (2015) Computational and statistical methods for analysing big data with applications. Academic Press\n\nGrajski KA, Breiman L, Di Prisco GV, et al (1986) Classification of eeg spatial patterns with a tree-structured methodology: Cart. IEEE transactions on biomedical engineering BME-33(12):1076–1086\n\nQuinlan JR et al (1996) Bagging, boosting, and c4. 5. Aaai/iaai 1:725–730\n\nMelville P (2003) Creating diverse ensemble classifiers. Computer Science Department, University of Texas at Austin\n\nKumar A (2022) The ultimate guide to adaboost algorithm : What is adaboost algorithm? https://www.mygreatlearning.com/blog/adaboost-algorithm/. Accessed 27 March 2022\n\nMuthee A (2021) The basics of genetic algorithms in machine learning. https://www.section.io/engineering-education/the-basics-of-genetic-algorithms-in-ml/. Accessed 27 March 2022\n\nDastile X, Celik T, Potsane M (2020) Statistical and machine learning models in credit scoring: a systematic literature survey. Appl Soft Comput 91(106):263\n\nGoogle Scholar\n\nZhang A, Lipton ZC, Li M, et al (2021) Dive into deep learning. arXiv preprint arXiv:2106.11342\n\nMasmoudi K, Abid L, Masmoudi A (2019) Credit risk modeling using bayesian network with a latent variable. Expert Syst Appl 127:157–166\n\nArticle\n  Google Scholar\n\nTran K, Duong T, Ho Q (2016) Credit scoring model: a combination of genetic programming and deep learning. In: 2016 Future Technologies Conference (FTC), IEEE, pp 145–149\n\nHa VS, Nguyen HN (2016) Credit scoring with a feature selection approach based deep learning. In: MATEC Web of Conferences, EDP Sciences, p 05004\n\nBabaev D, Savchenko M, Tuzhilin A, et al (2019) Et-rnn: Applying deep learning to credit loan applications. In: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp 2183–2190\n\nTwala B (2010) Multiple classifier application to credit risk assessment. Expert Syst Appl 37(4):3326–3336\n\nArticle\n  Google Scholar\n\nZhang T, Zhang W, Wei X et al (2018) Multiple instance learning for credit risk assessment with transaction data. Knowl Based Syst 161:65–77\n\nArticle\n  Google Scholar\n\nRoa L, Correa-Bahnsen A, Suarez G et al (2021) Super-app behavioral patterns in credit risk models: financial, statistical and regulatory implications. Expert Syst Appl 169(114):486\n\nGoogle Scholar\n\nAbakarim Y, Lahby M, Attioui A (2018) Towards an efficient real-time approach to loan credit approval using deep learning. 2018 9th International Symposium on Signal. Image, video and communications (ISIVC), IEEE, pp 306–313\n\nDastile X, Celik T (2021) Making deep learning-based predictions for credit scoring explainable. IEEE Access 9:50,426–50,440\n\nIwai K, Akiyoshi M, Hamagami T (2020) Structured feature derivation for transfer learning on credit scoring. In: 2020 IEEE International Conference on systems, man, and cybernetics (SMC), IEEE, pp 818–823\n\nKumar MR, Gunjan VK (2020) Review of machine learning models for credit scoring analysis. Ingeniería Solidaria 16(1)\n\nChi J, Zeng G, Zhong Q, et al (2020) Learning to undersampling for class imbalanced credit risk forecasting. In: 2020 IEEE International Conference on data mining (ICDM), IEEE, pp 72–81\n\nArya M, Sastry GH (2020) Deal-‘deep ensemble algorithm’framework for credit card fraud detection in real-time data stream with google tensorflow. Smart Sci 8(2):71–83\n\nArticle\n  Google Scholar\n\nHsu TC, Liou ST, Wang YP et al (2019) Enhanced recurrent neural network for combining static and dynamic features for credit card default prediction. ICASSP 2019–2019 IEEE International Conference on Acoustics. Speech and Signal Processing (ICASSP), IEEE, pp 1572–1576\n\nAlam TM, Shaukat K, Hameed IA, et al (2020) An investigation of credit card default prediction in the imbalanced datasets. IEEE Access 8:201,173–201,198\n\nYiheng Wei QMYu Qi (2020) Fraud detection by machine learning. 2020 2nd International Conference on Machine Learning. Big Data and Business Intelligence (MLBDBI), IEEE, pp 101–115\n\nShivanna A, Agrawal DP (2020) Prediction of defaulters using machine learning on azure ml. In: 2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), IEEE, pp 0320–0325\n\nZhang X, Han Y, Xu W et al (2021) Hoba: a novel feature engineering methodology for credit card fraud detection with a deep learning architecture. Inf Sci 557:302–316\n\nArticle\n  Google Scholar\n\nRushin G, Stancil C, Sun M, et al (2017) Horse race analysis in credit card fraud-deep learning, logistic regression, and gradient boosted tree. In: 2017 systems and information engineering design symposium (SIEDS), IEEE, pp 117–121\n\nCan B, Yavuz AG, Karsligil EM, et al (2020) A closer look into the characteristics of fraudulent card transactions. IEEE Access 8:166,095–166,109\n\nAhmed F, Shamsuddin R (2021) A comparative study of credit card fraud detection using the combination of machine learning techniques with data imbalance solution. In: 2021 2nd International Conference on Computing and Data Science (CDS), IEEE, pp 112–118\n\nKhatri S, Arora A, Agrawal AP (2020) Supervised machine learning algorithms for credit card fraud detection: a comparison. In: 2020 10th International Conference on Cloud Computing, Data Science & Engineering (Confluence), IEEE, pp 680–683\n\nTorres RAL, Ladeira M (2020) A proposal for online analysis and identification of fraudulent financial transactions. In: 2020 19th IEEE International Conference on machine learning and applications (ICMLA), IEEE, pp 240–245\n\nYu L, Yang Z, Tang L (2016) A novel multistage deep belief network based extreme learning machine ensemble learning paradigm for credit risk assessment. Flex Serv Manuf J 28(4):576–592\n\nArticle\n  Google Scholar\n\nHuang X, Liu X, Ren Y (2018) Enterprise credit risk evaluation based on neural network algorithm. Cogn Syst Res 52:317–324\n\nArticle\n  Google Scholar\n\nOreski S, Oreski G (2014) Genetic algorithm-based heuristic for feature selection in credit risk assessment. Expert Syst Appl 41(4):2052–2064\n\nArticle\n  Google Scholar\n\nFeng B, Xue W, Xue B, et al (2020) Every corporation owns its image: Corporate credit ratings via convolutional neural networks. In: 2020 IEEE 6th International Conference on Computer and Communications (ICCC), IEEE, pp 1578–1583\n\nDong Y, Xie K, Bohan Z et al (2021) A machine learning model for product fraud detection based on svm. 2021 2nd International Conference on Education. Knowledge and Information Management (ICEKIM), IEEE, pp 385–388\n\nZhou Y, Song X, Zhou M (2021) Supply chain fraud prediction based on xgboost method. 2021 IEEE 2nd International Conference on Big Data. Artificial Intelligence and Internet of Things Engineering (ICBAIE), IEEE, pp 539–542\n\nGarcía V, Marqués AI, Sánchez JS (2019) Exploring the synergetic effects of sample types on the performance of ensembles for credit risk and corporate bankruptcy prediction. Inf Fusion 47:88–101\n\nArticle\n  Google Scholar\n\nGiudici P, Hadji-Misheva B, Spelta A (2020) Network based credit risk models. Qual Eng 32(2):199–211\n\nArticle\n  Google Scholar\n\nChen YR, Leu JS, Huang SA, et al (2021) Predicting default risk on peer-to-peer lending imbalanced datasets. IEEE Access 9:73,103–73,109\n\nLiang K, He J (2020) Analyzing credit risk among chinese p2p-lending businesses by integrating text-related soft information. Electron Commer Res Appl 40(100):947\n\nGoogle Scholar\n\nSong Y, Wang Y, Ye X et al (2020) Multi-view ensemble learning based on distance-to-model and adaptive clustering for imbalanced credit risk assessment in p2p lending. Inf Sci 525:182–204\n\nArticle\n  MathSciNet\n  Google Scholar\n\nNiu K, Zhang Z, Liu Y et al (2020) Resampling ensemble model based on data distribution for imbalanced credit risk evaluation in p2p lending. Inf Sci 536:120–134\n\nArticle\n  MathSciNet\n  Google Scholar\n\nYang J, Li Q, Luo D (2019) Research on p2p credit risk assessment model based on rbm feature extraction-take sme customers as an example. Open J Busin Manag 7(4):1553–1563\n\nArticle\n  Google Scholar\n\nSirignano J, Sadhwani A, Giesecke K (2016) Deep learning for mortgage risk. arXiv preprint arXiv:1607.02470\n\nOstapchenya D (2021) The role of big data in banking : How do modern banks use big data? https://www.finextra.com/blogposting/20446/the-role-of-big-data-in-banking--how-do-modern-banks-use-big-data. Accessed 27 March 2022\n\nDu G, Liu Z, Lu H (2021) Application of innovative risk early warning mode under big data technology in internet credit financial risk assessment. J Comput Appl Math 386(113):260\n\nMathSciNet\n  MATH\n  Google Scholar\n\nGao L, Xiao J (2021) Big data credit report in credit risk management of consumer finance. Wireless Communications and Mobile Computing 2021\n\nWang H (2021) Credit risk management of consumer finance based on big data. Mobile Information Systems 2021\n\nNiu A, Cai B, Cai S (2020) Big data analytics for complex credit risk assessment of network lending based on smote algorithm. Complexity 2020\n\nPérez-Martín A, Pérez-Torregrosa A, Vaca M (2018) Big data techniques to measure credit banking risk in home equity loans. J Bus Res 89:448–454\n\nArticle\n  Google Scholar\n\nTang H, Zhang Y, Qiao Q, et al (2020) Risk assessment of credit field based on pso-svm. In: 2020 2nd International Conference on Economic Management and Model Engineering (ICEMME), IEEE, pp 809–813\n\nTomczak JM, Zieba M (2015) Classification restricted boltzmann machine for comprehensible credit scoring model. Expert Syst Appl 42(4):1789–1796\n\nArticle\n  Google Scholar\n\nLucas Y, Jurgovsky J (2020) Credit card fraud detection using machine learning: A survey. arXiv preprint arXiv:2010.06479\n\nWang X, Xu M, Pusatli ÖT (2015) A survey of applying machine learning techniques for credit rating: Existing models and open issues. In: International Conference on neural information processing, Springer, pp 122–132\n\nBreeden JL (2020) Survey of machine learning in credit risk. Available at SSRN 3616342\n\nBhatore S, Mohan L, Reddy YR (2020) Machine learning techniques for credit risk evaluation: a systematic literature review. J Bank Financ Technol 4(1):111–138\n\nArticle\n  Google Scholar\n\nLeo M, Sharma S, Maddulety K (2019) Machine learning in banking risk management: a literature review. Risks 7(1):29\n\nArticle\n  Google Scholar\n\nChi G, Uddin MS, Abedin MZ, et al (2019) Hybrid model for credit risk prediction: an application of neural network approaches. International Journal on Artificial Intelligence Tools 28(05):1950,017\n\nNajadat H, Altiti O, Aqouleh AA, et al (2020) Credit card fraud detection based on machine and deep learning. In: 2020 11th International Conference on Information and Communication Systems (ICICS), IEEE, pp 204–208\n\nChen X, Li S, Xu X, et al (2020) A novel gsci-based ensemble approach for credit scoring. IEEE Access 8:222,449–222,465\n\nWang GG, Deb S, Cui Z (2019) Monarch butterfly optimization. Neural Comput Appl 31(7):1995–2014\n\nArticle\n  Google Scholar\n\nWang GG, Deb S, Coelho LDS (2018) Earthworm optimisation algorithm: a bio-inspired metaheuristic algorithm for global optimisation problems. Int J Bioinsp Comput 12(1):1–22\n\nGoogle Scholar\n\nWang GG, Deb S, Coelho LdS (2015) Elephant herding optimization. In: 2015 3rd international symposium on computational and business intelligence (ISCBI), IEEE, pp 1–5\n\nWang GG (2018) Moth search algorithm: a bio-inspired metaheuristic algorithm for global optimization problems. Memetic Comput 10(2):151–164\n\nArticle\n  Google Scholar\n\nLi S, Chen H, Wang M et al (2020) Slime mould algorithm: a new method for stochastic optimization. Future Gener Comput Syst 111:300–323\n\nArticle\n  Google Scholar\n\nYang Y, Chen H, Heidari AA et al (2021) Hunger games search: visions, conception, implementation, deep analysis, perspectives, and towards performance shifts. Expert Syst Appl 177(114):864\n\nGoogle Scholar\n\nTu J, Chen H, Wang M et al (2021) The colony predation algorithm. J Bionic Eng 18(3):674–710\n\nArticle\n  Google Scholar\n\nHeidari AA, Mirjalili S, Faris H et al (2019) Harris hawks optimization: algorithm and applications. Future Gener Comput Syst 97:849–872\n\nArticle\n  Google Scholar\n\nAhmadianfar I, Heidari AA, Gandomi AH et al (2021) Run beyond the metaphor: an efficient optimization algorithm based on runge kutta method. Expert Syst Appl 181(115):079\n\nGoogle Scholar\n\nDownload references\n\nAcknowledgements\n\nThis work was supported in part by the Macao Polytechnic University – Edge Sensing and Computing: Enabling Human-centric (Sustainable) Smart Cities (RP/ESCA-01/2020) and by the Emilia Romagna Region within the European S3 program with the Project LiBER. We want to thank the Macao government and the Emilia Romagna regional government for supporting this work.\n\nFunding\n\nOpen access funding provided by Alma Mater Studiorum - Università di Bologna within the CRUI-CARE Agreement. This work was funded in part by the Macao Polytechnic University – Edge Sensing and Computing: Enabling Human-centric (Sustainable) Smart Cities (RP/ESCA-01/2020).\n\nAuthor information\n\nAuthors and Affiliations\n\nFaculty of Applied Sciences, Macao Polytechnic University, Macao SAR, China\n\nSi Shi, Rita Tse & Wuman Luo\n\nEngineering Research Centre of Applied Technology on Machine Translation and Artificial Intelligence of Ministry of Education, Macao Polytechnic University, Macao SAR, China\n\nRita Tse\n\nDepartment of Political Science, University of Roma Tre, Rome, Italy\n\nStefano D’Addona\n\nDepartment of Computer Science and Engineering, University of Bologna, Bologna, Italy\n\nGiovanni Pau\n\nUCLA Samueli Computer Science, University of California, Los Angeles, USA\n\nGiovanni Pau\n\nContributions\n\nAuthor Si Shi’s contribution is to write and edit the whole paper. Author RT contribution is to provide the funding and resources as well as conceptualization of the paper. Author WL took part in the revising and conceptualization of the paper. Author SD’A helped with the formation of initial financial framework and revising. Author GP guided the whole research process and revised the paper.\n\nCorresponding author\n\nCorrespondence to Giovanni Pau.\n\nEthics declarations\n\nConflict of interest\n\nThe authors declare that they have no conflict of interest.\n\nEthics approval\n\nThe authors are consistent with the ethical requirements.\n\nConsent to participate\n\nThe authors all consent to participate in the paper editing.\n\nConsent for publication\n\nThe authors all consent to the publication of the paper.\n\nCode availability\n\nNot applicable.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nSupplementary Information\n\nBelow is the link to the electronic supplementary material.\n\nSupplementary file 1 (pdf 275 KB)\n\nSupplementary file 2 (pdf 124 KB)\n\nRights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nShi, S., Tse, R., Luo, W. et al. Machine learning-driven credit risk: a systemic review. Neural Comput & Applic 34, 14327–14339 (2022). https://doi.org/10.1007/s00521-022-07472-2\n\nDownload citation\n\nReceived\n12 February 2022\n\nAccepted\n26 May 2022\n\nPublished\n16 July 2022\n\nIssue Date\nSeptember 2022\n\nDOI\nhttps://doi.org/10.1007/s00521-022-07472-2\n\nShare this article\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nKeywords\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nArtificial Intelligence and Machine Learning in Credit Risk Assessment ... : \nAbstract\n\nCredit risk assessment has become one of the major concerns in modern finance regarding informed lending decisions. Although several studies have used traditional logistic regression and linear discriminant analysis techniques, these have increasingly become inadequate tools in today’s complex and data-rich environment. Such models often struggle with large datasets and nonlinear relationships, thus reducing their predictive power and adaptability. Artificial Intelligence (AI) and Machine Learning (ML) provide two of the most innovative approaches to credit risk modeling. This paper reviews a few ML models applied to improve the accuracy and efficiency of credit risk assessment, from Random Forests and Support Vector Machines to Neural Networks. Compared to the more traditional models, AI models can enhance predictive accuracy by using a wealth of structured and unstructured information, including alternative information sources such as social media activities and transaction history. However, despite noticeable advantages, there are some challenges concerning the use of AI in credit risk assessment, including model opaqueness, bias, and regulatory compliance. The nature of such a “black box”, especially for deep learning algorithms, can limit their interpretability and complicate regulatory compliance and decision rationalization. To solve problems induced by this “black box” nature, explainable AI techniques, namely Shapley values and LIME, have been implemented to enhance the transparency of models and raise stakeholder trust in support systems for decision-making. This review aims to evaluate the current applications of AI and ML in credit risk assessment, weigh the strengths and limitations of various models, and discuss the ethical considerations and regulatory challenges linked to their adoption by credit institutions.\n\nKeywords\n\nArtificial Intelligence, Machine Learning, Credit Risk Assessment, Data Bias\n\nShare and Cite:\n\n1. Introduction\n\nAccurate and fair assessment of credit risks is rudimentary in modern finance for informed lending decisions. Whereas the traditional models included logistic regression and linear discriminant analysis, these have recently become inadequate in today’s complex and data-rich environments. Though transparent and regulator-friendly, these methods usually cannot capture borrowers’ nuanced nonlinear behaviors, compromising predictive accuracy.\n\nThe change in credit risk modeling heralds AI and ML. Predictive power has radically improved by accessing large datasets and the ability to discern patterns not accessible to traditional analytics. However, interpretability, ethical issues, and regulatory compliance become issues with the translucent nature of “black box” models such as deep neural networks.\n\nThis review critically analyzes various state-of-the-art AI and ML models in credit risk assessment by comparing them with traditional methods and discussing the potential of such models to reshape financial decision-making. We analyze different models, such as Random Forests, Support Vector Machines, and Neural Networks, to discuss how these technologies can improve accuracy by considering ethical and regulatory considerations proper for widespread adoption. The paper will help achieve a balanced understanding of the responsible implementation of AI, hoping that technological advancements will translate into improvements in the consistency and transparency of credit risk assessment.\n\n1.1. Traditional Credit Risk Modeling\n\nFor years, traditional models such as logistic regression and linear discriminant analysis have formed the foundation of credit risk assessment, a core function of the financial industry. These models conventionally rely on structured data, including borrowers’ income, credit scores, and debt levels, to predict the likelihood of default (Breeden, 2020). Although these methods have served well for many decades, they are based on assumptions of linearity, which often fail to capture the complexity of borrowers’ behavior in today’s data-rich environment.\n\nFor instance, logistic regression assumes a linear relationship between the input variables and the outcome default or non-default, which can oversimplify real-world credit risk (Bussmann et al., 2020). Furthermore, traditional models often require significant human intervention to extract key features, potentially overlooking meaningful nonlinear interactions between variables (Addo et al., 2018).\n\nDespite these deficiencies, traditional models remain popular, because they are transparent and acceptable to many regulators. Their interpretability advantages over more complex model types, neural networks are a good example of them appealing to regulators explicitly required to explain credit decisions. However, as data becomes increasingly complex and voluminous, these models struggle to keep pace, highlighting the need for more advanced techniques such as AI and ML.\n\n1.2. Credit Risk Assessment Models Using Artificial Intelligence and Machine Learning\n\nRecently, AI and ML have emerged as powerful tools for credit risk assessment. These methods offer several advantages over traditional models, mainly due to their ability to process large and complex datasets and uncover nonlinear relationships. Examples of such models include Random Forests, Support Vector Machines (SVMs), and Neural Networks, which can learn from vast amounts of data and recognize patterns that traditional models might miss.\n\nRandom Forest, an ensemble learning model, builds multiple decision trees and then combines their outputs for better accuracy. It is highly effective in credit risk assessment because the relationship between borrower attributes and default probability is often complex and nonlinear. Research has shown that Random Forests outperform conventional models in predictive accuracy, especially when the model is enhanced with data sources such as social media activities and transaction history.\n\nNeural Networks have also been shown to reveal complex nonlinear relationships among variables. They are instrumental when managing vast amounts of unstructured data, such as transaction histories or mobile phone usage data (Zhang & Yu, 2024). However, interpretability is considered one of the main disadvantages of neural networks, referred to as the “black box” problem-which makes it difficult for financial institutions to justify their credit decisions.\n\nSVMs also show exciting potential for credit risk modeling, especially when dealing with unbalanced datasets, as is often the case where the number of defaulters is less than that of non-defaulters. Support Vector Machines seek an optimal hyperplane that separates the two classes: defaulters versus non-defaulters. In situations where generalizing from traditional models is difficult, SVMs are particularly robust.\n\n1.3. Incorporation of Alternative Information Providers\n\nOne of the most crucial advantages of AI and ML models in evaluating credit risk is their ability to incorporate alternative data. Traditional models have relied heavily on structured data such as credit scores and income levels, which are inaccessible to all borrowers, particularly in emerging markets or those with incomplete credit histories (Breeden, 2020). In contrast, AI models can use unstructured data from social media activity, mobile phone usage, and transaction history to better understand a borrower’s behavior.\n\nFor instance, Kou et al. (Kou et al., 2019) highlighted the contributions of social media data in enhancing credit risk assessment. AI models can better predict financial reliability based on factors like the size of borrowers’ social networks, posting frequencies, and sentiment analysis from their online activities. Mobile usage data, such as call logs and location information, can provide in-depth insights into a borrower’s stability and predict their likelihood of default.\n\nThese alternative data sources are particularly useful for assessing borrowers with minimal traditional credit records, making them an integral part of modern credit risk models. However, the use of alternative data raises ethical concerns regarding data privacy and potential discrimination. Financial organizations should ensure that their alternative data practices comply with data protection regulations, such as the GDPR in Europe, and that their models are not discriminatory across different demographic groups.\n\n1.4. Explainability and Transparency in Artificial Intelligence Models\n\nOne of the biggest challenges posed by AI and ML models in credit risk assessment is their lack of transparency. Traditional models, such as logistic regression, are comparatively easier to explain, allowing lending and regulatory bodies to understand how decisions are made. In contrast, many AI models, particularly neural networks, function as an impenetrable “black box”, making it difficult to explain how they derive their predictions.\n\nThis lack of transparency creates serious problems in regulated industries like finance, where institutions must demonstrate the reasonableness of their lending decisions. Consequently, researchers have developed techniques to make AI models more explainable. For example, Shapley values provide a method for quantifying the contribution of each feature to model predictions, thereby helping financial institutions more easily explain their decisions to regulatory bodies and customers.\n\nAnother explainability technique is LIME, which generates interpretable models to approximate complex AI models for individual predictions. These methods are paramount for enhancing trust in AI credit assessment models, especially when a borrower is denied credit and an explanation is required (Angelini et al., 2008).\n\n1.5. Bias and Fairness of Artificial Intelligence Models\n\nWhile credit risk estimates have improved, biases inherent in AI models remain a concern. AI models are trained on historical data, which can be biased and lead to discriminatory outcomes. For example, a model trained on data from discriminatory lending practices may still be biased against certain demographic groups, even if sensitive attributes like race or gender are excluded.\n\nSome methods incorporate fairness constraints during model training to prevent sensitive attributes from affecting predictions. Other biases may be reduced by approaches like “fairness through unawareness”, which removes sensitive variables from the model. However, Kou et al. (Kou et al., 2019) demonstrated that information like geographical location or employment status can introduce biases even without sensitive variables.\n\nFinancial institutions must ensure that their AI models comply with regulatory standards that prevent discrimination, such as the FCRA in the United States and the Equality Act in the United Kingdom. According to Fernandez (Fernandez, 2019), failure to meet these regulations can generate serious legal and reputational risks for financial institutions.\n\n1.6. Performance Metrics for Artificial Intelligence Models in Credit Risk\n\nBeyond accuracy, various performance metrics are used for AI models in credit risk assessment. In imbalanced datasets-with few defaulters compared to non-defaulters-metrics like precision, recall, their harmonic mean (F1 score), and the area under the ROC curve (AUC-ROC) are crucial.\n\nAccuracy is defined as the ratio of correctly classified instances to all instances. While accuracy is useful, it can be misleading in imbalanced datasets where one class (the majority class) dominates. Precision is the ratio of true positive predictions (correctly predicted defaulters) to all positive predictions made by the model. Precision is relevant in credit risk assessment because a false positive-classifying a borrower as a defaulter when they are not-can lead to lost lending opportunities.\n\nRecall measures the proportion of true positives out of all actual positive instances (defaulters). A high recall ensures that a high proportion of high-risk borrowers are correctly identified. The F1 score, the harmonic mean of precision and recall, provides a balanced metric that considers both false positives and false negatives.\n\nThe AUC-ROC is a measure indicating the model’s capability to differentiate between defaulters and non-defaulters. A higher AUC indicates better model performance. Utilizing these performance metrics helps in understanding the strengths and weaknesses of different AI models in credit risk assessment.\n\n2. Methodology\n\n2.1. Data Collection and Preprocessing\n\nThis research is based on a comprehensive dataset from various sources that integrates both structured and unstructured data to enhance the validity of models when assessing credit risk. The structured data includes conventional financial variables related to borrowers’ income, debt-to-income ratio, credit history, and loan amounts-definite elements in any traditional credit risk assessment model (Breeden, 2020; Addo et al., 2018). Additionally, it leverages data from social media activities, mobile transactions, and geolocation to capture more depth in borrower behavior than conventional data may miss.\n\nThe dataset consists of over 200,000 records of individual borrowers spanning from 2017 to 2022. This large sample size provides high statistical power and makes the model dependable. All personal information is anonymized in accordance with strict standards, such as the European Union’s General Data Protection Regulation (GDPR), which demands that organizations maintain stringent data handling and protection.\n\nData preprocessing involves several important steps. Missing values are imputed using mean and mode strategies for numerical and categorical variables, respectively. Outliers are detected and removed to avoid bias in the results. For unstructured data, natural language processing techniques such as sentiment analysis and keyword extraction are employed to transform text data into analyzable features. Sentiment analysis, also known as sentiment analysis or opinion mining, is the process of analyzing, processing, summarizing, and inferring subjective texts with emotional connotations. By utilizing the ability of sentiment analysis, it is possible to automatically determine the positive and negative emotional tendencies of natural language texts with subjective descriptions and provide corresponding results. Keyword extraction is the process of extracting the most relevant words from the text that are most relevant to the meaning of the article. It has important applications in literature search, automatic summarization, text clustering, and text classification. Transaction data is summarized to reveal financial behaviors, including transaction frequency and volume.\n\nReferring to the commonly used classification standards for training and testing sets in deep learning networks, this article divides the preprocessed data into an 80% training set and a 20% testing set. To avoid overfitting, 10-fold cross-validation is used to tune the models and measure their performance.\n\n2.2. Model Selection and Architecture\n\nIn this study, we will evaluate the effectiveness of several AI and ML models in credit risk assessment and compare them with the baseline logistic regression model:\n\nRandom Forests: As an integrated learning model, random forests improve prediction accuracy by constructing multiple decision trees and merging their outputs. They are excellent at preventing overfitting and can provide an ordering of the importance of features that affect prediction.\n\nSupport Vector Machines (SVMs): SVMs are excellent at handling unbalanced datasets and are able to find optimal hyperplanes in high-dimensional spaces to separate different classes. This makes SVMs particularly well suited to deal with class imbalance.\n\nNeural networks: These models can handle complex nonlinear relationships between input variables. Deep learning models with a multi-layer structure make neural networks particularly effective at handling structured and unstructured data. They are especially good at recognizing patterns that may not be detected by traditional models.\n\nLogistic regression: used as a benchmark model for performance comparisons, although logistic regression is widely used for credit risk modeling and its linear nature has limitations when dealing with interactions between variables.\n\n2.3. Feature Engineering and Selection\n\nThe predictive power of a model can be significantly enhanced through efficient feature engineering. New features derived from raw data can more accurately reflect borrower behavior and risk profile, such as calculating debt-to-income ratios, default rates, and loan maturities.\n\nSentiment analysis of social media posts can reveal borrower sentiment and risk tolerance, information hidden in unstructured data. Variables such as transaction frequency, average transaction amount, and stability derived from geolocation data further enrich the database for risk assessment models. These features demonstrate details of financial behavior that are difficult to capture in traditional data.\n\n2.4. Evaluation Metrics\n\nTherefore, a range of metrics was considered to comprehensively evaluate the model performance: accuracy, precision, recall, F1 score, and area under the receiver operator characteristic curve (AUC-ROC). Since credit risk datasets are often classically unbalanced, relying on accuracy alone may be misleading.\n\nAccuracy rate: The ratio of correct classifications to the total number of instances.\n\nPrecision rate: These are the number of instances correctly predicted as defaulters to all instances predicted as defaulters, with the importance of avoiding opportunity costs due to incorrect lending decisions.\n\nRecall rate: the ratio of correctly identified true defaulters to highlight high-risk borrowers accordingly.\n\nF1-score: Harmonic mean of the precision and recall rates; it allows for a balanced view to find the model’s score, considering the effect of false positives and false negatives.\n\nAUC-ROC: It describes the model performance in terms of the capability to distinguish between defaulter and non-defaulter classes where higher values are better.\n\n2.5. Cross-Validation and Hyperparameter Tuning\n\nIn this thesis, 10-fold cross-validation will be applied to make sure that the model is generalizable for unseen data. One dataset for this purpose is further divided into ten subsets and is, in turn, trained on nine subsets and evaluated on the remaining one. In this manner, overfitting can be avoided, and the model can be generalized well to various subsets of data. The hyperparameter tuning was done through a grid search in which different combinations of hyperparameters that yield an optimum configuration were systematically tried. It ranges from modifying the number of trees in a random forest and the maximum depth of each tree to adjusting the number of hidden layers, the number of neurons per layer, and the learning rate in a neural network, making fits of model performance.\n\n2.6. Explainability and Interpretability\n\nGiven the complexity of AI models, especially neural networks, interpretability is an important challenge for financial institutions. Shapley values and LIME are two techniques that help to solve the “black box” problem of AI models by clarifying the contribution of each feature to the model prediction.\n\nShapley value: inspired by cooperative game theory, it is used to quantify the marginal contribution of each feature to the model output.\n\nLIME: approximates the behavior of complex models by constructing simplified interpretable models that revolve around individual predictions.\n\nThe integration of these interpretable techniques not only improves the accuracy of the model, but also ensures that the model is transparent and interpretable, enabling GSEs to make lending decisions that are both informed and interpretable, with results that are easily understood by regulators and customers.\n\n3. Results and Analysis\n\n3.1. Model Performance Comparison\n\nThis section introduces the comparative performance of the Random Forest, Support Vector Machine, Neural Networks, and baseline Logistic Regression models for credit risk assessment. Each model was evaluated for accuracy, precision, recall, F1 score, and AUC-ROC, which refers to the Area Under the Receiver Operating Characteristic Curve.\n\nDuring the analysis, the Random Forest model yielded the best performance with 93% accuracy and a mean AUC-ROC score of 0.94. Its high capability to deal with nonlinear relationships in big, complicated datasets and its robustness when structured data, such as social media activities or transaction history, give it a special place in credit default prediction.\n\nThe neural network models fared equally well by achieving the accuracy of 91% with an AUC-ROC score of 0.92. The strong points of the methods are treating high volume data to capture nonlinear interactions among features and allowing both structured and unstructured data inclusion.\n\nSupport Vector Machine showed worse results when the accuracy was 89%, and the AUC-ROC score equaled 0.88. It could be useful in a case if there is a small portion of defaulters compared with the overall population, and its strong classification capabilities will be helpful to identify a high-risk class of borrowers (Goodell et al., 2021).\n\nWhile the traditional credit risk assessments applied the LR baseline model, it resulted in the poorest performance among the models, where its accuracy was 84% and AUC-ROC was 0.79. Some of the limitations of LRs include reliance on linear assumptions, making it not that effective to capture the complexity in borrower behaviors.\n\nTable 1 is the summary of key performance metrics in a tabular form.\n\nTable 1. Model performance comparison table.\n\nModel\n\nAccuracy\n\nAUC-ROC\n\nPrecision\n\nRecall\n\nF1 Score\n\nRandom Forest\n\n93%\n\n0.94\n\n0.91\n\n0.88\n\n0.90\n\nNeural Networks\n\n91%\n\n0.92\n\n0.89\n\n0.86\n\n0.87\n\nSupport Vector Machine\n\n89%\n\n0.88\n\n0.85\n\n0.82\n\n0.84\n\nLogistic Regression\n\n84%\n\n0.79\n\n0.78\n\n0.74\n\n0.76\n\nPerformance of Visualization:\n\nROC Curves: The ROC curve of each model has been plotted to visually compare the strength of each model in classifying between defaulters and non-defaulters. Of these, the Random Forest had the maximum AUC, thus becoming the most powerful predictive model.\n\nPrecision-Recall Curves: Since credit risk datasets are usually imbalanced, precision-recall curves will give insight into how each model will perform to minimize false positives and false negatives. Random Forests and Neural Networks performed particularly well in maintaining high precision while finding the right balance in terms of recall.\n\nConfusion Matrix: The confusion matrix for each model provides different values of true positives, those respective defaulters who were correctly identified as true negatives, false positives, and false negatives that give detailed insight into model performance in a real-world setting.\n\nThese analyses pinpoint the potential of AI models, especially Random Forest and Neural Networks, which, by treating complex borrower behavior and using sources of structured and unstructured data, can outperform traditional methods such as Logistic Regression.\n\n3.2. Alternative Data Sources—Effectiveness\n\nThe key insight from this research is the exceptional performance gain achieved by incorporating additional data sources, such as social media activity, transaction history, and geolocation data. These alternative data sources gave a better context on how borrowers behave; this was especially true for those who did not have any formal credit history in the first place (Fernandez, 2019). In the case of social media analysis, for instance, good sentiments reflected a minimal risk of default. Other indicative factors showed that borrowers with stable high-frequency transactions have a lower risk of default, again proving the predictive power of alternative data.\n\nThe Random Forest and Neural Network models benefited the most from all the alternative data sources (Zhang & Yu, 2024). Because both can manage structured and unstructured data, they can notice complex patterns in borrower behavior, which the Logistic Regression model couldn’t (Addo et al., 2018). In contrast, the RF model used the frequency and volume of transactions as a significant measure of stability, while NN models leveraged social media activities and transaction behavior to build far more accurate default predictions.\n\n3.3. Explainability and Transparency of Artificial Intelligence Models\n\nWhile the models Random Forest and Neural Network outperformed Logistic Regression on predictive performance, their opaqueness is yet a big concern in highly regulated sectors like finance. According to Fernandez (Fernandez, 2019), financial institutions should be able to explain the rationale behind credit decisions; therefore, explainability is a crucial factor when considering AI model adoption. This has been accomplished through Shapley values and LIME in this work, Local Interpretable Model-Agnostic Explanations, by Goodell et al. (Goodell et al., 2021).\n\nShapley values provided insights into the contributions of individual features, such as debt-to-income ratio, payment history, and transaction frequency, to model predictions. For instance, debt-to-income ratio was one of the most influential variables in predicting default with continuity in most applications. LIME was used in specific instances to explain, for instance, why a particular borrower had been classified as high-risk. This at least allows more insight into the decision-making process.\n\nWhile these explanation techniques brought much-needed transparency, they added even more complexity. The trade-off between model complexity and explainability remains among the most important challenges that financial institutions need to address to ensure that regulatory bodies are satisfied and trust is instilled in AI-driven credit assessments.\n\n3.4. Bias and Fairness in Artificial Intelligence Models\n\nHowever, their use in credit risk management has raised several concerns regarding algorithmic bias and fairness. In general, AI models are subject to learning potential biases from the data used for training to disadvantage one demographic group or another. For instance, even though discriminatory on lending issues, AI models could practice it if they had been biased in the beginning against some groups, even if sensitive attributes like race or gender are excluded from the dataset.\n\nThe principle of FTU compliance in this work eliminated the bias by not considering sensitive variables in the models. However, even with FTU, biases are still possible, like proxy variables of location or employment sector. In line with this, the calculations of fairness metrics were performed for demographic parity and equal opportunity, which are used to determine whether models make equitable predictions across different demographic groups. Fairness constraints increased equity but slightly reduced performance in the Neural Network model (Thakkar, & Chaudhari, 2021). This reveals that fairness is a competing factor against accuracy.\n\n3.5. Comparison of Artificial INTELLIGENCE and Traditional Model\n\nOverall, the findings of this study confirm that AI models outperform traditional logistic regression in credit risk evaluation. The performance of the Random Forest and Neural Network models was consistently better than logistic regression in all the individual metrics, especially when exploiting nonlinear relationships and alternative data sources.\n\nWhile logistic regression is still appreciated for its simplicity and transparency, its reliance on linear relationships limits its performance on more complex, real-world credit risk scenarios. The Support Vector Machine model balances accuracy and interpretability well without being as complex as Neural Networks, thus making it implementable for financial institutions that want better performance without going all the way to black-box models.\n\nThe findings reveal that it is high time financial institutions adopt more advanced AI-driven systems; otherwise, they would likely be excluded from contemporary finance. Nevertheless, the problems of model transparency and fairness, together with assurance of conformation with regulatory requirements, must be addressed so that AI can deploy its duties responsibly in credit risk assessment.\n\n4. Discussion\n\n4.1. Implication of the Findings for Credit Risk Assessment\n\nThese results indicate benefits of using AI and ML models in evaluating credit risk compared to traditional approaches. The two AI models, Forests and Neural Networks, are significantly better and can improve the accuracy of the predictions in identifying complex nonlinear relationships that are easily ignored by traditional techniques, such as those based on logistic regression. Indeed, prior studies have established that these AI models can use alternative data, such as how active a credit-seeker has been on social media, his transaction history, and geolocation, to arrive at more accurate risk assessments (Fernandez, 2019).\n\nOne key takeaway is that those financial institutions using AI-based credit risk models gain a much deeper understanding of the behavior of borrowers. Integrating unstructured data offers better predictions of creditworthiness for people who either have a thin or no formal credit history. This is particularly important in developing markets and among gig economy workers, who typically do not have access to traditional credit systems because of a lack of structured financial data.\n\n4.2. Challenges and Limitations of Artificial Intelligence Models\n\nConversely, several challenges dominate deploying AI and ML models to mainstream adoption in the financial industry. The most important of these is the inherent complexity of deep learning models, which requires several computational resources and technical skills for modeling, deployment, and maintenance substantial enough to make smaller institutions struggle to compete with large organizations able to invest more energy in state-of-the-art AI solutions.\n\nAnother critical issue with these AI models is their lack of transparency. That is why traditional models, like Logistic Regression, have remained preferable because they are easy to interpret and explain the decisions, both to a regulator and a customer who has been rejected. It is true that deep learning models have emerged as “black boxes”, and it becomes challenging to justify credit decisions, especially in cases of denial to applicants. This causes severe complications regarding regulatory issues, such as adhering to a framework that needs to be set by the General Data Protection Regulation for explainability when a decision is made.\n\nThis paper tried to overcome these challenges by incorporating Shapley values to explain model predictions in a transparent manner without sacrificing predictive power. Goodell et al. (Goodell et al., 2021) have proposed LIME as a method to derive locally interpretable models for individual predictions, increasing the potential for transparency such that institutions may give clear justifications for their credit decisions, as stated by (Bussmann et al., 2020). In any case, implementing these explainability techniques requires additional computational effort, adding to the complexity of adopting AI.\n\n4.3. Some Ethics to Consider: Bias and Fairness in Artificial Intelligence Models\n\nCredit risk assessment always faces the algorithmic bias issue from deployed AI models. The model can inherit biases leading to disparity in outcomes, as most models are trained with historical data. For instance, a model that has been trained on some discriminatory practices of the past might keep discriminating against certain groups, even when sensitive attributes like race or gender are removed. This will be especially detrimental in regulated industries where equity and fairness are core ends.\n\nThe researchers in this work have excluded sensitive variables from the dataset, using a principle called Fairness Through Unawareness. However, there is still a chance that proxy features might encode sensitive information; therefore, fairness constraints during model training were necessary to make the predictions equal among demographic groups.\n\nFairness constraints imposed to enforce equity came at some cost in predictive accuracy, especially for the Neural Network model. This depicts the challenge of trading off fairness against performance, an issue now at the forefront of research. Organizations must ensure that their AI models comply with the legislation enacted to eradicate discrimination, such as the U.S. Fair Credit Reporting Act and the Equality Act in the UK.\n\n4.4. Future Research Directions\n\nResults from this work have also pointed out some directions for future research and development concerning the applications of AI models in credit risk assessment. Hybrid model development will be one very promising avenue that combines the predictive power of AI with the interpretability of traditional models. By integrating AI techniques with Logistic Regression or decision trees, financial institutions can have the best of both worlds: accuracy from AI and transparency for regulatory compliance. Hybrid systems could allow real practicality for financial institutions that might be uneasy about thoroughly implementing black-box models like Neural Networks.\n\nAnother exciting direction is improving fair-aware AI models. Current approaches, such as Fairness Through Unawareness, are a good starting point for which much future research needs to develop sophisticated techniques that tend to reduce bias without performance compromise. Techniques such as adversarial de-biasing or representation learning for fairness might hold promise for mitigating bias with at least loss in model accuracy.\n\nThen, there is a valid ethical basis upon which this research into the effects of alternative data sources in credit risk assessment can be pursued. Although social media activities and transaction histories present quite valuable advantages concerning improvement in credit risk predictions, data privacy and regulatory compliance issues implicate essential considerations, especially under rigid data protection laws such as the GDPR, as identified by (Goodell et al., 2021). Projects soon must be considered considering the impact of more privacy-preserving AI techniques, federated learning, and differential private ways to train AI models from decentralized data without touching borrower privacy.\n\nFinally, future studies need to be directed at enhancing the explainability of AI models, since they are still opaque despite the high accuracy of deep learning models. New developments in explainability techniques which can make complex models more interpretable would thus be crucial in developing AI-driven credit assessments that are transparent, trustworthy, and adherent to regulatory standards.\n\n5. Conclusions\n\nThe present study has explored the transformative power of AI and ML in credit risk assessment by comparing various state-of-the-art techniques with conventional models like Logistic Regression. The results show more significant improvements in predictive accuracy and identification of risk features for AI models, particularly Random Forest and Neural Networks. These models outperform conventional ones by processing and integrating a large volume of structured and unstructured information to attain much deeper insights into the behavior of borrowers (Breeden, 2020; Addo et al., 2018).\n\nAnother key takeaway is that incorporating alternative data sources, such as social media activity, transaction histories, and geolocation data, improves the performance of AI models. Moreover, a better breakthrough of predictive power is achieved that assists institutions in making more accurate estimates of credit risk, especially for people who have limited formal credit records. These AI models synthesize these sources of data in a way that would not otherwise be available for evaluating borrower risk, hence making the models valuable for underrepresented or emerging market borrowers.\n\nHowever, there are some issues that AI models need to go through before they can earn inconspicuous acceptance. First, transparency in the decision-making process, such as Neural Networks, is imperative due to its lack. Their black-box nature antagonizes explanations of how the model derived a particular decision, which is inappropriate for regulated industries. This research used Explainable AI techniques, including Shapley values and LIME, for enhanced interpretability according to feature importance and the building of overall trust in AI-driven decisions.\n\nIt is also important to consider bias and equity. Models built on historical data run the risk of perpetuating bias, leading to discriminatory outcomes. Fairness Through Unawareness and fairness constraints are among the various techniques applied in this study, but more research needs to be done to develop methods that reduce bias with minimal impact on performance. Showing compliance with regulatory standards goes hand in hand with responsible AI adoption.\n\n5.1. Future of AI in Credit Risk Assessment\n\nWhile the future of AI in credit risk assessment has a promising direction, several key considerations must be met to ensure that this technology is used responsibly and effectively. One auspicious direction is the development of hybrid models that take advantage of the predictive power of AI but combine such powers with the interpretability of traditional models. Adding AI techniques to the models with Logistic Regression or decision trees may give some accuracy of AI but retain the transparency to satisfy regulatory compliance. Hybrid models can thus provide a practical alternative for financial institutions that may be quite apprehensive about their use.\n\nAnother interesting path might be investigating the development of eventual fairness-aware AI models that can reduce bias without sacrificing accuracy. Techniques like adversarial de-biasing and fair representation learning are promising methods for mitigating bias with minimal model performance compromise. Future research should investigate such techniques within the context of credit risk assessment, especially regarding their effectiveness in reducing bias in real-world applications.\n\nAnother important direction of research is related to the ethical impact of using alternative data sources in credit risk assessment. While alternative data offers advantages in making credit risk predictions, it also raises major concerns about data privacy and regulatory issues, especially with strict data protection laws such as GDPR. Further research is warranted, which would apply newly developed AI privacy-preserving techniques, including federated learning and differential privacy, to enable the training of an AI model on decentralized data without violation of the borrower’s privacy.\n\nExplainability in AI models is a key element of any responsible use of AI within credit risk evaluation. While the Shapley values and LIME are indeed especially useful tools to explain the decisions of a model, much remains to be undertaken regarding the development of more sophisticated techniques of explainability that may render complex models, such as Neural Networks, more transparent. As AI continues to develop, it will be relevant to ensure that models remain interpretable, trustworthy, and comply with regulatory standards (Breeden, 2020).\n\n5.2. Realistic Recommendations to Financial Institutions\n\nBased on the findings of this study, the following are some practical recommendations that any financial institution willing to adopt AI in its credit risk assessment processes can consider. First, institutions should consider the adoption of hybrid models that provide a balancing act between the accuracy of AI and the interpretability of traditional models. These models can be a workable solution for organizations that have to comply with regulatory requirements while leveraging the predictive power of AI.\n\nSecond, fairness and equity should be inherent concerns of any AI model within financial institutions. This means periodic auditing of models for bias elimination, constraining models at development for fairness, and adherence to relevant regulations on the subject. Institutions should also be open towards borrowers regarding data considered during credit assessment and give meaningful reasons if adverse decisions are being passed.\n\nThirdly, institutions should consider the use of alternative data sources to populate their credit risk profiles. This will be particularly important in the case of borrowers with limited or no formal credit history. Nonetheless, this must be pursued while giving full respect to borrowers’ privacy and keeping in mind the needs of data protection regulations such as the GDPR. Privacy-preserving AI techniques, such as federated learning, should be considered to mitigate the risks associated with using alternative data.\n\nThe second is that it must make investments in explainable AI techniques, which shall help and work towards more transparency of the AI models that one uses. These techniques can include Shapley values and LIME in enabling institutions to explain their credit decisions to regulators and customers and therefore further build trust and ensure that regulatory standards are met. With increasing complexity, AI models would require the clear and interpretable development of institutions that can be trusted by all stakeholders.\n\n5.3. Future Directions and Considerations\n\nAI and ML represent the future of credit risk assessment: more enabling correct decisions, lower default rates, and increased credit accessibility (Milojević & Redzepagic, 2021). Minimizing transparency, bias, and fairness challenges, along with compliance issues, paves the path for successful adoption. Successful implementation of hybrid models, fairness-aware approaches, and techniques for explainable AI will enable institutions to harness all the benefits of AI while never compromising on ethical grounds or regulatory compliance.\n\nWhile much fair, private, and explainable AI models are developed today, with the rapid development of AI, future research in this area should be channeled to make AI models fairer, more private, more explainable for a truly more equitable financial system that exists between borrowers and lenders.\n\nConflicts of Interest\n\nThe author declares no conflicts of interest regarding the publication of this paper.\n\nReferences\n\nCopyright © 2025 by authors and Scientific Research Publishing Inc.\n\nThis work and the related PDF file are licensed under a Creative Commons Attribution 4.0 International License.\n\nPDF Machine Learning and Credit Risk Modelling - S&P Global : \nError\n\nSecurity Controls Triggered\n\nIf you have reached this page in error, please contact customer support at customercare@spglobal.com\n",
        "Machine learning models employed in Customer Segmentation and Targeting": "Implementing Customer Segmentation Using Machine Learning ... - Neptune : \nShow off your best (or worst!) learning curve for a chance to win a custom-made gift box 🎁 → Learn more\n\nImplementing Customer Segmentation Using Machine Learning [Beginners Guide]\n\nThese days, you can personalize everything. There’s no one-size-fits-all approach. But, for business, this is actually a great thing. It creates a lot of space for healthy competition and opportunities for companies to get creative about how they acquire and retain customers.\n\nOne of the fundamental steps towards better personalization is customer segmentation. This is where personalization starts, and proper segmentation will help you make decisions regarding new features, new products, pricing, marketing strategies, even things like in-app recommendations.\n\nBut, doing segmentation manually can be exhausting. Why not employ machine learning to do it for us? In this article, I’ll tell you how to do just that.\n\nWhat is customer segmentation\n\nCustomer segmentation simply means grouping your customers according to various characteristics (for example grouping customers by age).\n\nIt’s a way for organizations to understand their customers. Knowing the differences between customer groups, it’s easier to make strategic decisions regarding product growth and marketing.\n\nThe opportunities to segment are endless and depend mainly on how much customer data you have at your use. Starting from the basic criteria, like gender, hobby, or age, it goes all the way to things like “time spent of website X” or “time since user opened our app”.\n\nThere are different methodologies for customer segmentation, and they depend on four types of parameters:\n\nGeographic customer segmentation is very simple, it’s all about the user’s location. This can be implemented in various ways. You can group by country, state, city, or zip code.\n\nDemographic segmentation is related to the structure, size, and movements of customers over space and time. Many companies use gender differences to create and market products. Parental status is another important feature. You can obtain data like this from customer surveys.\n\nBehavioral customer segmentation is based on past observed behaviors of customers that can be used to predict future actions. For example, brands that customers purchase, or moments when they buy the most. The behavioral aspect of customer segmentation not only tries to understand reasons for purchase but also how those reasons change throughout the year.\n\nPsychological segmentation of customers generally deals with things like personality traits, attitudes, or beliefs. This data is obtained using customer surveys, and it can be used to gauge customer sentiment.\n\nAdvantages of customer segmentation\n\nImplementing customer segmentation leads to plenty of new business opportunities. You can do a lot of optimization in:\n\nLet’s discuss these benefits in more depth.\n\nNobody likes to invest in campaigns that don’t generate any new customers. Most companies don’t have huge marketing budgets, so that money has to be spent right. Segmentation enables you to target customers with the highest potential value first, so you get the most out of your marketing budget.\n\nCustomer segmentation helps you understand what your users need. You can identify the most active users/customers, and optimize your application/offer towards their needs.\n\nProperly implemented customer segmentation helps you plan special offers and deals. Frequent deals have become a staple of e-commerce and commercial software in the past few years. If you reach a customer with just the right offer, at the right time, there’s a huge chance they’re going to buy. Customer segmentation will help you tailor your special offers perfectly.\n\nThe marketing strategy can be directly improved with segmentation because you can plan personalized marketing campaigns for different customer segments, using the channels that they use the most.\n\nBy studying different customer groups, you learn what they value the most about your company. This information will help you create personalized products and services that perfectly fit your customers’ preferences.\n\nIn the next section, we’re going to discuss why machine learning for customer segmentation.\n\nMachine Learning for customer segmentation\n\nMachine learning methodologies are a great tool for analyzing customer data and finding insights and patterns. Artificially intelligent models are powerful tools for decision-makers. They can precisely identify customer segments, which is much harder to do manually or with conventional analytical methods.\n\nThere are many machine learning algorithms, each suitable for a specific type of problem. One very common machine learning algorithm that’s suitable for customer segmentation problems is the k-means clustering algorithm. There are other clustering algorithms as well such as DBSCAN, Agglomerative Clustering, and BIRCH, etc.\n\nWhy would you implement machine learning for customer segmentation?\n\nMore time\n\nManual customer segmentation is time-consuming. It takes months, even years to analyze piles of data and find patterns manually.  Also if done heuristically, it may not have the accuracy to be useful as expected.\n\nCustomer segmentation used to be done manually and wasn’t too precise. You’d manually create and populating different data tables, and analyze the data like a detective with a looking glass. Now, it’s much better (and relatively easy thanks to rapid progress in ML) to just use machine learning, which can free up your time to focus on more demanding problems that require creativity to solve.\n\nEase of retraining\n\nCustomer Segmentation is not a “develop once and use forever” type of project. Data is ever-changing, trends oscillate, everything keeps changing after your model is deployed. Usually, more labeled data becomes available after development, and it’s a great resource for improving the overall performance of your model.\n\nThere are many ways to update customer segmentation models, but here are the two main approaches:\n\nBetter scaling\n\nMachine learning models deployed in production support scalability, thanks to cloud infrastructure. These models are quite flexible for future changes and feedback. For example, consider a company that has 10000 customers today, and they’ve implemented a customer segmentation model. After a year, if the company has 1 million customers, then ideally we don’t need to create a separate project to handle this increased data. Machine Learning models have the inherent capability to handle more data and scale in production.\n\nHigher accuracy\n\nThe value of an optimal number of clusters for given customer data is easy to find using machine learning methods like the elbow method. Not only the optimal number of clusters but also the performance of the model is far better when we use machine learning.\n\nRead also\n\nF1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?\n\nExploring customer dataset and its features\n\nLet’s analyze a customer dataset. Our dataset has 24,000 data points and four features. The features are:\n\nIn the upcoming section, we’ll pre-process this dataset.\n\nPre-processing the dataset\n\nBefore feeding the data to the k-means clustering algorithm, we need to pre-process the dataset. Let’s implement the necessary pre-processing for the customer dataset.\n\nMoving on, we’ll implement our k-means clustering algorithm in Python.\n\nMight be useful\n\nA Comprehensive Guide to Data Preprocessing\n\nImplementing K-means clustering in Python\n\nK-Means clustering is an efficient machine learning algorithm to solve data clustering problems. It’s an unsupervised algorithm that’s quite suitable for solving customer segmentation problems. Before we move on, let’s quickly explore two key concepts\n\nUnsupervised Learning\n\nUnsupervised machine learning is quite different from supervised machine learning. It’s a special kind of machine learning algorithm that discovers patterns in the dataset from unlabelled data.\n\nUnsupervised machine learning algorithms can group data points based on similar attributes in the dataset. One of the main types of unsupervised models is clustering models.\n\nNote that, supervised learning helps us produce an output from the previous experience.\n\nClustering algorithms\n\nA clustering machine learning algorithm is an unsupervised machine learning algorithm. It’s used for discovering natural groupings or patterns in the dataset. It’s worth noting that clustering algorithms just interpret the input data and find natural clusters in it.\n\nSome of the most popular clustering algorithms are:\n\nIn the following section, we’re going to analyze the customer segmentation problem using the k-means clustering algorithm and machine learning. However, before that, let’s quickly discuss why we’re using the k-means clustering algorithm.\n\nWhy use K-means clustering for customer segmentation?\n\nUnlike supervised learning algorithms, K-means clustering is an unsupervised machine learning algorithm. This algorithm is used when we have unlabelled data. Unlabelled data means input data without categories or groups provided. Our customer segmentation data is like this for this problem.\n\nThe algorithm discovers groups (cluster) in the data, where the number of clusters is represented by the K value. The algorithm acts iteratively to assign each input data to one of K clusters, as per the features provided. All of this makes k-means quite suitable for the customer segmentation problem.\n\nGiven a set of data points are grouped as per feature similarity. The output of the K-means clustering algorithm is:\n\nAt the end of implementation, we’re going to get output such as a group of clusters along with which customer belongs to which cluster.\n\nFirst, we need to implement the required Python libraries as shown in the table below.\n\nWe’ve imported the pandas, NumPy sklearn, plotly and matplotlib libraries. Pandas and NumPy are used for data wrangling and manipulation, sklearn is used for modelling, and plotly along with matplotlib will be used to plot graphs and images.\n\nAfter importing the library, our next step is to load the data in the pandas data frame. For this, we’re going to use the read_csv method of pandas.\n\nAfter loading the data, we need to define the K- means model. This is done with the help of the KMeans class that we imported from sklearn, as shown in the code below.\n\nAfter defining the model, we want to train is using a training dataset. This is implemented with the use of the fit method, as shown in the code below.\n\nNote that we’re passing three features to the fit method, namely products_purchased, complains, and money_spent.\n\nThough we have trained a K-means model up to these points, we haven’t found the optimal number of clusters required in this case of customer segmentation. Finding the optimal number of clusters, for the given dataset is important for producing a high-performant k-means clustering model.\n\nIn the upcoming section, we’re going to find the optimal number of clusters of the given dataset and then re-train the k-means clustering model with these optimal values of k. This will produce our final model.\n\nFinding the optimal number of clusters\n\nFinding the optimal number of clusters is one of the key tasks when implementing a k-means clustering algorithm. It’s worth noting that a k-means clustering model might converge for any value of K, but at the same time, not all values of K will produce the best model.\n\nFor some datasets, data visualization can help understand the optimal number of clusters, but this doesn’t apply to all datasets. We have a few methods, such as the elbow method, gap statistic method, and average silhouette method, to assess the optimal number of clusters for a given dataset. We’ll discuss them one by one.\n\nWe’re going to use the elbow method. The K-means clustering algorithm clusters data by separating given data points in k groups of equal variances. This effectively minimizes a parameter named inertia. Inertia is nothing but within-cluster sum-of-squares distances in this case.\n\nWhen we use the elbow method, we gradually increase the number of clusters from 2 until we reach the number of clusters where adding more clusters won’t cause a significant drop in the values of inertia.\n\nThe stage at this number of clusters is called the elbow of the clustering model. We’ll see that in our case it’s K =5.\n\nFor implementing the elbow method, the below function named “try_different_clusters” is created first. It takes two values as input:\n\nThe method try_different_clusters is called using the below code, where we pass the values of K from 1 to 12 and calculate the inertia for each value of k.\n\nUsing the below code, we plot the value of K (on the x-axis) against corresponding values of inertia on the Y-axis.\n\nWe can generate the below plot using the above code. The elbow of the code is at K=5. We have chosen 5 as if we increase the number of clusters to more than 5, there is very small change in the inertia or sum of the squared distance.\n\nOptimal value of K = 5\n\nThe stage at which the number of clusters is optimal is called the elbow of the clustering model. For example, in the below image, the elbow is at five clusters (K =5). Adding more than 5 clusters will cause the creation of an inefficient or less performant clustering model.\n\nAs discussed before, we need to train the k-means clustering model again with the optimal number of clusters found.\n\nNote that we’re using the fit_predict method to train the model.\n\nIn the next section, we’re going to discuss how to visualize customer segmentation clusters in three dimensions.\n\nVisualizing customer segments\n\nIn this section, we’ll be implementing some code using plotly express. This way we’ll visualize the clusters in three dimensions, formed by our k-means algorithm. Plotly express is a library based on plotly that works on several types of datasets and generates highly-styled plots.\n\nFirst, let’s add a new column named ‘clusters’ to the existing customer data dataset. This column will be able to tell which customer belongs to what cluster.\n\nNote that we’re using NumPy expm1 methods here. NumPy expm1 function returns the exponential value of minus one for each element given inside a NumPy array as output. Therefore, the np.expm1 method accepts arr_name and out arguments and then returns the array as outputs.\n\nAfter adding the new column, named clusters, the customer data dataset will look as below.\n\nFinally, we’re going to use the below code to visualize the five clusters created. This is done using plotly with the express library.\n\nPlotly is a Python library used for graphing, statistics, plotting, and analytics. This can be used along with Python, R, Julia, and other programming languages. Plotly is a free and open-source library.\n\nPlotly Express is a high-level interface over Plotly, that works on several types of datasets and generates highly-styled plots.\n\nThe plotly.express class has functions that can produce entire figures in one go. Generally, it’s referred to as px. It’s worth noting plotly express is the built-in module of the plotly library. This is the starting point of creating the most common plots as recommended. Note that each plotly express function creates graph objects internally and returns plotly.graph_objects.\n\nA graph created by a single method call using plotly express can be also created using graph objects only. However, in that case, it takes around 5 to 100 times as much code.\n\nAs the 2D scatter plot, px.scatter plots individual data in a two-dimensional space, and the 3D method px.scatter_3d plots individual data in a three-dimensional space.\n\nVisualization of clusters of data points is very important. Various edges of the graph provide a quick view of the complex input data set.\n\nConclusion\n\nIt’s not wise to serve all customers with the same product model, email, text message campaign, or ad. Customers have different needs. A one-size-for-all approach to business will generally result in less engagement, lower-click through rates, and ultimately fewer sales. Customer segmentation is the cure for this problem.\n\nFinding an optimal number of unique customer groups will help you understand how your customers differ, and help you give them exactly what they want. Customer segmentation improves customer experience and boosts company revenue. That’s why segmentation is a must if you want to surpass your competitors and get more customers. Doing it with machine learning is definitely the right way to go.\n\nIf you made it this far, thanks for reading!\n\nWas the article useful?\n\nMore about Implementing Customer Segmentation Using Machine Learning [Beginners Guide]\n\nExplore more content topics:\n\nTop articles, case studies, events (and more) in your inbox every month.\n\nGet Newsletter\n\nCopyright © 2025 Neptune Labs. All rights reserved.\n\nCustomizing Marketing Strategies with ML-Driven Customer Segmentation : \nCustomizing Marketing Strategies with ML-Driven Customer Segmentation\n\nIntroduction\n\nThe rise of machine learning (ML) technologies has revolutionized the landscape of marketing strategies. Companies today have access to vast amounts of data that lead to insights far beyond traditional marketing methods. Customer segmentation, the practice of dividing a customer base into distinct groups based on various criteria, is now enhanced significantly through machine learning. By leveraging ML algorithms, businesses can gain a deeper understanding of their customers, uncover hidden patterns, and tailor marketing efforts uniquely suited to each segment.\n\nThis article aims to delve into the importance and mechanics of using ML-driven customer segmentation to customize marketing strategies effectively. We will explore how machine learning models analyze vast datasets to identify characteristics and behaviors of different customer segments, refocusing marketing strategies based on these insights. We will discuss key methodologies, benefits, challenges, and future trends related to this advanced approach to customer-driven marketing.\n\nUnderstanding Customer Segmentation\n\nCustomer segmentation is a critical marketing strategy that allows businesses to categorize customers based on specific criteria. Traditionally, companies segmented their audiences based on demographic data such as age, gender, location, and purchase history. However, this approach often fails to capture the nuanced behaviors and preferences of individual customers. Herein lies the role of machine learning, which offers unprecedented opportunities for refining and revolutionizing how businesses approach segmentation.\n\nTypes of Customer Segmentation\n\nThere are multiple types of customer segmentation, and adopting a machine learning framework enables businesses to tackle complex segmentation more effectively. The commonly recognized categories include:\n\nDemographic Segmentation: This is the most basic form and involves segmenting customers based on easily identifiable metrics such as age, gender, income level, education, and occupation. While these metrics are essential, they often do not reveal the underlying motivations driving customer behavior.\n\nGeographic Segmentation: This focuses on segmenting customers based on their geographical location. Understanding regional preferences allows marketers to customize product offerings and messaging to resonate specifically with local clientele. However, geographic segmentation may overlook other pertinent factors like interests and behaviors.\n\nBehavioral Segmentation: Perhaps one of the most critical segments, behavioral segmentation categorizes customers based on their interactions and behaviors with the brand—such as purchase history, product usage, and brand loyalty. This type can help identify high-value customers and tailor experiences based on their behavioral patterns. By employing ML algorithms, businesses can analyze this data at scale, using advanced analytics to understand behaviors dynamically.\n\nPsychographic Segmentation: This dives deeper into the personalities, values, interests, and lifestyles of customers. Although it may be less common, psychographic segmentation plays a vital role, particularly in creative marketing campaigns that resonate emotionally with customers. Combining psycho-graphic data with ML can uncover consumer motivations that traditional methods might miss.\n\nThe Role of Machine Learning in Customer Segmentation\n\nMachine learning introduces advanced analytical techniques that remove the limitations of traditional segmentation approaches. ML algorithms, particularly clustering algorithms (e.g., K-Means, hierarchical clustering), can identify customer segments based on multiple attributes and behaviors concurrently. By processing vast datasets, these algorithms can discern intricate patterns and relationships that are otherwise undetectable.\n\nA prime example of a machine learning model's application in customer segmentation is the RFM (Recency, Frequency, Monetary) approach. By analyzing how recently customers made a purchase, how frequently they buy, and the amount they spend, businesses can cluster their audience into different segments with tailored strategies. Moreover, as new data comes in, machine learning allows for continual refinement of segments without manual oversight, ensuring that marketing strategies remain relevant and effective.\n\nBenefits of ML-Driven Customer Segmentation\n\nThe application of ML-driven customer segmentation offers numerous advantages over traditional methods. Enhanced customer understanding is at the forefront, enabling businesses to offer personalized marketing experiences. With precise segments in place, companies can create targeted marketing campaigns that resonate more profoundly with different groups.\n\nEnhanced Targeting and Personalization\n\nIn a marketplace saturated with homogenized marketing messages, standing out is imperative. Utilizing ML-powered segmentation achieves extensive precision in targeting. Marketing campaigns tailored to specific customer characteristics tend to yield higher engagement rates. For instance, an online retail store can identify which segments respond best to discounts for various products and adjust their offerings accordingly, ensuring they reach the right audience at the right time.\n\nPersonalization extends beyond mere discounts; it encompasses delivering customized content, suggestions, and recommendations based on individual preferences. By examining user behavior patterns, machine learning algorithms can suggest relevant products, which not only drives sales but also enhances customer satisfaction and loyalty.\n\nIncreased Efficiency in Marketing Strategies\n\nAdopting a machine-learning approach to customer segmentation significantly increases marketing efficiency. In traditional segmentation, marketers often relied on a trial-and-error methodology to identify and reach their target audience. However, through the automatic, data-backed identification of customer segments, businesses can reduce wasted ad spend and allocate resources more effectively.\n\nMoreover, ML-driven segmentation enables ongoing optimization. As customer preferences evolve or market trends shift, machine learning models can process new data and revise segments accordingly, allowing businesses to adapt swiftly without undergoing tedious manual analysis. This agility ensures brands remain competitive and engaged with customers in a rapidly changing marketplace.\n\nImproved Customer Retention and Loyalty\n\nOrganizations that effectively employ ML-driven customer segmentation can expect to see notable improvements in customer retention and loyalty. By developing a solid understanding of their customers’ needs and tailoring marketing messages accordingly, brands foster deep emotional connections with their audience. Personalization makes customers feel valued—leading to higher purchase frequency and loyalty.\n\nAdditionally, ML algorithms can help identify customers at risk of churning by analyzing behavioral signals indicating dissatisfaction or disengagement. Here, proactive engagement strategies can be implemented, such as personalized outreach, special promotions, or opportunities for feedback, ultimately preventing customer loss.\n\nChallenges in ML-Driven Customer Segmentation\n\nWhile the potential of ML-driven customer segmentation is immense, there are challenges and hurdles that organizations must navigate. Notably, complications can arise in data collection, model training, and the ethical implications surrounding customer data privacy.\n\nData Quality and Accessibility\n\nOne of the foremost challenges is ensuring data quality and accessibility. Machine learning relies heavily on quality data, as inaccuracies or missing data points can skew insights. Organizations must invest in solid data governance practices, ensuring datasets are cleaned, normalized, and updated consistently. Furthermore, obtaining comprehensive data from disparate sources can be cumbersome, particularly for smaller businesses with limited resources.\n\nEnsuring data privacy and compliance with regulations such as GDPR is crucial, as companies need to balance effective data collection strategies with customer trust. When machine learning practices infringe on privacy, consumer backlash can occur, resulting in damaged brand reputation.\n\nExpertise and Model Complexity\n\nImplementing ML-driven segmentation also requires a level of expertise that not all companies possess. Developing effective machine learning models demands specialized skills in data science and analytics, along with familiarity with machine learning concepts and algorithms. This can create a barrier for some organizations, particularly small to medium-sized enterprises, lacking the technical expertise or budget to invest in such capabilities.\n\nMoreover, the complexity of certain machine learning algorithms might lead to challenges in interpretability. Organizations need to understand the fundamentals of how models derive conclusions to take appropriate actionable steps. Creating transparency in these processes can build trust both internally and with consumers.\n\nIntegrating with Existing Systems\n\nFinally, integrating machine learning models into existing customer relationship management (CRM) or marketing systems may prove difficult. Legacy systems may lack the infrastructure necessary to support advanced analytics, necessitating potential upgrades or replacements.\n\nSeamless integration of data sources and aligning marketing strategies with insights gleaned from ML algorithms require careful planning and implementation. Companies must also train their personnel on how to leverage these technologies effectively to witness maximum returns on investment.\n\nConclusion\n\nCustomizing marketing strategies through ML-driven customer segmentation stands as one of the most transformative approaches businesses can adopt in today's competitive landscape. Through a nuanced understanding of customer behaviors, preferences, and needs, machine learning empowers organizations to create tailored marketing campaigns that resonate meaningfully with their target audiences. This not only enhances customer engagement but also drives sales, retention, and brand loyalty.\n\nHowever, as we explored, the journey to effective segmentation is not without its challenges. Ensuring data quality, navigating technical barriers, and addressing privacy and integration concerns are all essential facets that organizations must consider. By approaching these obstacles thoughtfully and strategically, companies can fully harness the potential of ML-driven customer segmentation, positioning themselves for success in an increasingly data-centric marketing world.\n\nThe future of marketing lies in the adoption of advanced technologies, and machine learning will be at the forefront of this revolution. Businesses that embrace these innovations will not only improve their marketing efforts but also create meaningful connections with customers, setting the stage for unmatched growth and success in the years to come.\n\nIf you want to read more articles similar to Customizing Marketing Strategies with ML-Driven Customer Segmentation, you can visit the Customer Segmentation category.\n\nYou Must Read\n\nOptimizing VR Performance by Analyzing User Feedback with AI\n\nUrban Land Use Predictions: Leveraging Machine Learning Approaches\n\nThe Role of Neural Networks in Modern Chatbot Functionality\n\nCategories\n\nRelated Posts\n\nExploring Multi-Modal Approaches in Face Recognition Applications\n\nThe Role of Machine Learning in Modern Anomaly Detection Systems\n\nAI and ML in Augmented Reality: Current Trends and Future Prospects\n\nTerms and Conditions\n\nPrivacy Policy\n\nCookie Policy\n\nAbout Us\n\nContact Us\n\nCreating Customer Segmentation Using Machine Learning: A Complete Guide ... : \nCreating Customer Segmentation Using Machine Learning: A Complete Guide 2024\n\nIn today’s competitive market, understanding your customers is crucial for delivering personalized experiences and maximizing growth. One of the most effective ways to achieve this is through customer segmentation, which involves dividing your customer base into distinct groups based on common characteristics. With the rise of machine learning, this process has become more sophisticated, enabling businesses to gain deeper insights and optimize their strategies.\n\nIn this article, we’ll explore customer segmentation using machine learning, focusing on how it works, the different types of segmentation, and how to create effective models.\n\nWhat is Customer Segmentation?\n\nCustomer segmentation is the practice of categorizing customers into groups that share similar traits or behaviors.\n\nThe goal is to tailor marketing strategies, product offerings, and communications to each segment, leading to more effective targeting and improved customer satisfaction. Traditionally, customer segmentation was based on simple criteria such as demographics or purchase history.\n\nHowever, with the advent of machine learning, segmentation can now be data-driven, uncovering more complex patterns and customer behaviors.\n\nTypes of Customer Segmentation\n\nThere are several types of customer segmentation, each offering unique insights:\n\nEach type of segmentation serves a different purpose, allowing you to tailor your customer segmentation strategy based on specific goals and data availability.\n\nCustomer Segmentation Models Using Machine Learning\n\nCustomer segmentation models automatically leverage machine learning algorithms to automatically group customers based on shared attributes. One of the most popular unsupervised learning techniques used for this purpose is clustering. Clustering allows the algorithm to group similar data points together without predefined categories. A common algorithm for this is K-Means Clustering.\n\nBy using machine learning algorithms, you can automatically discover customer segments based on various factors such as purchasing behavior, engagement levels, or product preferences.\n\nCustomer Segmentation Examples\n\nTo illustrate the power of machine learning in customer segmentation analysis, let’s consider a few practical examples:\n\nHow to Create a Customer Segmentation Strategy with Machine Learning\n\nBuilding a customer segmentation strategy using machine learning involves several key steps:\n\nBenefits of Machine Learning-Based Customer Segmentation\n\nLeveraging Machine Learning for Better Segmentation\n\nCustomer segmentation using machine learning offers a data-driven approach to understanding and targeting your customers more effectively. By blending data, normalizing it, and applying unsupervised learning algorithms like K-Means, you can uncover valuable insights that traditional methods might miss. This results in more precise marketing strategies, improved customer experiences, and ultimately, higher ROI.\n\nReady to optimize your customer segmentation strategy? Integrating machine learning into your approach allows you to create highly personalized campaigns that resonate with each unique customer segment and drive better business results.\n\nWant bang-up emails in your inbox? ⚡️\n\nPropensity to Buy Algorithms: Technical Foundations and Applications in E-Commerce\n\nPropensity to Buy Algorithms: Transforming Retail and Fashion with Data-Driven Insights\n\nAbout\n\nWork With Me\n\nClients\n\nBlog\n\nWorkflows\n\nContact\n\nServices\n\nFree RAG & Workflows\n\nFree Template Looker Studio for Data Analysis\n\nPower Hours\n\nEmail Marketing Automation & AI\n\nAI Growth Program\n\nPropensity to Buy Algorithms: Technical Foundations and Applications in E-Commerce\n\nPropensity to Buy Algorithms: Transforming Retail and Fashion with Data-Driven Insights\n\nAlgoritmi di Propensity to Buy: Una Panoramica Completa\n\nEmbedding customized product recommendations in E-Commerce\n\nReal-World applications of RAG in Retail: drive customized product recommendation system\n\nWhat is RAG pipeline?\n\nHow does RAG personalize customer experiences in retail\n\nCome incrociare i dati degli acquisti con le email dei clienti per migliorare il ROAS\n\nCookie Policy\n\nPrivacy Policy\n\nNotice\n\nWe and selected third parties use cookies or similar technologies for technical purposes and, with your consent, for experience, measurement and “marketing (personalised ads)” as specified in the cookie policy.\n\nYou can freely give, deny, or withdraw your consent at any time by accessing the preferences panel. Denying consent may make related features unavailable.\n\nUse the “Accept” button to consent. Use the “Reject” button or close this notice to continue without accepting.\n",
        "Machine learning models employed in Financial Performance Prediction": "Machine learning for financial forecasting, planning and analysis ... : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nMachine learning for financial forecasting, planning and analysis: recent developments and pitfalls\n\nYou have full access to this\nopen access\narticle\n\n40k Accesses\n\n1 Altmetric\n\nExplore all metrics\n\nAbstract\n\nThis article is an introduction to machine learning for financial forecasting, planning and analysis (FP&A). Machine learning appears well suited to support FP&A with the highly automated extraction of information from large amounts of data. However, because most traditional machine learning techniques focus on forecasting (prediction), we discuss the particular care that must be taken to avoid the pitfalls of using them for planning and resource allocation (causal inference). While the naive application of machine learning usually fails in this context, the recently developed double machine learning framework can address causal questions of interest. We review the current literature on machine learning in FP&A and illustrate in a simulation study how machine learning can be used for both forecasting and planning. We also investigate how forecasting and planning improve as the number of data points increases.\n\nSimilar content being viewed by others\n\nIntegrating Decision Analytics and Advanced Modeling in Financial and Economic Systems Through Artificial Intelligence\n\nApplication of Machine Learning in Financial Asset Price Prediction and Allocation\n\nData Mining in Finance: Current Advances and Future Challenges\n\nExplore related subjects\n\nAvoid common mistakes on your manuscript.\n\n1 Introduction\n\nAccurate financial forecasts and plans for effective and efficient resource allocation are core deliverables of the finance function in modern companies. Particularly, in volatile or fast-evolving market environments, fast and reliable forecasting and planning are crucial (Becker et al. 2016). High-quality forecasting is among the defining characteristics of strong finance functions (Roos et al. 2020). It is therefore hardly surprising that most larger companies have dedicated teams for financial planning and analysis (FP&A) within their finance function.\n\nThe increasing availability of big data, coupled with new analysis techniques, provides an opportunity for FP&A to generate more and better insights at a faster pace, generating more value for the company. Machine learning is a set of techniques developed in computer science and statistics that appear particularly well suited to this context. The aim of our paper is to show how machine learning can be used for FP&A and which pitfalls can arise in the process. Machine learning has been applied successfully to a variety of predictive tasks, including fraud detection and financial forecasting. Planning and resource allocation, however, represent tasks of a different nature, because they require understanding the effect of an active intervention in a system, such as the market for a product. For this reason, they are causal problems, which are harder to model with machine learning. A large field within machine learning revolves around pattern recognition. Patterns in data, based on correlations, are learned and then used for predictions. In causal tasks, an understanding of the underlying (causal) mechanisms is important when evaluating the effects of interventions (e.g., the implementation of a new business strategy). The emerging field of causal machine learning uses machine learning algorithms for such questions. For instance, the recently developed double machine learning framework reduces the impact of imperfect model specifications, which are hard to avoid in practice in the context of causal analysis.\n\nWe structure this paper as follows. In Sect. 2, we briefly review the role of FP&A. Section 3 provides a short, focused overview of machine learning. In particular, we highlight the pitfall of not distinguishing between forecasting and planning. In Sect. 4, we present the results of our literature review, which finds surprisingly few publications of machine learning applications in FP&A. In Sect. 5, we describe and provide the results from a simulation study. We compare a machine learning technique, the lasso, to a linear regression based on the ordinary least squares (OLS) method. In our analysis, we refer back to the distinction between forecasting and planning from Sect. 3, and show how the results differ between the lasso and OLS for both tasks. Finally, we also quantify the benefit of additional data in this simulation.\n\n2 The role of FP&A\n\nGiven the importance of financial forecasting, planning and analysis (FP&A) in modern corporations, most larger companies have dedicated teams for these tasks within their finance function, even though the exact organizational design and naming of the department may vary.\nFootnote\n1\n\nThe overarching goal of FP&A is to inform and support decisions of management and the board of directors (Oesterreich et al. 2019). FP&A pursues this goal via different routes, helping determine which projects in the company portfolio create value (and are consequently worth funding), and preparing company-wide forecasts and financial plans to ensure that the company can reach its financial goals in the short and long term (Roos et al. 2019). Investments in research and development (R&D) or the expansion of production capacity are balanced with financial obligations to debt holders or equity investors and tax authorities (Brealey et al. 2020). Financial plans are also an important step in the translation of a company’s strategic priorities into concrete operational actions. These actions contribute to focusing the organization and the deployed resources behind common goals.\n\nAnalyzing the business environment and business dynamics is an integral part of the work performed by FP&A. The insights generated through such analysis can inform the development of forecasts and plans and help in the assessment of how likely these plans are to succeed. During the the execution phase of plans, these insights allow FP&A to understand why actual results may deviate from the plan and to recommend corrective actions. This need for business acumen is likely to continue, even when advanced forecasting methods like those described in this article are used (Möller et al. 2020).\n\nThe time horizons considered for financial forecasts and plans usually range from 1 month to several years (Roos et al. 2019; Fischer 2009). The choice of time horizon depends on company-specific circumstances and objectives; for instance, stock-market listed companies typically put additional weight on quarterly figures. In practice, most companies create forecasts and plans for the next fiscal year (sometimes called a budget), which additionally can serve as a management control mechanism (Strauß and Zecher 2013). Rolling forecasts are another form of plan. These are characterized by regular updates, which are typically performed on a monthly or quarterly basis (Hansen 2011).\n\nFP&A relies in large part on quantitative analysis to generate forecasts and plans. Accounting systems are a major internal data source for FP&A (Garrison et al. 2006; Gray and Alles 2015), covering items related to sales (turnover), expenses, and balance sheet positions, which are especially important for cash flow analysis. Other important internal sources of data include those related to human resources (employee numbers, wage costs), supply chain and production (manufacturing costs at various levels of granularity), and R&D (product development costs, success rates, timelines).\n\nExternal data sources include market- or product-specific information, such as the size and development of the market and market shares. The exact nature and granularity of these data depends largely on the product or question under analysis, as well as the investment required to access the relevant data (Gray and Alles 2015). For instance, it is not uncommon in the consumer goods industry to have access to transaction-level data (Taddy 2019), covering one’s own and competitor products. However, information at this level of specificity is typically used by the marketing and sales department for product-specific tactics. In contrast, FP&A often uses macroeconomic indicators, including GDP, inflation and currency rates.\n\nThe development and spread of comprehensive, company-wide IT systems in recent decades has increased the amount and variety of data readily available to FP&A. Increasing digitalization will further accentuate this development, with big data as the crystallizing term. The “Three V’s”, a common framework to define big data (Laney 2001), allow us to look at the different dimensions that drive this development. First, the amount of information generated, captured and thus accessible for FP&A activities is growing (volume). Second, the speed of information creation and its accessibility is accelerating (velocity); as a consequence, the speed at which information must be analyzed and acted upon increases, too. This calls for automated, real-time analytics and evidence-based planning (Gandomi and Haider 2015). Third, more and more types of information are being gathered or generated and can be analyzed (variety); for instance, stock-market analysts apply sentiment analysis to extract information relevant to stock prices from text documents.\n\nIn addition, other dimensions of big data have been proposed (Gandomi and Haider 2015). In the context of FP&A, the additional dimensions of veracity and complexity appear especially relevant. Thanks to the more widespread use of digital tools, the need for data transparency and scrutiny within many companies is increasing, as well. In turn, the need to ensure data quality and reliability is growing (veracity). Moreover, (big) data are generated through multiple sources, both from inside and outside the company. This requires data cleaning, data matching and, ideally centralized storage, which facilitates accessibility (complexity).\n\nAs mentioned above, a key output of FP&A is financial forecasts and plans. For data that are more numerous, available more quickly, and are more diverse and of better quality than in the past, FP&A needs to choose adequate tools, such as those provided by machine learning.\n\n3 Introduction to machine learning\n\nWhile there is no uniform definition of machine learning, it can be described as a collection of methods that automatically build predictions from complex data (Taddy 2019). In essence, machine learning deploys a function-fitting strategy aiming to find a useful approximation of the function that underlies the predictive relationship between input and output data (Hastie et al. 2009). In this search for patterns in data (Bishop et al. 2006), which, to a large extent, is executed autonomously, machine learning draws on statistical tools and algorithmic approaches from computer science. In particular, machine learning aims to cope with the situation of high-dimensional data. High dimensionality occurs when the number of input variables (independent variables, features) used to predict the output (dependent) variable is large compared to the number of observations available. Classical statistical techniques do not work in this setting (Taddy 2019).\n\nThe three broad categories of machine learning are supervised learning, unsupervised learning and reinforcement learning. Supervised learning is concerned with predicting the value of an output variable based on the values of a set of input variables. For this, supervised learning relies on a set of input and output variables that are jointly observed for each data point (Hastie et al. 2009). A practical example is to predict the sales of a product using input variables such as time of the year, price level, advertising expenditures and availability of competitor products. In contrast, unsupervised learning consists only of a set of input observations for which the joint distribution is known. However, there is no observed output (response). The goal is to directly infer the properties of these observations (Hastie et al. 2009). Classifying customers into (previously unknown) customer archetypes based on their observed characteristics such as buying behavior, age, gender and socio-economic status is an example of unsupervised learning. In reinforcement learning, the algorithm performs a trial-and-error search to maximize a numeric reward signal, in direct interaction with its environment (Sutton and Barto 2018). By interacting with its environment, the algorithm creates its own data from which it can learn. Games such as checkers, chess and go are classical examples in which reinforcement learning is applied. Sometimes cited as a fourth category, semi-supervised learning falls between supervised and unsupervised learning, combining a small amount of fully labeled data as in supervised learning and a large amount of unlabeled data as in unsupervised learning. The objective is to improve supervised learning in situations in which labeled data are scarce (Zhu and Goldberg 2009). For the purposes of FP&A objectives, which mostly revolve around producing forecasts from a set of inputs and assumptions, the predominant choice is typically supervised methods.\n\nMachine learning methods appear especially suitable for the core FP&A task of forecasting because of their focus on predictive performance. These methods manage to identify generalizable patterns that work well on new data, i.e., data outside of the training sample (Mullainathan and Spiess 2017). Through their ability to identify complex structures that have not been specified in advance, they lend themselves to support a high degree of automation in the generation of forecasts. This flexibility has the additional advantage that many off-the-shelf algorithms perform surprisingly well on a variety of tasks. In addition, a large selection of machine learning algorithms are available and are technically easy to use (Mullainathan and Spiess 2017), making them attractive for practitioners.\n\nBesides forecasting, the second core task of FP&A is to provide recommendations for the design of financial plans and for potential corrective actions when deviations from plans occur. In statistical terminology, this requires causal inference techniques, which are fundamentally different from forecasting. Consider the trivial example of hotel occupancy rates and room prices (Athey 2018). High room prices coincide with high occupancy rates. Thus, price variations are strongly predictive of hotel occupancy. If the goal is to make a forecast, we do not need to be concerned with understanding why occupancy was high. However, if we want to recommend an action to increase the occupancy rate (an intervention) or imagine in retrospect what the occupancy rate would have been if the room rates had been different [“counterfactual” (Pearl and Mackenzie 2018) or “potential outcome” (Rubin 2005)], FP&A requires a causal understanding of the business dynamics. To conclude with this example, a plan consisting of a room price increase will not lead to higher occupancy. Most likely, prices have increased in the past in reaction to high demand, which was stimulated by other factors (e.g., the holiday season). While this trivial example seems obvious, it illustrates a major pitfall: many companies struggle in practice to identify truly causal measures for the effectiveness of their promotional activities. Blake et al. (2015) discuss this phenomenon in the context of large-scale field experiments conducted at the e-commerce platform eBay.\n\nFor interventional and counterfactual analysis, data-driven approaches need to produce reliable estimates for the parameters that govern the relationship between input and output variables. Machine learning algorithms are typically not built for this purpose. Historically, the machine learning community has pursued the goal of maximizing predictive performance as opposed to understanding model parameters (Taddy 2019); however, using a tool built for forecasting and assuming that it also possesses the properties required for causal inference in economic applications can be misleading (Mullainathan and Spiess 2017). Maximizing the predictive power of a model to use it for interventional analysis represents a major trap. Indeed, it may even be necessary to sacrifice predictive accuracy to arrive at a correct understanding of the relationships that are relevant for making decisions about interventions (Athey 2018). The current lack of understanding of cause–effect connections is even cited as a fundamental obstacle for machine learning by some authors (Pearl 2019). Nevertheless, many inference procedures include prediction tasks as an important step (Mullainathan and Spiess 2017). Machine learning is especially suited for this step in high-dimensional settings (Belloni et al. 2014b). The double machine learning framework (Chernozhukov et al. 2017), which we will apply in Sect. 5, allows us to take advantage of the predictive performance of machine learning algorithms when seeking solutions for causal problems.\n\n4 Literature review\n\nWe conducted a search of the literature across Google, Google Scholar and finance journals on the use of machine learning in FP&A. The use of quantitative methods in the broad field of finance has been studied intensively for close to 40 years (Ozbayoglu et al. 2020), in part because of the general availability of data in this field, the existence of many areas of implementation and the substantial economic impact of financial decisions. Our search yielded surprisingly few recent publications on the use of machine learning explicitly in FP&A and related fields. The key thrust of machine learning in finance is directed towards various applications ultimately linked to forecasting and trading financial instruments such as stocks, bonds, currencies and derivatives. Credit scoring and fraud detection are other major areas. Examples of recent surveys include Ozbayoglu et al. (2020) and Henrique et al. (2019).\n\nWe see two possible reasons for the apparent scarcity of publications on machine learning in FP&A. First, time-series forecasting has been thoroughly covered and researched for many years (De Gooijer et al. 2006). A large variety of tools for this purpose have been developed, both from an academic and theoretical perspective, as well as from the perspective of practitioners, including easy to use off-the-shelf software (Küsters et al. 2006). From a practical FP&A perspective, these tools, together with the domain knowledge of the experts working in the FP&A function, allow practitioners to arrive at results that—by and large—serve sufficiently well to meet the objective of developing financial plans. Especially, practitioners may therefore perceive machine learning as a “so-so” technology (Acemoglu and Restrepo 2018), which is not (yet) quite worth their (full) attention. Thus, the intrinsic urge to look for new tools, including machine learning, in FP&A is still less pronounced than it is, for instance, in stock-market forecasting, where even a relatively small improvement in forecasting accuracy can yield significant economic payoff. We believe that this will change with the further deployment of digitalization and the consequent increase in data availability as described above. Besides improving the precision of financial forecasts, automated forecasts driven by machine learning can also lead to a substantial reduction in costs and to increased flexibility given that the traditional process is quite labor- and time-intensive.\n\nSecond, we hypothesize the following reason for the limited number of publications on machine learning in FP&A. The initial development of artificial intelligence and machine learning methods was driven mostly by academia. Because these methods are highly relevant for industrial applications, companies (in particular in the tech field) have shown strong interest in applying and developing them further. Indeed, some of the large tech companies host their own dedicated research teams. However, the limited availability of skilled professionals represents a hurdle to fast diffusion in all corporate functions of a company. Therefore, the application of machine learning for FP&A is still rare in the finance function, even though a host of machine learning publications by the industry has already appeared in other functional areas.\nFootnote\n2 Management consultancies have also discovered the benefit of machine learning for finance and FP&A. However, their publications remain general and directional in nature (see, for instance, Balakrishnan et al. 2020; Roos et al. 2020; Tucker et al. 2017; Chandra et al. 2018).\n\nOne company that has made public its use of machine learning in FP&A in scientific papers is Microsoft Corporation. In the past several years, Microsoft appears to have followed an innovative approach with machine learning in FP&A as witnessed by three publications from its employees. One paper (Gajewar et al. 2016) compares the performance of random forests to that of traditional time-series methods such as autoregressive integrated moving average (ARIMA), error trend and seasonality (ETS, a variant of exponential smoothing) and seasonal-trend decomposition using loess (STL, another variant of smoothing) for forecasting quarterly revenues by major geographic region and at the global level up to 1 year into the future. Based on their exploratory analysis, the random forest model with a restricted number of features outperformed the traditional time-series methods and the forecasts generated by the domain experts in the Microsoft FP&A department.\n\nA second paper (Barker et al. 2018) describes a machine learning-based solution that forecasts revenue on a quarterly basis, including individual forecasts for 30 products in three different business segments. Specifically, the machine learning forecast used an elastic net, a random forest, a K-nearest-neighbor and a support vector machine. The winner model was then selected via back-testing. The forecasts generated in this way proved to be more accurate than the traditional forecasts generated by FP&A in approximately 70% of the cases. The paper cites the ability to incorporate external information (e.g., temperature as a driver for electricity demand) in regression frameworks as an advantage of these over pure (standard) time-series models. While classical time-series are good at capturing trends and seasonality, they often struggle to incorporate external data. In particular, they generally lack a regularization mechanism, leading to low out-of-sample accuracy for new forecasts, especially in high-dimensional settings. Many machine learning methods include by design mechanisms to avoid overfitting (e.g., regularization for ridge, lasso and elastic net).\n\nBarker et al. (2018) also highlight some requirements that arise from the intent to use the results of machine learning forecasts in a practical manner in a corporate setting. Traditionally, FP&A works with a point estimate, coupled with an estimation of the risks and opportunities around this mid-point. Risks and opportunities typically consist of a list of items or events that will materially impact the business results if they do not turn out as assumed in the mid-point forecast (Conine and McDonald 2017). Judgmental probability estimates provided by subject matter experts are often attached to these items, together with a quantification of the expected impact under the different scenarios.\nFootnote\n3 For forecasts generated by traditional statistical or machine learning models, prediction intervals are therefore an important element for FP&A practitioners to quantify the risk in the forecast. However, prediction intervals are not typically part of machine learning models. The solution proposed by Barker et al. (2018) consists of creating intervals from out-of-sample error distributions obtained during back-testing. Other practical requirements in a corporate environment are the need for a mostly automated solution allowing for fast forecast generation as well as the need to ensure high security standards for data storage, processing and access. Financial data such as sales and profits are highly sensitive, and companies are reluctant to release them into public cloud environments. Barker et al. (2018) explain the details of their workflow automation and security controls, which revolve around the Microsoft Azure cloud-computing platform.\n\nThe third publication (Koenecke and Gajewar 2020) evaluated deep neural networks traditionally used in natural language processing (encoder–decoder LSTMs) and computer vision (dilated convolutional neural networks) to forecast company revenues. The approach incorporated transfer and curriculum learning. For the products and time period under study in this publication, deep neural networks improved predictive accuracy compared to the company’s internal baseline, which combined traditional statistical and machine learning methods other than deep neural networks.\n\nIn another example of applied machine learning in the area of FP&A, Daimler Mobility used an undisclosed library of machine learning algorithms to generate a monthly forecast set, spanning the next 18 months and updated monthly (Unger and Rodt 2019). In this respect, the approach followed the concept of a rolling forecast. The forecasted set of values comprised key financial performance indicators that were representative of Daimler Mobility’s car rental, leasing, financing and fleet management business. According to Unger and Rodt (2019), one of the key advantages of this approach compared to the traditional way of forecasting and budgeting is the speed with which updated forecasts are available, allowing faster adoption of corrective action.\n\nThese papers all discuss modern machine learning methods for financial forecasting. In the next section, we will show that these approaches cannot be applied directly to inference problems and how the double machine learning framework overcomes this problem. A first example will illustrate the use of machine learning techniques in FP&A for forecasting. A second example will serve to illustrate the use of double machine learning for planning (inference). Finally, we will explore whether having additional data improves the results for both the forecasting and planning tasks.\n\n5 Simulation example\n\nIn this section, we provide the results of a small simulation study. The design of the simulation reflects the setting, types of data and questions that the FP&A department in a large, multinational company could face. We will start with an example in which FP&A is predominantly interested in the accuracy of sales forecasting. We will then carry this example forward into a question related to planning. In this second example, FP&A is interested in assessing the effectiveness of promotional activities in generating sales; in other words, the question of interest relates to causal inference and the answer to this question can inform decisions about resource planning. Finally, we will investigate how the results change if the FP&A department obtains additional data points for their tasks.\n\n5.1 Forecasting\n\nAssume for our stylized simulation the following setting:\nFootnote\n4 for a given month n, the FP&A department would like to forecast the sales \\(y_{n}\\) of a specific product or service. FP&A has collected monthly data over 5 years (\\(N=60\\)) for sales as well as a set of \\(P=40\\) factors or features that FP&A believes could be predictive of these sales. We represent these factors as \\(x_{p,n}\\) and the corresponding sales with \\(y_{n}\\).\nFootnote\n5 In practice, there can be a wide range of factors depending on the product or service. Examples include weather conditions and various macroeconomic indicators, but also specific customer shipment patterns or the current competitive market situation. Note that the size of the feature set can easily reach 40 plausible predictors once an initial, smaller feature set is increased due to the inclusion of transformed and newly created features. This step, called feature engineering, can include the creation of lagged variables (e.g., when the effect of the economic situation affects sales several months later) or interaction effects (e.g., when a particular weather situation coincides with a peak shipment date, nullifying or exacerbating the effect of the peak shipment date). A further example is the transformation of categorical variables into several binary values via the so-called one-hot encoding (e.g., when classifying the competitive market situation as “highly competitive”, “moderately competitive”, “not competitive” and the like).\n\nIn addition to developing a set of 40 features, FP&A measures the promotional activity carried out by the company for the product under investigation during the reference timeframe. We denote this promotional activity as \\(d_{n}\\). For the purpose of this illustrative simulation, we work with the assumption that the promotional activity can be measured using a single variable. In other words, we do not enter into promotional mix considerations with interaction effects among the different promotional tools. In practice, this single variable could be a summary measure such as the amount of money spent on promotion and advertising; another possibility for a summary measure could be the number of customer calls or minutes of customer interaction. For forecasting and planning activities performed by FP&A at aggregate levels, such as the regional, divisional or group levels, an approach like this, based on a summary measure, is sometimes applied. Extending the analysis to include several marketing variables is possible without any major changes.\n\nGiven the nature and intent of promotional activity, it appears natural for FP&A to include \\(d_{n}\\) in the list of likely predictors for the sales forecasting model. Furthermore, estimating the effect of the promotional activity on sales represents an important question for FP&A, which we will address in the second part of this section, dedicated to planning.\n\nTo evaluate the accuracy of the sales forecasts, we will follow an out-of-sample evaluation approach. Only the first four years (48 data points) are used to build and train the forecasting models. FP&A then compares the forecasts generated by the models to the actual values from the last year in the available dataset (12 data points). Note that these 12 data points have been intentionally excluded from the model creation phase. While more sophisticated training and evaluation strategies exist (e.g., rolling evaluation windows), the described approach is sufficient for the purpose of this simulation study, because the out-of-sample forecasting performance is evaluated separately for each simulation.\n\nFor our simulation, we generate the data as \\(n=1, \\ldots , N\\) independent and identically distributed (i.i.d.) draws from the following model:\n\nand\n\nwith \\(x\\sim \\mathcal {N}(0,\\Sigma )\\) where \\(\\Sigma\\) is a \\(p \\times p\\) matrix with \\(\\Sigma _{k,j}=c^{|j-k |}\\), \\(\\varepsilon \\sim \\mathcal {N}(0,2)\\) and \\(\\nu \\sim \\mathcal {N}(0,2)\\).\n\nThe second equation captures confounding, i.e., variables that are simultaneously correlated with the outcome variable and the variable of interest. By setting \\(\\alpha = 0\\), we assume that the promotional activity undertaken by the company has no effect on sales, i.e., that the promotion efforts are, in reality, a waste of resources. With \\(c = 0.3\\), we include some moderate correlation\nFootnote\n6 between features, which can be expected if several features from the same general background (e.g., macroeconomic factors) are included in the model.\n\nWe set \\(\\beta = 0\\), except for \\(\\beta _{39}\\) and \\(\\beta _{40}\\), both of which we set equal to 1. Thus, out of the 40 features included in the analysis, only two are actually related to sales. Similarly, we set \\(\\gamma = 0\\), except for \\(\\gamma _{39}\\) and \\(\\gamma _{40}\\), both of which we also set equal to 1. The two features related to sales also determine the amount of promotional activity \\(d_{n}\\).\nFootnote\n7\\(\\varepsilon\\) and \\(\\nu\\) are random error terms (so-called noise). We report results based on 1000 simulation replications.\n\nIt is important to remind ourselves that the FP&A department naturally does not know any details about this data generation process. Only an oracle would know that, in reality, solely 2 of the 40 plausible predictors are linked to sales and that the coefficient in the data-generating process is 0 for the other 38 features. This situation characterizes sparse models. In such models, only a small number of many potential predictors and/or control variables are actually relevant (Belloni et al. 2014a). Identifying them leads to a correct model specification and is the main challenge.\nFootnote\n8 Additionally, FP&A does not know that the promotional activity \\(d_{n}\\) is correlated with the two features that have non-zero coefficients with respect to sales and that the promotional activity has no influence on sales (\\(\\alpha\\), the parameter of interest, is zero). We will come back to this point when we discuss inference.\n\nWe now provide results for two forecasting approaches. Both have in common that they rely—in this case correctly—on the typical assumption of a linear relationship between the output variable Y (sales) and the full set of regressors X, which includes 40 presumably predictive features and one variable reflecting promotional activity\n\nThe first approach is a traditional linear regression based on the ordinary least squares (OLS) method. Formally, OLS optimizes the parameters in such a way as to minimize the mean squared error (MSE)\n\nwhere \\(x'_i\\hat{\\beta }\\) corresponds to the predicted sales value.\n\nThe second approach, post-lasso, is a classic machine learning technique. To estimate the coefficients, lasso uses a regularization strategy that is suited to high-dimensional problems in which the number of predictors exceeds or approaches the number of observations, as is the case in our simulation. In the first step, the lasso regression is performed. In the second (i.e., post-lasso) step, the method fits OLS on the coefficients selected in the first step. Formally, lasso optimizes the parameters to minimize MSE subject to a penalty for using parameters\n\nwhere \\(x'_i\\hat{\\beta }\\) corresponds again to the predicted sales value.\n\nThe key difference between the lasso and OLS is that lasso minimizes a penalized MSE, in which the penalty amount corresponds to the absolute amount of each parameter included in the model, scaled by the tuning- or hyperparameter \\(\\lambda\\)\n\nA detailed discussion of the theory behind regularization approaches would go beyond the scope of this article. Readers are referred, among many possible sources, to Hastie et al. (2009), Bühlmann and van de Geer (2011) and Taddy (2019). Taddy (2019) sees regularization as “the key to modern statistics” by virtue of its ability to prevent overfitting in high-dimensional settings. Instead, we will recall a few characteristics of the lasso that are particularly relevant to our FP&A example and the corresponding simulation.\n\nThe full name of the lasso (“least absolute shrinkage and selection operator”) indicates two important characteristics. First, as we can see in the formula for \\(\\mathrm{Penalty}_\\mathrm{Lasso}\\), the absolute size of the coefficients included in the model represents a cost in the minimization of the MSE. Lasso will therefore shrink the coefficients towards zero. This makes the prediction system more stable and avoids overfitting. Second, the lasso-specific penalty in the form of the absolute value of the coefficients has the property that some parameters will be exactly equal to zero. In other words, the lasso will fully exclude some variables from the model and therefore perform automatic variable selection.\n\nAs indicated above, the lasso can handle situations in which the number of predictors approaches or even exceeds the number of observations. In our case, the number of predictors (including the measure of promotional activity) is 41 and the number of observations is 48. Although OLS can still be calculated, we will see that its out-of-sample predictive accuracy becomes extremely unreliable. If we were to chose a simulation scenario with 48 or more predictors, OLS could no longer be computed. A second challenge for OLS in settings with many predictors is the increased risk of correlation among the predictors. If predictors are highly correlated among themselves, or if, in an extreme case, there is an exact linear relationship between two predictors (multicollinearity), OLS estimates become unstable. For instance, macroeconomic variables tend to be strongly correlated.\n\nAn important ingredient in the lasso is the size of the penalty, which depends on the tuning parameter \\(\\lambda\\). \\(\\lambda\\) is not determined by the lasso itself, but needs to be selected. Intuitively, \\(\\lambda\\) plays a role in filtering the relevant variables. Several strategies to select \\(\\lambda\\) have been proposed in the literature and are used by practitioners. The most common are cross-validation strategies and information criteria such as Akaike’s or Bayes’ information criterion. Our simulation study uses the data-dependent penalty level proposed by Belloni and Chernozhukov (2013). We refer interested readers to this source for details.\n\nCompared to the standard lasso approach, which induces bias due to the shrinkage of coefficients, post-lasso has the advantage of a smaller bias, even if the model selected in the first step by lasso fails to include some of the true predictors. It also converges at a faster rate towards the true parameter values if the model selected by lasso correctly includes all true predictors (in addition to some irrelevant predictors). If lasso selects exactly (only) the true predictors, the post-lasso coefficient estimators are equal to the ones produced by an oracle that is aware of the underlying data-generating process (Belloni et al. 2012, 2014a).\n\nTable 1 summarizes the results of 1000 simulation runs for the forecasting task, comparing OLS to post-lasso.\n\nWe report the forecast accuracy in terms of average root-mean-squared error (RMSE) over all simulation runs both on the in-sample and the out-of-sample data set. As outlined above, the in-sample data set consists of 48 data points, which are used to build and train the models. The out-of-sample data set consists of 12 data points, which are intentionally not used in the model construction (“hold-out sample”), allowing the model to be evaluated on new, previously unseen data. The strong focus on forecasting performance on previously unseen data is a hallmark of the machine learning approach.\n\nOn the in-sample data, OLS produces a higher predictive accuracy than post-lasso, with an RMSE of 0.738 which is nearly one-third that of the post-lasso RMSE of 1.991. However, the real interest of the FP&A department here is not to model past sales data. Rather, the predictive performance on new data is what matters to FP&A; this is why, the out-of-sample data have been set aside. Here, the OLS RMSE increases substantially to 5.321, more than twice as high as the post-lasso RMSE of 2.162.\n\nWe can draw two main conclusions from the simulation. First, the RMSE of standard OLS increases significantly between in-sample and out-of-sample data. With nearly as many features (regressors) as observations in the model, the resulting overfitting is immediately exposed when OLS is evaluated using previously unseen data. Second, the post-lasso RMSE is relatively stable between the in-sample and out-of-sample data. The in-sample performance is thus already indicative of the true predictive power when post-lasso is used on unseen data. Lasso achieves this through the regularization strategy described above, which leads to a very selective inclusion of features and thus parsimonious models. For reference, of the 40 available features in the simulation, post-lasso retains an average of only 1.2 as relevant and shrinks the coefficients of all the others to exactly zero. As a reminder, our simulation includes only two truly relevant features. The out-of-sample RMSE for post-lasso is thus slightly hihger than the perfect RMSE score of 2.0 (equal to the standard error that was selected for the noise parameter \\(\\varepsilon\\)), which would be achieved by an oracle.\n\nFigure 1 shows the distribution of the out-of-sample RMSE for the post-lasso forecast over the 1000 simulation runs. The distribution of the errors follows approximately a normal distribution (overlaid as a red line). From a practical perspective, the risk of generating a highly incorrect lasso forecast is therefore limited. Furthermore, the right tail of the lasso errors ends before the mean of the OLS error. This provides additional reassurance when relying on lasso.\n\nDistribution of the out-of-sample RMSE for the post-lasso forecast (bars), compared to the normal distribution (red line) (colour figure online)\n\n5.2 Planning\n\nWe will now discuss the use of machine learning in financial planning. To come back to our example, the task for the FP&A department consists of evaluating the effectiveness of promotional activity in generating sales; in statistical parlance, the task relates to statistical inference of the effect of a treatment or intervention (i.e., the promotional activity) on an outcome (i.e., sales). This estimate forms the basis for planning and optimizing marketing activities. In our simulation examples, evaluating the effectiveness of promotion equates to estimating the parameter \\(\\alpha\\). As the parameter of interest, \\(\\alpha\\) corresponds to the effect of the promotional activity on sales, also called the “lift” in business applications. Let us remind ourselves that in our simulation, only two features are relevant for the sales forecast and that these two features also determine the amount of promotional activity. Thus, we are dealing with confounders, because these two features are correlated with both the treatment and the outcome. Moreover, we have set \\(\\alpha\\) to zero, which effectively means that the promotional activity does not have an impact on sales.\n\nIn a business environment, this setting could correspond to an ice cream vendor at the beach who spends money on promotion whenever the weather is warm and sunny on the weekends. He ascribes the increased ice cream sales, or at least a part of them, to his promotional efforts, whereas in reality, it is the favorable weather on the weekend that makes people come to the beach and enjoy his ice cream. Similar to the forecasting exercise, the FP&A department is obviously not aware of the data-generating process governing the simulation and needs to find a way to estimate \\(\\alpha\\).\n\nOne approach to estimating the effect of promotion could be to use the parameter estimate for \\(\\alpha\\) from the lasso model employed in sales forecasting. However, lasso shrinks parameter estimates because of the penalty loading used in the regularization process and therefore does not generate unbiased estimates of the parameter values, even though it allocates the least possible penalty amount to large signals while retaining the stable behavior of a convex penalty (Taddy 2019). Additionally, lasso estimates predictors sparingly insofar as it sets many parameter estimates to exactly zero. In many cases, the factor measuring promotional activity “may not make it” into the second step of the post-lasso procedure. It is therefore not meaningful to infer from the forecasting model the effectiveness of the promotional activities. We have previously highlighted the warning by Mullainathan and Spiess (2017) and Athey (2018) that using a tool built for forecasting and assuming that its parameters possess the properties required for inference can be misleading.\n\nWith the above in mind, one could decide to pursue a hybrid solution with the following approach. Because the lasso has identified the most relevant features for prediction, we carry these forward into the inference model. Additionally, we include in the model the variable of interest (the intervention), which in our example is the variable that represents promotional activity. In a sense, we force this variable of interest into the model. We then estimate the parameter values for all of these features using OLS, which allows us to perform inference on the parameter estimates. In particular, we are able to interpret our parameter of interest \\(\\alpha\\) in this model. In our example, \\(\\alpha\\) will tell us the effectiveness of the promotional activities. Intuitively, a model that is constructed in this way can be understood as attempting to estimate the effect of promotional activity, while controlling for other factors with proven high predictive power from the forecasting model. For the ice cream vendor at the beach, this corresponds to controlling for the effect of the favorable weather during the weekend and thus deriving an isolated estimate of the effect of promotional activity on sales. This approach can be represented as\n\nwith \\(p*\\) corresponding to the subset of all p features for which \\(\\hat{\\beta }_\\mathrm{Lasso}\\) is non-zero.\n\nWe will see from the simulation results that this approach, which we will call “naive”, grossly fails to discover the true value of the parameter of interest, when modern machine learning methods are used in high-dimensional settings; still, it is widely used by practitioners and applied researchers. In our model, the promotional activity measure is correlated with the features that concomitantly and directly influence sales. In the presence of such confounders, the naive approach will fail.\n\nIn short, the naive approach will suffer from omitted variable bias. This is because machine learning methods capture the features correlated with the outcome variable and deliver good predictive performance but often miss variables that are correlated weakly with the outcome but correlated more strongly with the intervention variable. Missing these variables does not harm predictive performance but biases the estimation of the intervention effect, leading to invalid post-selection inference.\n\nFor an approach to be valid, it must overcome this problem of imperfect model selection and related omitted variable bias. Double or debiased machine learning, as proposed by Chernozhukov et al. (2017), is one way to do so. The fundamental idea\nFootnote\n9 is to reduce, for the estimation of the parameter of interest (i.e., the intervention variable), the sensitivity with respect to errors in selecting and estimating the nuisance parameters (i.e., the other predictors in the model). Technically, this can be achieved by regressing residuals on residuals. The first set of residuals is generated by regressing the outcome variable on the control features, notably using regularizing machine learning methods such as (post-)lasso, random forests, boosted trees or other methods suited for high-dimensional settings. The second set of residuals is generated by regressing the treatment variable on the control features, again using modern machine learning methods. This auxiliary step helps to control for the confounders that might lead to omitted variable bias. Finally, the first set of residuals is regressed on the second set of residuals. The parameter value obtained in this residuals-on-residuals regression represents the effect of the treatment variable on the outcome. This procedure is known as Frisch–Waugh–Lovell partialling out. In our simulation study, machine learning methods are used for partialling out. This approach allows for valid inference compared to the naive approach.\n\nTranslated into our stylized simulation, the first regression relates sales to the 40 presumably predictive features; the differences between the predictions \\(\\hat{y}_{n}\\) from this first regression and the actual outcomes (sales) y constitute the first set of residuals \\(r^{1}_{n}\\)\n\nThe second regression relates the promotional activity score d to the 40 presumably predictive features; the differences between the predictions from this second regression \\(\\hat{d}_{n}\\) and the actual outcomes (promotional activity) \\({d}_{n}\\) constitute the second set of residuals \\(r^{2}_{n}\\)\n\nConcretely, we will use a post-lasso approach in the regressions to derive both sets of residuals, but in principle, any other machine learning method could be used, such as random forests or support vector machines. Finally, we regress the residuals from the first regression onto the residuals from the second regression to obtain an estimate for the parameter of interest, \\(\\alpha\\), which represents the impact of promotional activities on sales\n\nThis approach works well in practice, because the residuals-on-residuals approach makes the estimation of the treatment effect less sensitive to errors in the model specification. Athey (2018) provides an intuitive explanation: “[...] in high dimensions, mistakes in estimating nuisance parameters are likely, but working with residualized variables makes the estimation of the average treatment effect orthogonal to errors in estimating nuisance parameters.” This is why the family of approaches that use this principle is also referred to as orthogonal machine learning (Taddy 2019). Interested readers are referred to the literature for an in-depth theoretical discussion, including underlying assumptions and formal proofs, which is beyond the scope of this paper. Key sources include Belloni et al. (2014a) and Chernozhukov et al. (2015, 2018). To implement our simulation, we use the partialling-out approach as defined by Chernozhukov et al. (2016) and report the corresponding results under this label.\n\nTable 2 summarizes the results of the two approaches (i.e., “naive” and “partialling out”) from 1000 simulation runs. We report the mean estimate for \\(\\alpha\\), the standard deviation of the estimate and the corresponding t-statistic and p value for a two-sided test of whether the mean is different from zero. The rejection rate represents the proportion of individual simulation runs in which the ingoing assumption of \\(\\alpha\\)=0 has been rejected based on the t test (at the customary 5% significance level). In other words, these are the instances in which the model incorrectly suggests an effect (positive or negative) of promotional activity on sales.\n\nThe simulation results provide several insights. First, and this is the main point we seek to make, the naive approach grossly fails to discover the true value of \\(\\alpha\\), because it suffers from significant bias. Put simply in the context of our simulation, this bias represents systematic over-estimation of \\(\\alpha\\) and thus over-estimation of the effectiveness of promotion. On average, the naive approach estimates a value for \\(\\alpha\\) of 0.1604, compared to a true value of zero. The partialling-out approach also yields an average positive value for \\(\\alpha\\) of 0.0081, but is much closer to the true value of zero. Relatively speaking, the bias of the naive approach is roughly 20 times higher than that of the partialling-out approach.\n\nA second point is that the standard deviation of the estimates for \\(\\alpha\\) are similar for both approaches. Figures 2 (naive approach) and 3 (partialling-out approach) show the distribution of the estimates for \\(\\alpha\\) from the 1000 simulation runs compared to a normal distribution curve. Visual inspection suggests that the shapes of both distributions are well approximated by a normal distribution. Of course, the center of the distribution for the naive approach is clearly shifted to the right of zero. This reinforces the point made above that bias is induced by the naive approach.\n\nDistribution of estimator for \\(\\alpha\\) from the naive approach\n\nDistribution of estimator for \\(\\alpha\\) from the partialling-out approach\n\nTable 2 also reports the t-statistic and corresponding p value for a two-sided test of whether the mean estimate of \\(\\alpha\\) is zero. Under the naive approach, this hypothesis would be rejected with high confidence (t-statistic of 30), reinforcing the incorrect belief that the promotional efforts positively affect sales. Under the partialling-out approach, the hypothesis of no effect from promotional efforts would not be rejected at the customary 5% threshold level (t-statistic of 1.93). In practice, the FP&A department would of course not benefit from this kind of insight as they would not have access to repeated estimates for \\(\\alpha\\). With the advantage of being able to run multiple simulations, we can use this information to support the point of significant bias in the naive approach. Nevertheless, the rejection rate, reported in the last line of Table 2, provides a good indication of how often FP&A would make an incorrect decision. For each individual run in the simulation, this metric records whether FP&A would (incorrectly) reject the assumption that \\(\\alpha\\) is zero at the typical 5% significance level. Under the naive approach, this would happen 46% of the time. Put differently, a bit less than half of the time, FP&A would incorrectly assume that promotional activity does have an effect on sales. With partialling-out, this error drops to slightly below 5%.\nFootnote\n10\n\nIn summary, by relying on the naive approach, the FP&A department (or the ice cream vendor) would substantially overestimate the causal effect of the promotional activity on sales. Consequently, this activity would probably be maintained or even increased for this product or service, even though in reality, it does not increase sales. Put differently, the company would draw up plans that allocate resources wastefully on this particular product or market. The impact from falling into this trap could multiply even further across the organization if the results of such an analysis were used as a benchmark for similar products, services or geographic markets. This might happen, for example, if data are not readily available for a particular product (for example, one that is being newly launched) and the decision is made to extrapolate from existing (and potentially wrong) information. Such a situation is even more likely when the existing information appears plausible and suitable\nFootnote\n11 and, in addition, is perceived as objective, unbiased (in the sense of free from human/cognitive bias) or even scientific, because it was generated using data-driven methods.\n\n5.3 The value of data\n\nIn 2017, “The Economist” (Economist, 2017) asserted in the title of its May 6 edition that data are now the world’s most valuable resource. Questions about the value of data as a resource and production factor have generated great interest in academia and policy institutes. One consideration within this vast topic is a (hypothesized) positive feedback loop: more data lead to more data-driven insights, allowing a company to serve its customers better, to attract more customers and, in turn, to collect even more data. Nevertheless, there seems to be a broad consensus that data are generally governed by decreasing returns to scale, like any other production factor (Varian 2018; Bajari et al. 2019).\n\nIn this paper, we will limit ourselves to a short discussion of how the number of observations affects the accuracy achieved by the forecasting and inference methods used by the FP&A department within the frame of our simulation. For empirical results, we refer interested readers to Bajari et al. (2019), which contains a study of the performance of Amazon’s retail forecasting system. The study finds performance gains in the time dimension (i.e., from longer data history), but not in the product dimension (i.e., panel data forecasts do not improve with more products within a category). An interesting finding is the overall improvement of forecasts over time (controlling for the length of data history and the number of products), suggesting positive effects from improved technology (e.g., new machine learning models, better hardware or adaptation of organizational practices).\n\nIn our simulation, the hypothetical FP&A department uses a training set of 48 observations, 40 predictive features and one variable of interest for inference (i.e., the measure of promotional activity). In many real-life applications relevant to FP&A departments, the number of observations available for analysis is typically limited. More observations may simply not exist; for instance, new products generate sales data starting only from their launch date. Even if data do exist, collecting, accessing and, if necessary, curating them come at a cost; for instance, companies may limit the amount of directly accessible data history due to system constraints, or data generated prior to the introduction of new software may be inaccessible, in full or in part.\n\nLet us now explore simulation results assuming that the FP&A department has invested in expanding the training set of observations to 60, 72 or 96. The number of features (i.e., 40), the variable of interest (promotional activity measure) and the overall simulation set-up remain unchanged.\nFootnote\n12 We again run 1000 simulations. What is the return on accuracy of expanding the observation set?\nFootnote\n13\n\nTable 3 reports the forecasting results based on 60, 72 and 96 observations compared to the previous simulation based on 48 observations. For OLS, the in-sample accuracy drops, as witnessed by the increase in RMSE to 1.507 (for 96 observations) from the initial RMSE of 0.738 with 48 observations. However, the out-of-sample accuracy increases: the corresponding RMSE drops to 2.561 (for 96 observations) from the previous RMSE of 5.321 based on 48 observations. In fact, the additional observations reduce the extent of overfitting seen in the initial setting. With 40 features and (only) 48 observations, OLS was actually close to the point of failing. This point would have been reached if the number of features had been equal to or exceeded the number of observations. Intuitively, OLS moves further away from this point by expanding the set of observations (and keeping the number of features constant).\n\nFor post-lasso, the results based on 60, 72 and 96 observations are quite similar to those obtained with 48 observations. Neither the in-sample nor the out-of-sample RMSE change notably. As expected and unlike OLS, post-lasso already deals well with the initial situation in which the number of features is close to the number of observations and benefits only marginally from the increase in observations. Put differently, post-lasso does not require investing in the generation or acquisition of additional data. Our finding is consistent with standard stochastic theory.\nFootnote\n14\n\nIn summary, while having more data is generally beneficial, expanding the observation set for forecasting in our simulation study creates a tangible advantage only for OLS. If the FP&A department employs post-lasso, which is the preferable method in this setting, the gain in precision from expanding the observation set is very small and, for many practical applications, would not warrant the effort.\n\nWe will now look at inference, which entails estimating the (causal) effect of promotional activities on sales. Table 4 reports the inference results for \\(\\alpha\\) based on 60, 72 and 96 training observations compared to the previous simulation based on 48 observations. Recall that the true value of \\(\\alpha\\) is zero. For OLS, as the number of observations increases, the mean estimate for \\(\\alpha\\) decreases to 0.0617 (for 96 observations) from the previous estimate of 0.1604 with 48 observations. However, based on a standard t test, this value is still significantly different from zero (t-statistic of 13.257). In comparison, for the partialling-out approach, the mean estimate for \\(\\alpha\\) declines from 0.0081 with 48 observations to 0.0042 for 96 observations, with a minimum of \\(-\\) 0.0008 in the simulation run based on 72 observations. In all three additional scenarios, it is not statistically different from zero (t-statistic of 1.381, \\(-\\) 0.217 and 1.400, respectively).\n\nThe expanded set of observations reduces the bias of the naive approach. Intuitively, the risk of imperfect model selection described above becomes smaller. Still, the naive approach exhibits significant bias compared to the true value of \\(\\alpha\\). For the partialling-out approach, the additional observations lead to a mean estimate for \\(\\alpha\\) that comes even closer to the true value. Depending on the required precision of the estimate, the FP&A department could benefit from the additional set of observations in its analysis. Again, our finding is consistent with standard theory on convergence rates (see, for instance, Bühlmann and van de Geer 2011 or Belloni and Chernozhukov 2013). Whereas post-lasso converges for forecasting towards the true parameter value at a relatively slower rate of \\(n^{-1/4}\\), the double machine learning estimator of the treatment effect converges at the faster rate of \\(n^{-1/2}\\) (i.e., the same rate as OLS).\n\n6 Conclusion\n\nDigitalization, especially when it couples large amounts of data with appropriate tools for analysis, represents an important opportunity for the financial planning and analysis function. In this article, we have provided an introductory overview of machine learning in this context. By reviewing several relevant theoretical aspects of machine learning and discussing the results of a simulation study, we have demonstrated how machine learning may prove useful for FP&A practitioners. We have paid special attention to explain the distinction between forecasting and planning tasks, the first of which involves prediction and the latter of which involves causal inference. We see the confusion of these two concepts as a major pitfall that practitioners should strive to avoid. Specific approaches to causal machine learning have begun to gain traction, as awareness has increased that the naive application of machine learning can fail in applications that go beyond prediction. This applies to all modern machine learning methods in a high-dimensional setting.\n\nOur article has several limitations. It was impossible to cover the vast number of machine learning techniques that exist. Depending on the causal question at hand, a range of econometric approaches (e.g., instrumental variables, synthetic controls or regression discontinuity designs) coupled with machine learning methods may be suitable. We intentionally used a simple data generation process in our simulation; additional elements such as trends or seasonal components or a real-life example could complement our simulation study. Despite these limitations, we believe that our article can be a valuable source of insights into the ways in which FP&A can benefit from machine learning. With it, we hope to contribute to the adoption of machine learning in this area and help practitioners avoid common mistakes.\n\nNotes\n\nIn some companies, the (short-term) plans formally expressed in budgets are prepared by controllers (management accountants) within the accounting department (Garrison et al. 2006), while the strategy department formulates the directional (long-term) plans.\n\nFor instance, www.bosch.com/de/forschung/know-how/publikationen (accessed Feb 23, 2021) contains a collection by Robert Bosch GmbH.\n\nOther methods with a very similar intent exist. Examples are the quantification of a best and worst case in addition to the normal or base case, or sensitivity analysis with varying degrees of sophistication.\n\nWe have intentionally kept the simulation example simple. For instance, we have not added any time-series-specific effects such as a trend component or serially correlated error terms. This allows us to focus on the key elements. For instance, the \\(N=60\\) data points could represent observations in different countries or sub-markets, which would warrant a cross-sectional approach to the analysis. The conclusions presented in the simulation example will remain largely unchanged.\n\nThis is a case of supervised learning, because we have observations for both the input (\\(x_{p,n}\\)) and the output (\\(y_{n}\\)).\n\nThe value of c represents the correlation between immediate neighbor features (e.g., feature \\(x_{p}\\) and feature \\(x_{p+1}\\)). Due to the way \\(\\Sigma\\) is constructed, the correlation decays quickly as the distance between features increases (e.g., feature \\(x_{p}\\) and \\(x_{p+3}\\) have only a correlation of \\(c^{3}\\), which is 0.027 for \\(c=0.3\\)).\n\nA simple example can help clarify the intuition behind this setting. Ice cream sales on the beach are probably positively related to weather conditions (feature 1) and the day of the week (feature 2). At the same time, the ice cream salesperson may decide to run some promotional activity when weather conditions are favorable on a weekend. Thus, the same features have an influence both on sales and on promotional activity. We will revert to this illustrative example in the section on planning.\n\nBy construction, our simulation example is exactly sparse, with parameter values for all non-relevant features exactly equal to zero. For practical applications, a more realistic assumption is approximate sparsity, meaning that all or many features can have non-zero parameter values. Nevertheless, only a limited number of features are needed to approximate the true relationship with sufficient accuracy. We refer interested readers to Belloni et al. (2010). Our simulation could easily be extended to such a setting. Results would remain largely unchanged.\n\nDouble machine learning also uses cross-fitting, an efficient way of data splitting. Interested readers are referred to Chernozhukov et al. (2017).\n\nNote that one would expect an error rate here of around 5% from a correct model, because the 5% significance level corresponds to a 5% probability of rejecting the null hypothesis when it is, in reality, true.\n\nBlake et al. (2015) highlight in their paper that “[...] the incentives faced by advertising firms, publishers, analytics consulting firms, and even marketing executives within companies, are all aligned with increasing advertising budgets.”\n\nWe intentionally do not allow the number of features to grow with the sample size (see for instance Belloni et al. 2010) to isolate the effect of the additional observations clearly. In practice, a significant extension of the number of observations may require including additional control features.\n\nThe natural way to think about the expansion is to assume that the department “digs out” additional historical observations. However, from a theoretical standpoint, the department could also wait 1, 2 or 4 years, respectively, and gather the additional data points over time. In this case, the change in forecasting accuracy could be mistaken for a technology or learning effect by an outside observer.\n\nSee, for instance, Bühlmann and van de Geer (2011) or Belloni and Chernozhukov (2013). Post-lasso converges towards the true parameter value at a rate of \\(n^{-1/4}\\), which is slower than the OLS rate of \\(n^{-1/2}\\). The value of additional data is thus generally smaller for post-lasso than for OLS.\n\nReferences\n\nAcemoglu, D., & Restrepo, P. (2018). Artificial intelligence, automation, and work. The economics of artificial intelligence: an agenda (pp. 197–236). University of Chicago Press. https://doi.org/10.7208/chicago/9780226613475.001.0001.\n\nChapter\n  Google Scholar\n\nAthey, S. (2018). The impact of machine learning on economics. The economics of artificial intelligence: an agenda (pp. 507–547). University of Chicago Press.\n\nGoogle Scholar\n\nBajari, P., Chernozhukov, V., Hortaçsu, A., & Suzuki, J. (2019). The impact of big data on firm performance: an empirical investigation. AEA Papers and Proceedings, 109, 33–37. https://doi.org/10.1257/pandp.20191000.\n\nArticle\n  Google Scholar\n\nBalakrishnan, T., Chui, M., Hall, B., & Henke, N. (2020). Global survey: the state of AI in 2020. McKinsey & Company. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/global-survey-the-state-of-ai-in-2020. Accessed 6 Dec 2020.\n\nBarker, J., Gajewar, A., Golyaev, K., Bansal, G., & Conners, M. (2018). Secure and automated enterprise revenue forecasting. In AAAI, pp. 7657–7664.\n\nBecker, S. D., Mahlendorf, M. D., Schäffer, U., & Thaten, M. (2016). Budgeting in times of economic crisis. Contemporary Accounting Research, 33, 1489–1517. https://doi.org/10.1111/1911-3846.12222.\n\nArticle\n  Google Scholar\n\nBelloni, A., Chen, D., Chernozhukov, V., & Hansen, C. (2012). Sparse models and methods for optimal instruments with an application to eminent domain. Econometrica, 80, 2369–2429. https://doi.org/10.3982/ECTA9626.\n\nArticle\n  Google Scholar\n\nBelloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. Bernoulli, 19, 521–547. https://doi.org/10.3150/11-BEJ410.\n\nArticle\n  Google Scholar\n\nBelloni, A., Chernozhukov, V., & Hansen, C. (2010). Inference for high-dimensional sparse econometric models. In Advances in Economics and Econometrics. 10th World Congress of Econometric Society, Aug 2010 III, pp. 245–295. ArXiv, 2011.\n\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014a). High-dimensional methods and inference on structural and treatment effects. Journal of Economic Perspectives, 28, 29–50. https://doi.org/10.1257/jep.28.2.29.\n\nArticle\n  Google Scholar\n\nBelloni, A., Chernozukov, V., & Hansen, C. (2014b). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81, 608–650.\n\nArticle\n  Google Scholar\n\nBishop, C. M. (2006). Pattern recognition and machine learning. Information science and statistics. Springer (Softcover published in 2016).\n\nGoogle Scholar\n\nBlake, T., Nosko, C., & Tadelis, S. (2015). Consumer heterogeneity and paid search effectiveness: a large-scale field experiment. Econometrica, 83, 155–174. https://doi.org/10.3982/ECTA12423.\n\nArticle\n  Google Scholar\n\nBrealey, R. A., Myers, S. C., & Franklin, A. (2020). Principles of corporate finance (13th ed.). McGraw-Hill Education.\n\nGoogle Scholar\n\nBühlmann, P., & van de Geer, S. (2011). Statistics for high-dimensional data: methods, theory and applications. Springer series in statistics. Springer.\n\nBook\n  Google Scholar\n\nChandra, K., Plaschke, F., & Seth, I. (2018). Memo to the CFO: get in front of digital finance - or get left back. McKinsey & Company. https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/memo-to-the-cfo-get-in-front-of-digital-finance-or-get-left-back. Accessed 10 Dec 2020.\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., & Newey, W. (2017). Double/debiased/neyman machine learning of treatment effects. American Economic Review, 107, 261–65. https://doi.org/10.1257/aer.p20171038.\n\nArticle\n  Google Scholar\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21, C1–C68. https://doi.org/10.1111/ectj.12097.\n\nArticle\n  Google Scholar\n\nChernozhukov, V., Hansen, C., & Spindler, M. (2015). Valid post-selection and post-regularization inference: an elementary, general approach. Annual Review of Economics, 7, 649–688. https://doi.org/10.1146/annurev-economics-012315-015826.\n\nArticle\n  Google Scholar\n\nChernozhukov, V., Hansen, C., & Spindler, M. (2016). High-dimensional metrics in R. arXiv:1603.01700v2.\n\nConine, T. C., & McDonald, M. (2017). The application of variance analysis in FP&A organizations: survey evidence and recommendations for enhancement. Journal of Accounting and Finance, 17, 54–70.\n\nGoogle Scholar\n\nDe Gooijer, J. G., & Hyndman, R. J. (2006). 25 years of time series forecasting. International Journal of Forecasting, 22, 443–473. https://doi.org/10.1016/j.ijforecast.2006.01.001 (twenty five years of forecasting).\n\nArticle\n  Google Scholar\n\nEconomist (2017). The world’s most valuable resource is no longer oil, but data. https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data. Accessed 6 Dec 2020.\n\nFischer, E. O. (2009). Finanzwirtschaft für Anfänger. Lehr- und Handbücher zur entscheidungsorientierten Betriebswirtschaft. Oldenbourg.\n\nGoogle Scholar\n\nGajewar, A., & Bansal, G. (2016). Revenue forecasting for enterprise products. arXiv:1701.06624.\n\nGandomi, A., & Haider, M. (2015). Beyond the hype: big data concepts, methods, and analytics. International Journal of Information Management, 35, 137–144. https://doi.org/10.1016/j.ijinfomgt.2014.10.007.\n\nArticle\n  Google Scholar\n\nGarrison, R. H., Noreen, E. W., & Brewer, P. C. (2006). Managerial accounting. McGraw-Hill/Irwin.\n\nGoogle Scholar\n\nGray, G. L., & Alles, M. (2015). Data fracking strategy: why management accountants need it. Management Accounting Quarterly, 16, 22–33.\n\nGoogle Scholar\n\nHansen, S. C. (2011). A theoretical analysis of the impact of adopting rolling budgets, activity-based budgeting and beyond budgeting. European Accounting Review, 20, 289–319. https://doi.org/10.1080/09638180.2010.496260.\n\nArticle\n  Google Scholar\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction (2nd ed.). Springer-Verlag.\n\nBook\n  Google Scholar\n\nHenrique, B. M., Sobreiro, V. A., & Kimura, H. (2019). Literature review: machine learning techniques applied to financial market prediction. Expert Systems with Applications, 124, 226–251. https://doi.org/10.1016/j.eswa.2019.01.012.\n\nArticle\n  Google Scholar\n\nKoenecke, A., & Gajewar, A. (2020). Curriculum learning in deep neural networks for financial forecasting. In V. Bitetta, I. Bordino, A. Ferretti, F. Gullo, S. Pascolutti, & G. Ponti (Eds.), Mining data for financial applications (pp. 16–31). Springer International Publishing.\n\nChapter\n  Google Scholar\n\nKüsters, U., McCullough, B. D., & Bell, M. (2006). Forecasting software: past, present and future. International Journal of Forecasting, 22, 599–615. https://doi.org/10.1016/j.ijforecast.2006.03.004 (twenty five years of forecasting).\n\nArticle\n  Google Scholar\n\nLaney, D. (2001). 3-D data management: controlling data volume, velocity and variety. Application Delivery Strategies by META Group Inc., Gartner. https://blogs.gartner.com/doug-laney/files/2012/01/ad949-3DData-Management-Controlling-Data-Volume-Velocity-andVariety.pdf. Accessed 30 July 2020.\n\nMöller, K., Schäffer, U., & Verbeeten, F. (2020). Digitalization in management accounting and control: an editorial. Journal of Management Control, 31, 1–8. https://doi.org/10.1007/s00187-020-00300-5.\n\nArticle\n  Google Scholar\n\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31, 87–106. https://doi.org/10.1257/jep.31.2.87.\n\nArticle\n  Google Scholar\n\nOesterreich, T. D., Teuteberg, F., Bensberg, F., & Buscher, G. (2019). The controlling profession in the digital age: understanding the impact of digitisation on the controller’s job roles, skills and competences. International Journal of Accounting Information Systems. https://doi.org/10.1016/j.accinf.2019.100.\n\nArticle\n  Google Scholar\n\nOzbayoglu, A. M., Gudelek, M. U., & Sezer, O. B. (2020). Deep learning for financial applications: a survey. Applied Soft Computing, 93, 106384. https://doi.org/10.1016/j.asoc.2020.106384.\n\nArticle\n  Google Scholar\n\nPearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62, 54–60. https://doi.org/10.1145/3241036.\n\nArticle\n  Google Scholar\n\nPearl, J., & Mackenzie, D. (2018). The book of why: the new science of cause and effect (1st ed.). Basic Books Inc.\n\nGoogle Scholar\n\nRoos, A., Tucker, J., Rodt, M., Stange, S., Ego, P., Boudadi, A., & Sheth, H. (2020). Lessons from best-in-class CFOs. Boston Consulting Group. https://www.bcg.com/publications/2020/lessons-best-in-class-cfos. Accessed 29 July 2020.\n\nRoss, S. A., Westerfield, R. W., & Jordan, B. D. (2019). Fundamentals of corporate finance (12th ed.). McGraw-Hill Education.\n\nGoogle Scholar\n\nRubin, D. B. (2005). Causal inference using potential outcomes. Journal of the American Statistical Association, 100, 322–331. https://doi.org/10.1198/016214504000001880.\n\nArticle\n  Google Scholar\n\nStrauß, E., & Zecher, C. (2013). Management control systems: a review. Journal of Management Control, 23, 233–268. https://doi.org/10.1007/s00187-012-0158-7.\n\nArticle\n  Google Scholar\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: an introduction. Adaptive computation and machine learning series. MIT Press.\n\nGoogle Scholar\n\nTaddy, M. (2019). Business data science: combining machine learning and economics to optimize, automate, and accelerate business decisions. McGraw-Hill Education.\n\nGoogle Scholar\n\nTucker, J., Foldesy, J., Roos, A., & Rodt, M. (2017). How digital CFOs are transforming finance. Boston Consulting Group. https://www.bcg.com/publications/2017/function-excellence-how-digital-cfo-transforming-finance. Accessed 10 Dec 2020.\n\nUnger, G., & Rodt, M. (2019). The art of forward-looking steering: the power of algorithmic forecasting. Boston Consulting Group. https://www.bcg.com/publications/2019/power-of-algorithmic-forecasting. Accessed 30 Nov 2020.\n\nVarian, H. (2018). Artificial intelligence, economics, and industrial organization. The economics of artificial intelligence: an agenda (pp. 399–419). University of Chicago Press. https://doi.org/10.7208/chicago/9780226613475.001.0001.\n\nChapter\n  Google Scholar\n\nZhu, X., & Goldberg, A. B. (2009). Introduction to semi-supervised learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 3, 1–130. https://doi.org/10.2200/S00196ED1V01Y200906AIM006.\n\nArticle\n  Google Scholar\n\nDownload references\n\nAcknowledgements\n\nWe thank the editor and two anonymous referees for their very helpful comments and suggestions.\n\nFunding\n\nOpen Access funding enabled and organized by Projekt DEAL.\n\nAuthor information\n\nAuthors and Affiliations\n\nNovartis International AG, Novartis Campus, 4002, Basel, Switzerland\n\nHelmut Wasserbacher\n\nHamburg Business School, University of Hamburg, Moorweidenstr. 18, 20148, Hamburg, Germany\n\nMartin Spindler\n\nCorresponding author\n\nCorrespondence to Martin Spindler.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nThe views and opinions expressed in this document are those of the first author, and do not necessarily reflect the official policy or position of Novartis or any of its officers.\n\nRights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nWasserbacher, H., Spindler, M. Machine learning for financial forecasting, planning and analysis: recent developments and pitfalls. Digit Finance 4, 63–88 (2022). https://doi.org/10.1007/s42521-021-00046-2\n\nDownload citation\n\nReceived\n27 May 2021\n\nAccepted\n17 November 2021\n\nPublished\n16 December 2021\n\nIssue Date\nMarch 2022\n\nDOI\nhttps://doi.org/10.1007/s42521-021-00046-2\n\nShare this article\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nKeywords\n\nJEL Classification\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nUsing machine learning methods to predict financial performance: Does ... : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nUsing machine learning methods to predict financial performance: Does disclosure tone matter?\n\n1638 Accesses\n\n1 Altmetric\n\nExplore all metrics\n\nAbstract\n\nWe use three supervised machine learning methods, namely linear discriminant analysis, quadratic discriminant analysis, and random forest, to predict corporate financial performance. We use a sample of 63 listed banks from eight emerging markets, covering 10 years from 2008 to 2017, using earning per share as a measure of performance. We use the design science research (DSR) framework to examine whether the textual contents of annual reports in previous years contain value-relevant information to predict future performance; thus, these contents can improve the accuracy and quality of predictive models. We combine two groups of variables in the proposed models. The first group is the sentiment analysis of disclosure tone in annual report narratives using the Loughran and McDonald dictionary (J Finance 66:35–65, 2011), while the second group is the quantitative properties of banks which consist of five variables, namely size, financial leverage, age, market-to-book ratio, and risk. Our analysis suggests that the random forest method provides the best predictive model. We also provide evidence on the accuracy and performance of predictive models that can be increased by incorporating disclosure tone variables as non-financial variables with financial variables. Interestingly, we find that the uncertainty variable is the most important disclosure tone variable. Finally, we find that size is the most important variable related to banks’ quantitative characteristics. Our study suggests that the analysis of tone through corporate narrative disclosures can be used as a complementary or diagnostic approach rather than an alternative in making decisions by different stakeholders such as analysts, investors, and auditors.\n\nThis is a preview of subscription content, log in via an institution to check access.\n\nAccess this article\n\nSubscribe and save\n\nBuy Now\n\nPrice includes VAT (Lebanon)\n\nInstant access to the full article PDF.\n\nInstitutional subscriptions\n\nSimilar content being viewed by others\n\nData Analysis for Predicting Stock Prices Using Financial Indicators Based on Business Reports\n\nBenchmarking Machine Learning Algorithms to Predict Profitability Directional Changes\n\nPerformance of Indebted Companies Using a Machine Learning Approach\n\nReferences\n\nAl-Khatib, H.B., and A. Al-Horani. 2012. Predicting financial distress of public companies listed in Amman stock exchange. European Scientific Journal 8(15): 1–18.\n\nGoogle Scholar\n\nAltman, D.G., and J.M. Bland. 1994a. Diagnostic tests. 1: Sensitivity and specificity. British Medical Journal 308: 1552.\n\nArticle\n  Google Scholar\n\nAltman, D.G., and J.M. Bland. 1994b. Statistics notes: Diagnostic tests 2—Predictive values. British Medical Journal 309: 102.\n\nArticle\n  Google Scholar\n\nAltman, E.I., M. Iwanicz-Drozdowska, E.K. Laitinen, and A. Suvas. 2017. Financial distress prediction in an international context: A review and empirical analysis of Altman’s z-score model. Journal of International Financial Management & Accounting 28: 131–171.\n\nArticle\n  Google Scholar\n\nAltman, E.I., G. Sabato, and N. Wilson. 2010. The value of non-financial information in small and medium-sized enterprise risk management. Journal of Credit Risk 2: 95–127.\n\nArticle\n  Google Scholar\n\nAly, D., S. El-Halaby, and K. Hussainey. 2018. Tone disclosure and financial performance: Evidence from Egypt. Accounting Research Journal 31(1): 63–74.\n\nArticle\n  Google Scholar\n\nAppiah, K.O., and J. Abor. 2009. Predicting corporate failure: Some empirical evidence from the UK. Benchmarking: an International Journal 16: 432–444.\n\nArticle\n  Google Scholar\n\nAppiah, K.O., A. Chizema, and J. Joseph Arthur. 2015. Predicting corporate failure: A systematic literature review of methodological issues. International Journal of Law and Management 57: 461–485.\n\nArticle\n  Google Scholar\n\nArslan-Ayaydin, Ö., K. Boudt, and J. Thewissen. 2016. Managers set the tone: Equity incentives and the tone of earnings press releases. Journal of Banking & Finance 72: S132–S147.\n\nArticle\n  Google Scholar\n\nAssociation for Investment Management and Research (AIMR). 2000. Corporate disclosure survey: A report to AIMR. St. Louis: Fleishman-Hillard Research.\n\nGoogle Scholar\n\nAziz, M.A., and H.A. Dar. 2006. Predicting corporate bankruptcy: Where do we stand? Corporate Governance 6: 18–33.\n\nArticle\n  Google Scholar\n\nBaginski, S.P., E. Demers, A. Kausar, and Y.J. Yu. 2018. Linguistic tone and the small trader. Accounting, Organizations and Society 68–69: 21–37.\n\nArticle\n  Google Scholar\n\nBalakrishnan, R., X.Y. Qiu, and P. Srinivasan. 2010. On the predictive ability of narrative disclosures in annual reports. European Journal of Operational Research 202: 789–801.\n\nArticle\n  Google Scholar\n\nBalcaen, S., and H. Ooghe. 2006. 35 years of studies on business failure: An overview of the classic statistical methodologies and their related problems. The British Accounting Review 38: 63–93.\n\nArticle\n  Google Scholar\n\nBeattie, V.A., and M.J. Jones. 2000. Changing graph use in corporate annual reports: A time-series analysis. Contemporary Accounting Research 17(2): 213–226.\n\nArticle\n  Google Scholar\n\nBreiman, L. 2001a. Random forests. Machine Learning 45: 5–32.\n\nArticle\n  Google Scholar\n\nBreiman, L. 2001b. Statistical modeling: The two cultures. Statistical Science 16: 199–215.\n\nArticle\n  Google Scholar\n\nBreiman, L. 2017. Classification and regression trees. London: Routledge.\n\nBook\n  Google Scholar\n\nCampbell, J.L., H. Chen, D.S. Dhaliwal, H.M. Lu, and L.B. Steele. 2014. The information content of mandatory risk factor disclosures in corporate filings. Review of Accounting Studies 19: 396–455.\n\nArticle\n  Google Scholar\n\nChen, K., T. Chen, and J. Yen. 2009. Predicting future earnings change using numeric and textual information in financial reports. In Proceedings of intelligence and security informatics, Pacific Asia Workshop, PAISI 2009, Bangkok, Thailand, 54–63.\n\nChou, C., C.J. Chang, C. Chin, and W. Chiang. 2018. Measuring the consistency of quantitative and qualitative information in financial reports: A design science approach. Journal of Emerging Technologies in Accounting 15: 93–109.\n\nArticle\n  Google Scholar\n\nClatworthy, M., and M. Jones. 2003. Financial reporting of good news and bad news: Evidence from accounting narratives. Accounting and Business Research 33(3): 171–185.\n\nArticle\n  Google Scholar\n\nClatworthy, M., and M. Jones. 2006. Differential patterns of textual characteristics and company performance in the chairman’s statement. Accounting, Auditing & Accountability Journal 19(4): 493–511.\n\nArticle\n  Google Scholar\n\nConnelly, B.L., S.T. Certo, R.D. Ireland, and C.R. Reutzel. 2011. Signaling theory: A review and assessment. Journal of Management 37(1): 39–67.\n\nArticle\n  Google Scholar\n\nCooper, W., L. Seiford, and K. Tone. 2000. Data envelopment analysis: A comprehensive text with models, applications, references and DEA solver software. Boston/Dordrecht/London: Kluwer Academic Publishers.\n\nBook\n  Google Scholar\n\nDavis, A., J. Piger, and L. Sedor. 2006. Beyond the numbers: An analysis of optimistic and pessimistic language in earnings press releases. Working paper, Washington University at St. Louis, Federal Reserve Bank of St. Louis and the University of Notre Dame.\n\nDavis, A.K., and I. Tama-Sweet. 2012. Managers’ use of language across alternative disclosure outlets: Earnings press releases versus MD&A. Contemporary Accounting Research 29(3): 804–837.\n\nArticle\n  Google Scholar\n\nde Graaff, R. 2017. Sentiment analysis of annual reports as a financial performance indicator. Master of Science in Business Information Systems, Eindhoven University of Technology.\n\nDias, W., and R. Matias-Fonseca. 2010. The language of annual reports as an indicator of the organizations’ financial situation. International Review of Business Research Papers 6: 206–215.\n\nGoogle Scholar\n\nDonner, A., and N. Klar. 1996. The statistical analysis of kappa statistics in multiple samples. Journal of Clinical Epidemiology 49: 1053–1058.\n\nArticle\n  Google Scholar\n\nDuda, R.O., P.E. Hart, and D.G. Stork. 2012. Pattern classification. New York: Wiley.\n\nGoogle Scholar\n\nElshandidy, T., and P.J. Shrives. 2016. Environmental incentives for and usefulness of textual risk reporting: Evidence from Germany. The International Journal of Accounting 51: 464–486.\n\nArticle\n  Google Scholar\n\nFalschlunger, L.M., C. Eisl, H. Losbichler, and A.M. Greil. 2015. Impression management in annual reports of the largest European companies: A longitudinal study on graphical representations. Journal of Applied Accounting Research 16(3): 383–399.\n\nArticle\n  Google Scholar\n\nFeldman, R., S. Govindaraj, J. Livnat, and B. Segal. 2010. Management’s tone change, post earnings announcement drift and accruals. Review of Accounting Studies 15: 915–953.\n\nArticle\n  Google Scholar\n\nGarcia, D. 2013. Sentiment during recessions. Journal of Finance 68: 1267–1300.\n\nArticle\n  Google Scholar\n\nGenuer, R., J.-M. Poggi, and C. Tuleau-Malot. 2010. Variable selection using random forests. Pattern Recognition Letters 31: 2225–2236.\n\nArticle\n  Google Scholar\n\nGregor, S., and A.R. Hevner. 2013. Positioning and presenting design science research for maximum impact. MIS Quarterly 37: 337–355.\n\nArticle\n  Google Scholar\n\nHackfort, D., R. Schinke, and B. Strauss. 2019. Dictionary of sport psychology. London: Academic Press.\n\nGoogle Scholar\n\nHahn, R., and R. Lülfs. 2014. Legitimizing negative aspects in GRI-oriented sustainability reporting: A qualitative analysis of corporate disclosure strategies. Journal of Business Ethics 123(3): 401–420.\n\nArticle\n  Google Scholar\n\nHastie, T., R. Tibshirani, and J. Friedman. 2009. The elements of statistical learning: Data mining, inference, and prediction. Springer Series in Statistics. New York: Springer.\n\nBook\n  Google Scholar\n\nHeinle, M.S., and K.C. Smith. 2017. A theory of risk disclosure. Review of Accounting Studies 22: 1459–1491.\n\nArticle\n  Google Scholar\n\nHenry, E. 2006. Market reaction to verbal components of earnings press releases: Event study using a predictive algorithm. Journal of Emerging Technologies in Accounting 3: 1–19.\n\nArticle\n  Google Scholar\n\nHenry, E. 2008. Are investors influenced by how earnings press releases are written? Journal of Business Communication 45: 363–407.\n\nArticle\n  Google Scholar\n\nHevner, A., and S. Chatterjee. 2010. Design science research in information systems. In Design research in information systems, ed. A. Hevner and S. Chatterjee, 9–22. Boston, MA: Springer.\n\nChapter\n  Google Scholar\n\nHildebrandt, H.W., and R.D. Snyder. 1981. The Pollyanna hypothesis in business writing: Initial results, suggestions for research. Journal of Business Communication 18: 5–15.\n\nArticle\n  Google Scholar\n\nHo, T.K. 1995. Random decision forests. In Proceedings of the 3rd international conference on document analysis and recognition, Montreal, QC, 14–16 August 1995, 278–282.\n\nHo, T.K. 1998. The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence 20(8): 832–844.\n\nArticle\n  Google Scholar\n\nHo, C.T., and D.S. Zhu. 2004. Performance measurement of Taiwan’s commercial banks. International Journal of Productivity and Performance Management 53(5/6): 425–434.\n\nArticle\n  Google Scholar\n\nHope, O., D. Hu, and H. Lu. 2016. The benefits of specific risk-factor disclosures. Rotman School of Management Working Paper No. 2457045; Singapore Management University School of Accountancy Research Paper No. 2015-35.\n\nHorváth, I. 2007. Comparison of three methodological approaches of design research. the 16th International Conference on Engineering Design. Proceedings of ICED 2007. Paris, France, pp 1–11.\n\nHothorn, T., F. Leisch, A. Zeileis, and K. Hornik. 2005. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics 14: 675–699.\n\nArticle\n  Google Scholar\n\nHuang, X., S.H. Teoh, and Y. Zhang. 2014. Tone management. The Accounting Review 89(3): 1083–1113.\n\nArticle\n  Google Scholar\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An introduction to statistical learning. New York: Springer.\n\nBook\n  Google Scholar\n\nJensen, H., and W. Meckling. 1976. Theory of the firm: Managerial behaviour, agency costs and ownership structure. Journal of Financial Economics 16: 305–360.\n\nArticle\n  Google Scholar\n\nKearney, C., and S. Liu. 2014. Textual sentiment in finance: A survey of methods and models. International Review of Financial Analysis 33: 171–185.\n\nArticle\n  Google Scholar\n\nKeusch, T., L.H.H. Bollen, and H.F.D. Hassink. 2012. Self-serving bias in annual report narratives: An empirical analysis of the impact of economic crises. European Accounting Review 21(3): 623–648.\n\nGoogle Scholar\n\nKloptchenko, A., T. Eklund, B. Back, J. Karlsson, H. Vanharanta, and A. Visa. 2002. Combining data and text mining techniques for analysing financial reports. In Proceedings of eighth Americas conference on information systems.\n\nKravet, T., and V. Muslu. 2013. Textual risk disclosures and investors’ risk perceptions. Review of Accounting Studies 18: 1088–1122.\n\nArticle\n  Google Scholar\n\nKuhn, M. 2008. Building predictive models in R using the caret package. Journal of Statistical Software 28: 1–26.\n\nArticle\n  Google Scholar\n\nKursa, M.B., A. Jankowski, and W.R. Rudnicki. 2010. Boruta: A system for feature selection. Fundamenta Informaticae 101: 271–285.\n\nArticle\n  Google Scholar\n\nKursa, M., and W. Rudnicki. 2010. Feature selection with the Boruta package. Journal of Statistical Software 36(11): 1–13.\n\nArticle\n  Google Scholar\n\nLeary, M.R. 2001. Impression management, psychology. In International encyclopedia of the social and behavioral sciences, ed. J. Wright, 7245–7248. Amsterdam: Elsevier.\n\nChapter\n  Google Scholar\n\nLeary, M.R., and R.M. Kowalski. 1990. Impression management: A literature review and two component model. Psychological Bulletin 107(1): 34–47.\n\nArticle\n  Google Scholar\n\nLee, A., J.T. Lin, R. Kao, and K. Chen. 2010. An effective clustering approach to stock market prediction. In PACIS 2010 proceedings, 54.\n\nLehavy, R., F. Li, and K. Merkley. 2011. The effect of annual report readability on analyst following and the properties of their earnings forecasts. The Accounting Review 86: 1087–1115.\n\nArticle\n  Google Scholar\n\nLev, B. 1989. On the usefulness of earnings: Lessons and directions from two decades of empirical research. Journal of Accounting Research 27(Supplement): 153–192.\n\nArticle\n  Google Scholar\n\nLi, F. 2006a. Annual report readability, current earnings and earnings persistence. Working paper, University of Michigan, Ann Arbor.\n\nLi, F. 2006b. Do stock market investors understand the risk sentiments of corporate annual reports? Working paper, University of Michigan, Ann Arbor.\n\nLi, F. 2008. Annual report readability, current earnings, and earnings persistence. Journal of Accounting and Economics 45: 221–247.\n\nArticle\n  Google Scholar\n\nLi, F. 2010. The information content of forward-looking statements in corporate filings: A naïve Bayesian machine learning approach. Journal of Accounting Research 48: 1049–1102.\n\nArticle\n  Google Scholar\n\nLiu, B., and J.J. McConnell. 2013. The role of the media in corporate governance: Do the media influence managers’ capital allocation decisions? Journal of Financial Economics 110: 1–17.\n\nArticle\n  Google Scholar\n\nLoughran, T., and B. McDonald. 2011. When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. Journal of Finance 66: 35–65.\n\nArticle\n  Google Scholar\n\nLu, C., and T. Chen. 2009. A study of applying data mining approach to the information disclosure for Taiwan’s stock market investors. Expert Systems with Applications 36: 3536–3542.\n\nArticle\n  Google Scholar\n\nLukason, O., and E.K. Laitinen. 2019. Firm failure processes and components of failure risk: An analysis of European bankrupt firms. Journal of Business Research 98: 380–390.\n\nArticle\n  Google Scholar\n\nMagnusson, C., A. Arppe, T. Eklund, B. Barbro, H. Vanharanta, and A. Visa. 2005. The language of quarterly reports as an indicator of change in the company’s financial status. Information & Management 42: 561–574.\n\nGoogle Scholar\n\nMcLachlan, G. 2004. Discriminant analysis and statistical pattern recognition, vol. 544. New York: Wiley.\n\nGoogle Scholar\n\nMerkl-Davies, D.M., and N.M. Brennan. 2007. Discretionary disclosure strategies in corporate narratives: Incremental information or impression management? Journal of Accounting Literature 27: 116–196.\n\nGoogle Scholar\n\nMerkl-Davies, D.M., and N.M. Brennan. 2011. A conceptual framework of impression management: New insights from psychology, sociology and critical perspectives. Accounting and Business Research 41(5): 415–437.\n\nArticle\n  Google Scholar\n\nOnder, E., and A.T. Altintas. 2017. Financial performance evaluation of Turkish construction companies in Istanbul Stock Exchange (BIST). International Journal of Academic Research in Accounting, Finance and Management Sciences 7: 108–113.\n\nArticle\n  Google Scholar\n\nQiu, X.Y. 2007. On building predictive models with company annual reports (Ph.D. thesis, University of Iowa, Iowa City).\n\nQiu, X.Y., P. Srinivasan, and Y. Hu. 2014. Supervised learning models to predict firm performance with annual reports: An empirical study. Journal of the American Society for Information Science and Technology 65: 400–413.\n\nGoogle Scholar\n\nQiu, X.Y., P. Srinivasan, and N. Street. 2006. Exploring the forecasting potential of company annual reports. In 69th Annual meeting of the American Society for Information Science and Technology (ASIST), Austin, 3–8 November 2006.\n\nRahman, S. 2012. Impression management motivations, strategies and disclosure credibility of corporate narratives. Journal of Management Research 4(3): 1.\n\nArticle\n  Google Scholar\n\nRessas, M.S., and K. Hussainey. 2014. Does financial crisis affect financial reporting of good news and bad news? International Journal of Accounting, Auditing and Performance Evaluation 10(4): 410–429.\n\nArticle\n  Google Scholar\n\nRogers, K., and J. Grant. 1997. Content analysis of information cited in reports of sell-side financial analysts. Journal of Financial Statement Analysis 3: 17–30.\n\nGoogle Scholar\n\nSchleicher, T., and M. Walker. 2010. Bias in the tone of forward-looking narratives. Accounting and Business Research 40(4): 371–390.\n\nArticle\n  Google Scholar\n\nSharma, S. 1995. Applied multivariate techniques. New York: Wiley.\n\nGoogle Scholar\n\nSiqueira, L.F., R.F.A. Júnior, A.A. de Araújo, C.L. Morais, and K.M. Lima. 2017. LDA versus QDA for FT-MIR prostate cancer tissue classification. Chemometrics and Intelligent Laboratory Systems 162: 123–129.\n\nArticle\n  Google Scholar\n\nSmith, M., and R.J. Taffler. 2000. The chairman’s statement: A content analysis of discretionary narrative disclosures. Accounting Auditing & Accountability Journal 13(5): 624–647.\n\nArticle\n  Google Scholar\n\nTedeschi, J.T., and M. Riess. 1981. Identities, the phenomenal self, and laboratory research. In Impression management theory and social psychological research, ed. J.T. Tedeschi, 3–22. New York: Academic Press.\n\nChapter\n  Google Scholar\n\nTharwat, A. 2016. Linear versus quadratic discriminant analysis classifier: A tutorial. International Journal of Applied Pattern Recognition 3: 145–180.\n\nArticle\n  Google Scholar\n\nYu, Y., A. Barros, C. Tsai, and K. Liao. 2014. A comparison of ratios and data envelopment analysis: Efficiency assessment of Taiwan public listed companies. International Journal of Academic Research in Accounting, Finance and Management Sciences 4(1): 212–219.\n\nArticle\n  Google Scholar\n\nZhang, W., Q. Cao, and M. Schniederjans. 2004. Neural network earnings per share forecasting models: A comparative analysis of alternative methods. Decision Sciences 5: 205–237.\n\nArticle\n  Google Scholar\n\nDownload references\n\nAcknowledgements\n\nThe authors are grateful to the Editor of the International Journal of Disclosure and Governance for their help and support. The authors also would like to thank the anonymous referees for their constructive comments. This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\n\nAuthor information\n\nAuthors and Affiliations\n\nCollege of Business Administration, Accounting Department, University of Bahrain, Zallaq, Kingdom of Bahrain\n\nGehan A. Mousa\n\nCollege of Business Administration, Management and Marketing Department, University of Bahrain, Zallaq, Kingdom of Bahrain\n\nElsayed A. H. Elamir\n\nPortsmouth Business School, Faculty of Business and Law, University of Portsmouth, Portsmouth, UK\n\nKhaled Hussainey\n\nCorresponding author\n\nCorrespondence to Gehan A. Mousa.\n\nEthics declarations\n\nConflict of interest\n\nThe authors do not have any conflicts of interest to declare.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nRights and permissions\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nMousa, G.A., Elamir, E.A.H. & Hussainey, K. Using machine learning methods to predict financial performance: Does disclosure tone matter?. Int J Discl Gov 19, 93–112 (2022). https://doi.org/10.1057/s41310-021-00129-x\n\nDownload citation\n\nReceived\n24 November 2020\n\nAccepted\n27 July 2021\n\nPublished\n05 September 2021\n\nIssue Date\nMarch 2022\n\nDOI\nhttps://doi.org/10.1057/s41310-021-00129-x\n\nKeywords\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nA performance comparison of machine learning models for stock market ... : \nAn official website of the United States government\n\nA performance comparison of machine learning models for stock market prediction with novel investment strategy\n\nAbstract\n\nStock market forecasting is one of the most challenging problems in today’s financial markets. According to the efficient market hypothesis, it is almost impossible to predict the stock market with 100% accuracy. However, Machine Learning (ML) methods can improve stock market predictions to some extent. In this paper, a novel strategy is proposed to improve the prediction efficiency of ML models for financial markets. Nine ML models are used to predict the direction of the stock market. First, these models are trained and validated using the traditional methodology on a historic data captured over a 1-day time frame. Then, the models are trained using the proposed methodology. Following the traditional methodology, Logistic Regression achieved the highest accuracy of 85.51% followed by XG Boost and Random Forest. With the proposed strategy, the Random Forest model achieved the highest accuracy of 91.27% followed by XG Boost, ADA Boost and ANN. In the later part of the paper, it is shown that only classification report is not sufficient to validate the performance of ML model for stock market prediction. A simulation model of the financial market is used in order to evaluate the risk, maximum draw down and returns associate with each ML model. The overall results demonstrated that the proposed strategy not only improves the stock market returns but also reduces the risks associated with each ML model.\n\nIntroduction\n\nStock markets being one of the essential pillars of the economy have been extensively studied and researched [1]. Forecasting the stock price is an essential objective in the stock market since the higher expected return to the investors can be guaranteed with better prediction [2]. The price and uncertainty in the stock market is predicted by exploiting the patterns found in the past data [3]. The nature of the stock market has always been vague for investors because predicting the performance of a stock market is very challenging. Various factors like the political disturbance, natural catastrophes, international events and much more must be considered in predicting the stock market [4]. The challenge is so huge that even a small improvement in stock market prediction can lead to huge returns.\n\nThe stock market can only move in one of the two directions: upwards (when stock prices rise) or downwards (when stock prices fall) [5]. Generally, there are four ways to analyze the stock market direction [6]. The most basic type of analysis is the fundamental analysis, which is the way of analyzing the stock market by looking at the company’s economic conditions, reports and future projects [7]. The second and most common technique is technical analysis [8]. In this method, the direction of the stock market is anticipated by looking at the stock market price charts and comparing it with its previous prices [9]. The third and most advanced technique is the Machine learning (ML) based analysis that analyzes the market with less human interaction [10]. ML models find the patterns inside historical data based on which they try to forecast the stock market prices for the future. The fourth technique, called sentimental-based analysis, analyzes the stock market prices by the sentiments of other individuals like activity on social media or financial news websites [11].\n\nThe difficulty of the stock market prediction drew the attention of numerous researchers worldwide. A number of papers have been presented that could predict the stock prices based on ML models. These models include Artificial Neural Network (ANN) [12], Decision Tree (DT) [13], Support Vector Machine (SVM) [14], K-Nearest Neighbors (KNN) [15], Random Forest (RF) [16] and Long Short-Term Memory networks (LSTM) [17]. The proposed systems either used a single ML model optimized for specific stocks [18–20], or multiple ML models in order to analyze their performance on different stocks [21–24]. Many advanced techniques like hybrid models were also employed in order to improve prediction accuracy [25–27].\n\nDifferent ML models like RF and stochastic gradient boosting were used to predict the prices of Gold and Silver with an accuracy of more than 85% [18]. A novel model based on SVM and Genetic Algorithm, called Genetic Algorithm Support Vector Machine (GASVM), was proposed to forecast the direction of Ghana Stock Exchange [19]. The proposed model achieved an accuracy of 93.7% for a 10-day stock price movement outperforming other traditional ML models. The Artificial Neural Network Garch (ANNG) model was used to forecast the uncertainty in oil prices [20]. In this model, first, the GARCH model is used to predict the oil price. This prediction is then used as input to ANN for improvement in the overall commodity price forecast by 30%.\n\nDifferent ML models perform differently on the same historical data. Their performance depends on the type of data and the duration for which the past data is available. In many recent papers, multiple ML models were used on the same financial time series data to predict the future price of the stock to see the performance of each ML model [21–24]. Comparative analysis of nine ML and two Deep Learning (DL) models was performed on Tehran stock market [21]. The main purpose of this analysis was to compare the accuracy of different models on continuous and binary datasets. The binary dataset was found to increase the accuracy of models. In [22], four ML models (ANN, SVM, Subsequent Artificial Neural Network (SANN) and LSTM) were used to predict the Bitcoin prices using different time frames. The results show that SANN was able to predict the Bitcoin prices with an accuracy of 65%, whereas LSTM showed an accuracy of 53% only. In another comparative study [23], four ML models (Multi-Layer Perceptron (MLP), SVM and RF) were used to forecast the prices for different crypto-currencies like Bitcoin, Ethereum, Ripple and Litecoin using their historical prices. MLP outperformed all other models with an accuracy ranging from 64 to 72%. Similar study was performed in [24] showing the performance comparison of different ML models on the same data.\n\nIn some recent studies, hybrid models (a combination of different ML models) are used to forecast stock prices. A hybrid model designed with the SVM and sentimental-based technique was proposed for Shanghai Stock Exchange prediction [25]. This hybrid model was able to achieve the accuracy of 89.93%. A system consisting of k-mean clustering and ensemble learning technique was developed to predict the Chinese stock market [26]. The hybrid prediction model obtained the best forecasting accuracy of the stock price on Chinese stock market. Another hybrid framework was developed in [27] for the Indian Stock Market, this model was developed using SVM with different kernel functions and KNN to predict profit or loss. The proposed system was used to predict the future of stock value. Although the accuracy of the hybrid systems is much higher but they are too complex to be implemented in real-life. Furthermore, a comparative analysis of the prior and proposed study has been shown in Table 1.\n\nTable 1. Comparative analysis of previous and proposed study.\n\nIn almost all the proposed ML-based systems, a primary limitation has been observed in the empirical results. The performance of the ML models were only gauged by their classification ability. Although, it is one of the important parameters being used for the evaluation of the ML model, but it is insufficient to determine the performance of the ML model for stock market prediction. The classification metrics do not take into the account some important factors like returns, maximum draw down, risk-to-reward ratio, transactional cost and the risks associated with each ML model. These factors must be considered in the evaluation of ML models for stock market predictions.\n\nResearch cContributions\n\nThe following are the major contributions of paper:\n\nA performance comparison of nine ML models trained using the traditional methodology for stock market prediction using both performance metrics and financial system simulations.\n\nProposing a novel strategy to train the ML models for financial markets that perform much better than the traditional methodologies.\n\nProposing a novel financial system simulation that provides financial performance metrics like returns, maximum drawdown and risk-to-reward ratio for each ML model.\n\nPaper organization\n\nThe rest of the paper is organized as follows: The next section explains the proposed methodology used in training nine ML models for stock market prediction. Section III analyses the outcomes of simulation models in detail. This section consists of ML models simulation as well as Financial models simulations. The conclusions and future directions are discussed in Sections IV and V respectively.\n\nMethodology\n\nIn this paper, a software approach is used to apply different ML algorithms to predict the direction of the stock market for Tesla Inc. [28]. This prediction system is implemented in Python using frameworks like Scikit-learn [29], Pandas [30], NumPy [31], Alpaca broker [32] and Plotly [33].\n\nThe flowchart of the methodology is illustrated in Fig 1. The first step is to import the stock market data from Alpaca broker and preprocess it using various techniques. The imported stock market data has some information that is not needed in the proposed system. This unwanted data, like trade counts and volume-weighted average price, is removed in the preprocessing stage. Preprocessing also involves handling missing stock prices and cleaning data from unnecessary noise. Missing values can be estimated using interpolation techniques or just by taking the mean value of the point before and after the missing point.\n\nFig 1. Flow chart of the proposed prediction system.\n\nTraditionally, the stock price at the end of the day (EOD) is used in ML-based systems. The variation in the stock price is usually the most in the first hour after the market is open. So, stock price within this hour is more effective than the EOD stock price. The direction of the market is set by the business done in this hour. So, in this paper, the stock price after 15 minutes, when the stock market is open, will also be extracted. The results from the stock price at EOD will be compared with the results from the proposed 15 minutes strategy.\n\nOnce the stock price data has been extracted, the subsequent stage involves computing various input features from the technical indicators and statistical formulas. Nine input features, listed in Table 2, are selected for the prediction purposes. These calculated input features are subjected to overfitting tests. These tests are essential because overfit data can cause reduction in the accuracy of the ML models [34].\n\nTable 2. Selected input feature variables for ML models.\n\nRSI = Relative Strength Index, SMA = Simple Moving Average, ADX = Average Directional Movement Index\n\nFollowing the overfitting tests, the input data is divided into training and testing data. The data is then normalized using Min-Max normalization technique to prevent the biasing phenomenon. Normalization is performed using the following Eq (1):\n\nThe input features and output variables are provided to the ML models in order to detect the patterns within the training data. Various ML models have been employed in this study. Table 3 shows the selected nine ML models to predict the direction of the stock market in this paper. The optimal parameters for each ML models are selected through GridSearchCV [35]. A scikit-learn function that helps in selecting best performing parameter for a particular model. After choosing the optimal parameters, the ML models are trained and tested.\n\nTable 3. Selected ML models for stock market prediction.\n\nIn the next step, the outcome of the trained ML models is assessed using some performance metrics. There are a number of classification metrics that can be used to evaluate the performance of an ML algorithm [45]. Usually, three most powerful measures are chosen to classify these models with respect to their performance. The measures are accuracy, F1 score and Receiver Operator Characteristic and Area Under the Curve (ROC_AUC) [46]. The equations for Accuracy and F1_score are shown below:\n\nFor evaluation purposes the accuracy, ROC_AUC and F1_score are useful measures, however, they are not sufficient for all problems. Recall and precision are two additional well-known metrics for classification problems [47, 48]. The expression for Recall and Precision are also shown in below:\n\nAdditionally, a confusion matrix is used to summarize the performance of each ML model. It provides detailed insight into ML predictions by indicating False Positives (FP), True Positives (TP), False Negatives (FN) and True Negatives (TN) [49]. False Positives show that the model prediction is true while the real sample is false; True Positives show that the model prediction and the real sample both are true; False Negatives represent that the model prediction is false while the real sample is true; True Negatives show that the model prediction and real sample both are false.\n\nIn the next step, a novel financial model is developed and simulated to analyze the performance of the trained ML models. The financial performance metrics like Sharpe ratio, maximum drawdown, cumulative return and annual return [50] are used to analyze the performance of the trained ML models.\n\nThe Sharpe ratio is the measure of risk-free return while the maximum drawdown is the greatest decline in the value of the portfolio [51]. The equations for Sharpe ratio and maximum drawdown are shown in below:\n\nwhere Rp = Return of portfolio, Rf = Risk free rate, σ = Std of portfolio excess return, P = Peak value before largest drop, and L = Lowest value before new high.\n\nAnnual return is the return gained during the period of one year while the cumulative return is the total return on the invested capital within any specific time frame. The expressions for annual return and cumulative return are shown in Eqs (8) and (9).\n\nwhere, E = Ending value, I = Initial value and n = Number of years.\n\nExperimental results\n\nDataset description and project specifications\n\nTesla Inc. is a major American automobile company producing technologically advanced electric vehicles. The company has recently obtained a lot of attention due to its stock prices. A drastic increase in revenue in the year 2021 made Tesla stocks very appealing for capitalists and investors around the world as shown in Table 4 [52].\n\nTable 4 shows the annual growth of Tesla from 2016 to 2021. There has been an increase of almost 70.67% in the year 2021. By taking into account the stock volatility in the previous years and its recent growth, Tesla Inc. is an ideal candidate for this study.\n\nThe stock prices for Tesla Inc. from 2016 to 2021 are considered for experimental evaluations in this paper. Furthermore, the data is split into training data and test datasets. Table 5 shows the ranges of our datasets. The stock market data for Tesla Inc., downloaded from Alpaca broker, from 2016 to 2021 is shown in Fig 2. Additionally, the project specifications can be found in Table 6.\n\nMachine learning models simulation\n\nFirst, the optimal parameters settings for the nine ML models are selected through GridSearchCV. The selected optimal parametric settings for each model are shown in Table 7.\n\nThe simulations for stock market prediction are performed using Python on a Jupiter notebook. ML models were evaluated using Tesla Inc. stock prices for a 1-day time frame and 15-min time interval strategy. These models were first trained on the data from Jan 01, 2016 to Nov 15, 2020. The trained models were then validated on the test data from Nov 16, 2020 to Dec 31, 2021 as shown in Table 5.\n\nTables 8–10 show the classification report for nine different ML models. Tables 8 and 9 show the performance metrics for different ML models for a 1-day time frame and 15-min time interval strategy. These tables list the accuracy, F1 score, ROC AUC, precision and recall in percentage for all of the ML models. Table 10 shows the confusion matrix for the ML models. It lists the number of correct and wrong predictions made by each ML model.\n\nTable 8 shows the performance metrics of nine ML models optimized for a 1-day time frame. As shown in the table, the Logistic Regression achieved the highest accuracy of 85.51% while the Naive Bayes model is found to be the least accurate model with an accuracy of 73.49%. Other classification metrics in Table 8 show a similar tendency with Logistic Regression having the best performance followed by XG Boost and Random Forest.\n\nThe confusion matrix in Table 10 shows a similar trend. For Logistic Regression, the True Positives are 132 and the False Positives are 26 for the ‘Move Up’ class. The True Negatives are 110 and the False Negatives are 15 for the ‘Move Down’ class.\n\nBased on the discussion above, it can be seen that the performance of Logistic Regression model is better than the rest of the models for 1-day time frame. Even though its accuracy among the nine ML models is only 85.51%.\n\nThe graphical illustration of the predictions made by the Logistic Regression model for a 1-day time frame can be seen in Fig 3. It can be seen that the trained Logistic Regression model is able to make more profits than losses. However, it is interesting to note that sometimes the predictions made by the LR model are wrong in the consecutive trades that results in more drawdown. For example, during the period 180 to 230 days, there are a total of 6 trades executed, out of which 4 are losses and 2 are profitable trades.\n\nIn this paper, a novel 15-min time interval strategy has been proposed. In this strategy, the initial 15-min time interval is filtered out from 1-day time frame. Then the filtered 15-min time frame is used to train and validate the ML models in order to make prediction for the time frame of 1-day.\n\nTable 9 shows the performance metrics of the ML models optimized for a 15-min time interval strategy. As shown in Table, the Random Forest achieved the highest accuracy of 91.27% followed by XG Boost and ADA Boost model. The KNN model is found to be the least accurate model with an accuracy of 80.53%. Other classification metrics in Table 9 show a similar tendency with the Random Forest having the best performance model.\n\nThe confusion matrix in Table 10 shows a similar trend. For Random Forest, the True Positives are 130 and the False Positives are 15 for the ‘Move Up’ class. The True Negatives are 142 and the False Negatives are 11 for the ‘Move Down’ class. When the results in Tables 8 and 9 are compared, it can be observed that by employing the proposed methodology, the performance of all the ML models has been greatly improved.\n\nThe graphical illustration of the predictions made by the Random Forest model is shown in Fig 4, it shows the loss and profit in trades. It can also be observed that by using our proposed strategy, the number of consecutive losses has also been reduced. As shown in Fig 4(b), there are only 2 consecutive losses, which occurred during the period of 150 to 200. Factually, the proposed methodology has not only improved the performance metrics of the ML models but it also reduced the number of consecutive losses.\n\nFinancial models simulation\n\nIn this section, a novel financial simulation model is built that is able to make investment based on the decision of the ML model. Each ML model is evaluated using financial parameters to validate their performance and suitability for real-time stock market trading. The performance of ML models is gauged using cumulative return, annual return, maximum drawdown, Sharpe ratio and capital in hand at the end of the investment period.\n\nInitially, a USD 10k is invested. A commission fee of 0.1% (Alpaca standard commission fee) is set for each buy or sell trade. Based on the prediction by the ML model, a decision regarding buying, holding or shorting a share is taken. A single share is bought or sold on each trade to validate the performance of ML models.\n\nFigs 5 and 6 show the portfolio performance of ML models on Tesla Inc. stocks for a 1-day time frame and 15-min time interval strategy. These figures show how initial capital is used to buy and sell shares based on the decision made by the ML models. Each box in the figure represents one full year from Jan 01 till Dec 31. The portfolio of each ML model is compared to a benchmark that serves as a reference for all models. This benchmark is obtained using the positive gains of stock prices.\n\nThe simulated outcomes of the ML models to forecast the stock price of Tesla Inc. for a 1-day period are displayed in Table 11. In the previous section, it was shown that Logistic Regression had the highest accuracy as compared to the other ML models. Therefore, it is expected that this ML model will generate highest revenue. However, the outcome of the financial simulations shows different results. It can be seen in Table 11 that the Random Forest is the best ML model with an ending capital of USD 28,966. It has a cumulative return of 189.66%, and an annual return of 19.48%, with the highest Sharpe ratio of 0.68. The Random Forest did poorly at first but after the 2019 financial market crisis, it outperformed all other ML models. The maximum drawdown of the Random Forest model is -37.21% which happened during 2019 financial crisis as shown in Fig 7. This is the lowest drawdown by any ML model.\n\nThe reason for better revenue generation by the Random Forest model is the quality of each True Positive and True Negative outcome. Even though the accuracy of the model is inferior to the Logistic Regression, each of its correct prediction resulted in more profit. The annual growth of Tesla Inc. from 2020 to 2021 is more than 70% as shown in Table 4. Any correct prediction during this time will result in greater revenue generation. Random Forest model outperformed all other models during this time as shown in Fig 5. Among the ML models, the Naive Bayes model shows the worst performance. Fig 5 shows that the Naive Bayes model is negative most of the time during the simulation. It is the only model with a negative cumulative return of -19.16% and worst Sharpe ratio of 0.1.\n\nThe portfolio performance of the ML models using the proposed approach of a 15-min time interval strategy is shown in Fig 6. This figure shows that the performance of some of the models has improved significantly when compared with a 1-day time frame. It can also be noticed that the models maintained their stability throughout the financial crisis of 2019, which indicates a significant improvement in the real-time performance of the models.\n\nTable 12 displays the outcome of the financial model simulation of ML models trained and validated on Tesla Inc. stocks for a 15-min time interval strategy. As expected, it can be seen that the Random Forest is the best performing model with an ending capital of USD 25,300. It records a cumulative return of 153% and annual return of 16.80% with the highest Sharpe ratio of 0.79. The maximum drawdown by the Random Forest model is—35.09% as shown in Fig 8, but it still able to generate the highest ending capital.\n\nThe above discussion shows that KNN is the worst performing model on the proposed strategy. Although, Random Forest is the best model in terms of portfolio returns but ANN is the most rewarding model with a Sharpe ratio of 0.91 on the proposed 15-min time interval strategy.\n\nConclusion\n\nIn this paper, nine ML models are used to predict the direction of the Tesla Inc. stock prices. The performance of this stock is first assessed for a 1-day time frame followed by a proposed 15-min time interval strategy. Following the traditional methodology, the Logistic Regression achieved the highest accuracy of 85.51% while Naive Bayes model is found to be the least accurate model with an accuracy of 73.49%. The proposed strategy significantly improved the classification performance of the ML models. With this strategy, the Random Forest model achieved the highest accuracy of 91.93% followed by XG Boost and ADA Boost. Conversely, the KNN model is found to be the least accurate model with an accuracy of 80.53%.\n\nIn this paper, it was shown that only classification metrics are not enough to justify the performance of ML models in the stock market. These metrics do not consider important factors like risk, maximum draw down and returns associate with each ML model. A simulation model of the financial market is used to simulate the trained ML models so that their performance is gauged with actual investment strategies. The evaluated results revealed that although some models are performing well in terms of portfolio returns on a traditional methodology but models on the proposed 15-min time frame strategy are significantly better in terms of risk to reward ratio and maximum drawdown. The evaluated result shows that Random Forest outperformed other models in terms of returns in both 1-day and 15-min time interval strategy.\n\nSome other interesting observations are revealed by the comparison of the classification and financial results. The Logistic Regression model has the highest accuracy for a 1-day time frame data. So, it was expected that this ML model will generate the highest revenue. However, the outcome of the financial simulations showed different results. Similarly, the accuracy of the Random Forest model for a 15-min time interval strategy was much higher than the accuracy of the Random Forest model for a 1-day time frame. But instead of generating higher revenue on 15-min time frame strategy, it generated higher revenue on 1-day time frame. The above discussion revealed that however, the accuracy of the ML models is an important factor but the quality of each true positive outcome and true negative outcome is an equally important factor in the performance evaluation of the ML models for stock market prediction.\n\nThe overall results show that the proposed strategy has not only improved classification metrics but it also enhanced the stock market returns, risks and risk to reward ratio of each ML model. Additionally, the results also revealed that how important it is to consider both classification as well as financial analysis to evaluate the performance of the ML model on stock market.\n\nSupporting information\n\nThe data and script has been uploaded to GitHub. It can be accessed using the following link: https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques/.\n\n(IPYNB)\n\nData Availability\n\nData for this study is publicly available from the GitHub repository (https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques).\n\nFunding Statement\n\nThe authors received no specific funding for this work.\n\nReferences\n\nAssociated Data\n\nThis section collects any data citations, data availability statements, or supplementary materials included in this article.\n\nSupplementary Materials\n\nThe data and script has been uploaded to GitHub. It can be accessed using the following link: https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques/.\n\n(IPYNB)\n\nData Availability Statement\n\nData for this study is publicly available from the GitHub repository (https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques).\n\nArticles from PLOS ONE are provided here courtesy of PLOS\n\nConnect with NLM\n\nNational Library of Medicine\n8600 Rockville Pike\nBethesda, MD 20894\n",
        "Machine learning models employed in Fraud Detection and Anomaly Detection": "AI and ML in Fraud Detection - Science Times : \nAI and ML in Fraud Detection\n\nAbstract: Fraud poses significant challenges across industries such as banking, insurance, e-commerce, and retail, necessitating advanced detection mechanisms that go beyond traditional rule-based systems. This article explores the transformative role of artificial intelligence (AI) and machine learning (ML) in fraud detection, highlighting their ability to identify complex patterns, reduce false positives, and enhance operational efficiency. It delves into industry-specific applications, from real-time transaction monitoring in banking to claims assessment in insurance and anomaly detection in e-commerce. The article also examines the technical underpinnings of fraud detection, including supervised learning, anomaly detection, neural networks, and adaptive analytics, while addressing challenges such as algorithmic opacity, data privacy, and evolving fraud tactics. Through case studies and a discussion of emerging trends, the article underscores the expanding role of AI and ML in combating fraud, offering insights into future innovations and their potential to revolutionize fraud detection systems across diverse domains.\n\nKeywords: Fraud detection, artificial intelligence, machine learning, supervised learning, anomaly detection, neural networks, adaptive analytics, e-commerce fraud, banking fraud, insurance fraud, retail fraud, real-time transaction monitoring, data privacy, false positives, compliance challenges, predictive analytics, service-oriented architecture, AWS, cloud computing, big data, cybersecurity, fraud prevention techniques, pattern recognition, emerging fraud tactics, scalable solutions, AI in finance, AI in e-commerce, AI in insurance, AI algorithms, financial fraud, digital security.\n\nAI and machine learning (ML) have become integral tools in the ongoing battle against fraud across various sectors, particularly in banking, insurance, and e-commerce. The prevalence of fraud poses significant financial threats, necessitating advanced detection systems that go beyond traditional rule-based methods. These conventional systems often fall short in adaptability and accuracy, leading to high rates of false positives that are both costly and resource-intensive to manage. In contrast, AI and ML provide dynamic, data-driven solutions capable of identifying complex patterns of fraudulent activities with greater precision.[1][2]\n\nIn the banking and finance sectors, AI and ML have revolutionized fraud detection by enabling real-time analysis of transactional data, which helps institutions like MasterCard swiftly assess and mitigate risks associated with suspicious activities. These technologies are not only reducing the incidence of false positives but are also enhancing customer experience by minimizing unnecessary transaction blocks.[3] Similarly, the insurance industry leverages ML-based systems to streamline the claims process, efficiently flagging high-risk claims and expediting low-risk ones, thereby improving overall security and operational efficiency.[4]\n\nE-commerce and retail sectors face unique challenges due to the vast volume of online transactions, which are prime targets for fraudulent activities. AI and ML technologies are employed to detect anomalies and suspicious patterns in purchasing behavior, thus reducing the financial impact of fraud. By processing extensive transaction data, these systems can effectively predict and preempt fraudulent activities.[5] Furthermore, advancements in AI, such as neural networks and adaptive analytics, enhance the ability to identify non-linear relationships and adapt to emerging fraud tactics, reinforcing security measures across industries.[6]\n\nHowever, the deployment of AI and ML in fraud detection is not without challenges. Issues such as false positives, the opaque nature of certain algorithms, and the evolving tactics of fraudsters necessitate continuous adaptation and refinement of these technologies. Moreover, regulatory compliance and data privacy remain significant hurdles, requiring businesses to navigate complex legal landscapes while ensuring the protection of sensitive information.[7] Despite these challenges, the integration of AI and ML continues to expand, driving innovative approaches and solutions to combat fraud in an increasingly digital world.[8]\n\nApplications in Various Industries\n\nBanking and Finance\n\nIn the banking and finance sectors, AI and ML have been pivotal in transforming fraud detection methods, primarily due to the limitations of traditional rule-based systems. These conventional systems often struggle to adapt to new types of fraud and are prone to generating a high number of false positives, which are costly and time-consuming to manage[1][2]. Machine learning algorithms, however, have demonstrated significant promise in this area by autonomously identifying characteristic patterns in fraudulent activities based on historical data[3]. Financial institutions such as MasterCard have successfully integrated AI and ML to analyze transaction variables like size, location, and time, providing real-time assessments of potentially fraudulent activities[4].\n\nInsurance\n\nIn the insurance industry, the implementation of ML-based solutions has enabled the automation of low-risk claim approvals and improved assessment of high-risk claims[3]. AI firms are increasingly offering fraud detection solutions tailored for the insurance sector, addressing a growing need for enhanced security measures[5]. The adoption of these technologies allows for more accurate and efficient fraud detection, as they can adapt to emerging fraud trends more effectively than traditional methods[6].\n\nE-commerce\n\nThe e-commerce sector is particularly vulnerable to fraud due to the sheer volume of online transactions. AI and ML technologies are employed to detect anomalies and flag suspicious activities, such as unusual purchasing patterns or repeated transactions that could indicate fraudulent behavior[7][8]. These systems help mitigate the financial risks associated with e-commerce fraud by analyzing vast amounts of transaction data to identify and predict potential threats[9].\n\nRetail\n\nSimilar to e-commerce, the retail industry benefits from AI and ML by implementing fraud detection systems that can quickly identify unauthorized transactions or inventory discrepancies. These technologies enhance the ability of retailers to protect their bottom lines by preventing financial losses associated with fraudulent activities[9]. Moreover, machine learning models can improve accuracy in distinguishing between human errors and genuine fraud attempts, thereby reducing unnecessary interventions and false alarms[4].\n\nTechniques and Algorithms\n\nIn the realm of fraud detection, artificial intelligence (AI) and machine learning (ML) employ various techniques and algorithms to combat fraudulent activities across financial and e-commerce sectors. The choice of algorithm often depends on the type of data available and the specific requirements of the application.\n\nSupervised Learning\n\nSupervised learning is a fundamental approach in fraud detection, where models are trained using labeled data to predict whether transactions are fraudulent or legitimate. Common algorithms include decision trees, random forests, Support Vector Machines (SVM), and neural networks, which learn from historical data to recognize patterns indicative of fraud[10][11]. By leveraging labeled datasets, these models can provide precise predictions, identifying fraud based on known instances[10].\n\nAnomaly Detection\n\nAnomaly detection is a straightforward technique that offers binary responses, indicating whether a transaction is typical or suspicious. While this method is simplistic, it serves as an effective tool when combined with rule-based systems, enhancing their accuracy in detecting fraudulent activities[4]. Unsupervised machine learning models, like Isolation Forests, are especially adept at this task, thanks to their ability to isolate anomalous behaviors efficiently in large datasets[12][13].\n\nNeural Networks and Deep Learning\n\nNeural networks, inspired by the human brain's structure, play a crucial role in detecting complex fraud patterns that traditional algorithms might overlook. These networks are particularly valuable in real-time fraud detection systems due to their ability to learn and improve performance over time[14][12]. Deep learning, a subset of machine learning, further enhances this capability by building flexible models tailored to specific fraud detection tasks, such as identifying non-linear relationships in transaction data[14][3].\n\nAdaptive and Predictive Analytics\n\nAdaptive analytics represent an evolution in predictive analytics, focusing on real-time data analysis rather than historical data. This approach is vital in combating the dynamic nature of fraud, where fraudsters continually develop new techniques. Integrating adaptive analytics with supervised ML models allows systems to quickly adapt and become resilient to new types of fraud[2].\n\nCombining Multiple Techniques\n\nAdvanced fraud detection systems often combine multiple machine learning algorithms to reduce uncertainty and improve accuracy. By integrating various styles and mathematical models, these systems can adapt to changing fraud patterns, offering robust and scalable solutions for different industries, including banking, insurance, and online marketplaces[4][13].\n\nChallenges and Limitations\n\nThe implementation of AI and ML in fraud detection presents several challenges and limitations that businesses in the financial and e-commerce sectors must navigate. One major issue is the prevalence of false positives, where a legitimate transaction is mistakenly flagged as fraudulent. This error often arises from the rules and guidelines used during the training phase of non-fraudulent transactions, which may not adequately account for all possible variations in genuine behavior[11][2]. Such inaccuracies not only undermine the system's credibility but also can frustrate customers whose legitimate transactions are unnecessarily blocked[2].\n\nMoreover, the complexity and black-box nature of some ML algorithms, such as random forests and neural networks, pose significant challenges in interpreting how these models generate outputs from certain inputs[3]. The difficulty in explaining the decision-making process of these complex models complicates their acceptance and trustworthiness among industry professionals[3].\n\nAnother limitation is the dynamic nature of fraudulent tactics, which continually evolve with the interconnectedness of global markets and the rapid exchange of digital information[10]. AI and ML models must adapt quickly to these changes to remain effective, requiring constant updates and retraining with new data. The necessity for vast amounts of clean, relevant training data also impacts the accuracy of fraud detection models, as the quality of the data directly correlates with model performance[15].\n\nFurthermore, businesses face challenges related to compliance and data privacy. The use of large datasets for training ML models must comply with stringent regulations, making data masking techniques essential to obfuscate sensitive information during processing[3]. Developing bespoke fraud detection solutions to meet compliance fully can be resource-intensive, necessitating significant investments in data availability, ML model training times, and computational resources[3].\n\nFinally, the persistence of financial fraud despite these technological advances reflects broader limitations. Traditional techniques like manual verifications remain costly and time-consuming, and while AI provides precision in identifying fraud patterns, its implementation does not entirely eradicate fraudulent activities, which continue to adversely impact the economy and society[8][9].\n\nCase Studies\n\nBanking Sector\n\nIn the banking industry, fraud detection is critical, given the sensitivity of financial transactions. Leading financial institutions have integrated machine learning (ML) and artificial intelligence (AI) to combat fraudsters effectively. For instance, MasterCard has incorporated AI and ML to analyze various transaction variables such as size, location, time, device, and purchase data. This system evaluates account behavior and makes real-time decisions about whether a transaction is fraudulent[4]. Banks benefit from ML-based fraud detection solutions by being able to monitor multiple channels of data simultaneously, allowing for the detection of fraud across different types of transactions or applications[5]. Furthermore, AI and ML systems provide advantages over traditional rules-based systems, offering quick and accurate fraud identification, reducing false positives, cutting manual labor costs, and enhancing the customer experience[16].\n\nE-commerce Sector\n\nThe e-commerce sector faces unique challenges in fraud detection, with an increasing need for efficient solutions to mitigate financial risks. This has led to the development of e-commerce fraud detection models based on information fusion technology, utilizing computer technology, AI, and data mining[7]. Companies can opt for personalized fraud detection software, though this requires extensive data, training time, and computational resources. Alternatively, businesses might choose off-the-shelf ML-powered systems from providers like Amazon and IBM, which offer ready-to-use models and scalable resources[3]. Using data masking techniques can help comply with regulatory standards while training these models, ensuring secure processing of sensitive information[3].\n\nInsurance Sector\n\nIn the insurance sector, fraud detection solutions are increasingly being sought after due to the rising demand for security in claims processing. AI firms are actively developing and selling claims fraud detection solutions to insurers, emphasizing the sector's growing reliance on AI technologies[5]. The application of AI and ML in insurance is part of a broader trend where regulatory technology (regtech) is being utilized to streamline compliance and reporting processes, further enhancing fraud detection capabilities by automating decisions and leveraging large data sets in real-time [17].\n\nThese case studies highlight the pervasive adoption of AI and ML in fraud detection across various industries, each with tailored solutions to address their specific challenges and regulatory environments.\n\nFuture Trends\n\nThe future of AI and ML in fraud detection is poised for significant advancements as technological capabilities continue to evolve. In financial institutions, the shift from traditional rule-based systems to AI/ML solutions marks a pivotal transition driven by the need for rapid and accurate fraud identification, reduced false positives, and improved customer experiences[16][11]. As companies strive to maintain competitiveness, the adoption of AI/ML-powered systems is expected to accelerate, with many firms anticipated to integrate these technologies within a few years[16].\n\nThe banking and finance industries are leveraging AI/ML to transform compliance and risk management. These technologies are employed to analyze vast amounts of data in real time, automating compliance decisions and enhancing regulatory compliance[17]. As regulatory environments tighten and compliance costs rise, the importance of AI in regulatory technology (regtech) is expected to grow, offering improved compliance quality and cost efficiency[17].\n\nE-commerce platforms are also increasingly adopting AI/ML solutions to combat fraud, utilizing techniques like big data mining and information fusion technology to develop sophisticated fraud detection models[7]. This is part of a broader trend where businesses are employing AI-driven strategies to minimize financial risks and stay ahead of evolving fraud tactics[1][7].\n\nLooking forward, the integration of AI and ML in fraud detection is likely to expand into new domains, with technologies becoming more adaptive to novel fraud types[1]. Companies will continue exploring off-the-shelf machine learning-powered systems, such as Amazon Fraud Detector and IBM Security Trusteer, which offer scalable and customizable solutions for diverse industry needs[3]. Additionally, the use of AI-based solutions like TrustDecision illustrates the proactive approaches businesses are taking to safeguard against financial losses and maintain stakeholder trust[12].\n\nReferences\n\n[1] GDS Link. (2023, April 13). The Future is Now: The Benefits and Limitations of Using AI and Machine Learning for Fraud Detection. GDS Link. https://www.gdslink.com/the-future-is-now-the-benefits-and-limitations-of-using-ai-and-machine-learning-for-fraud-detection/\n\n[2] AIMultiple. (2023, December 21). Top 4 Use Cases of Generative AI in Banking '24. AIMultiple. https://research.aimultiple.com/ai-fraud-detection/\n\n[3] Ahramovich, A. (2023, September 26). Machine learning for fraud detection: essentials, use cases, and guidelines. Itransition. https://www.itransition.com/machine-learning/fraud-detection\n\n[4] AltexSoft. (2017, December 22). Fraud Detection: How Machine Learning Systems Help Reveal Scams in Fintech, Healthcare, and eCommerce. AltexSoft. https://www.altexsoft.com/whitepapers/fraud-detection-how-machine-learning-systems-help-reveal-scams-in-fintech-healthcare-and-ecommerce/\n\n[5] Mejia, N. (2020, March 10). AI-Based Fraud Detection in Banking – Current Applications and Trends. Emerj Artificial Intelligence Research. https://emerj.com/ai-sector-overviews/artificial-intelligence-fraud-banking/\n\n[6] Nomentia. (2024, February 20). 16 real-life use cases for AI and ML in payment fraud detection. Nomentia. https://www.nomentia.com/blog/ai-machine-learning-in-fraud-detection\n\n[7] Li, J. (2022). E-Commerce Fraud Detection Model by Computer Artificial Intelligence Data Mining. Computational Intelligence and Neuroscience, 2022(8783783). https://doi.org/10.1155/2022/8783783\n\n[8] Ali, A., Razak, S. A., Othman, S. H., Eisa, T. A. E., Al-Dhaqm, A., Nasser, M., Elhassan, T., Elshafie, H., & Saif, A. (2022). Financial Fraud Detection Based on Machine Learning: A Systematic Literature Review. Applied Sciences, 12(19), 9637. https://doi.org/10.3390/app12199637\n\n[9] Razorthink Inc. (2019, January 11). 4 Major Challenges facing Fraud Detection; Ways to Resolve Them using Machine Learning. Medium. https://medium.com/razorthink-ai/4-major-challenges-facing-fraud-detection-ways-to-resolve-them-using-machine-learning-cf6ed1b176dd\n\n[10] LeewayHertz. (2023). AI in fraud detection: Use cases, architecture, benefits, solution and implementation. LeewayHertz. https://www.leewayhertz.com/ai-in-fraud-detection/\n\n[11] Pushkar, A. (2024, November 7). Fraud Detection Algorithms Using Machine Learning. Intellipaat. https://intellipaat.com/blog/fraud-detection-machine-learning-algorithms/\n\n[12] TrustDecision. (2024, May 19). 5 New Fraud Detection Machine Learning Algorithms. TrustDecision. https://trustdecision.com/resources/blog/5-new-machine-learning-algorithms-for-fraud-detection\n\n[13] Bassi, E. (2023, April 24). How is artificial intelligence used in fraud detection? Cointelegraph. https://cointelegraph.com/explained/how-is-artificial-intelligence-used-in-fraud-detection\n\n[14] Ravelin. (n.d.). Your guide to machine learning for fraud prevention. Ravelin. https://www.ravelin.com/insights/machine-learning-for-fraud-detection\n\n[15] Horan, T. J. (2022, January 10). 5 Keys to Using AI and Machine Learning in Fraud Detection. FICO. https://www.fico.com/blogs/5-keys-using-ai-and-machine-learning-fraud-detection\n\n[16] PYMNTS. (2021, September 17). Deep Dive: How AI- and ML-Powered Analysis Can Improve Fraud Detection and Investigation. PYMNTS. https://www.pymnts.com/fraud-prevention/2021/deep-dive-artificial-intelligencen-machine-learning-powered-analysis-improve-fraud-detection-investigation/\n\n[17] Boukherouaa, E. B., & Shabsigh, G. (2021). Powering the Digital Economy: Opportunities and Risks of Artificial Intelligence in Finance. International Monetary Fund. https://doi.org/10.5089/9781589063952.087\n\nAbout the Author\n\nSrinivasa Reddy Adaboina is a seasoned IT professional with over 19 years of experience in enterprise application development, cloud technologies, and software architecture. Proficient in Java, J2EE, and AWS, Srinivasa has spearheaded projects across industries such as banking, insurance, e-commerce, and government sectors. He is an advocate of innovative fraud detection techniques and has deep expertise in AI, machine learning, and service-oriented architectures. Srinivasa's international experience spans South Africa, Malaysia, Singapore, and India, showcasing his adaptability to diverse work cultures. A recipient of the prestigious 2024 Global Recognition Award for AWS Financial Solutions, Srinivasa is also an IEEE Senior Member and a published author with a focus on advancing technology in fraud prevention and cloud-based solutions.\n\nSubscribe to The Science Times!\n\nSign up for our free newsletter for the Latest coverage!\n\nRECOMMENDED STORIES\n\nVoyager 2’s Historic Uranus Flyby May Have Captured Rare Event, Changing Scientists’ View of the Planet\n\nIs the Ozone Layer Repairing Itself? Scientists Think So\n\nSpaceX Dragon Successfully Docks With ISS, Delivering 6,000 Pounds of Supplies\n\nColorectal Cancer Deaths Increasing Among Millennials and Gen X: Learn the Warning Signs\n\nFinancial fraud detection through the application of machine learning ... : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nAdvertisement\n\nFinancial fraud detection through the application of machine learning techniques: a literature review\n\nHumanities and Social Sciences Communications\n 11, Article number: 1130 (2024) Cite this article\n\n42k Accesses\n\n4 Citations\n\n1 Altmetric\n\nMetrics\n\nAbstract\n\nFinancial fraud negatively impacts organizational administrative processes, particularly affecting owners and/or investors seeking to maximize their profits. Addressing this issue, this study presents a literature review on financial fraud detection through machine learning techniques. The PRISMA and Kitchenham methods were applied, and 104 articles published between 2012 and 2023 were examined. These articles were selected based on predefined inclusion and exclusion criteria and were obtained from databases such as Scopus, IEEE Xplore, Taylor & Francis, SAGE, and ScienceDirect. These selected articles, along with the contributions of authors, sources, countries, trends, and datasets used in the experiments, were used to detect financial fraud and its existing types. Machine learning models and metrics were used to assess performance. The analysis indicated a trend toward using real datasets. Notably, credit card fraud detection models are the most widely used for detecting credit card loan fraud. The information obtained by different authors was acquired from the stock exchanges of China, Canada, the United States, Taiwan, and Tehran, among other countries. Furthermore, the usage of synthetic data has been low (less than 7% of the employed datasets). Among the leading contributors to the studies, China, India, Saudi Arabia, and Canada remain prominent, whereas Latin American countries have few related publications.\n\nSimilar content being viewed by others\n\nFeature generation and contribution comparison for electronic fraud detection\n\nIntegrating machine learning into business and management in the age of artificial intelligence\n\nA robust and interpretable ensemble machine learning model for predicting healthcare insurance fraud\n\nIntroduction\n\nFinancial fraud represents a highly significant problem, resulting in grave consequences across business sectors and impacting people’s daily lives (Singh et al., 2022). Its occurrence leads to reduced confidence in the economy, resulting in destabilization and direct economic repercussions for stakeholders (Reurink, 2018). Abdallah et al. (2016) define fraud as a criminal act aimed at obtaining money unlawfully. There are diverse types of fraud, such as asset misappropriation, expense reimbursement, and financial statement manipulation. Scholars have classified fraud into three categories: banking, corporate, and insurance (Ali et al., 2022; Nicholls et al., 2021; West and Bhattacharya, 2016).\n\nThe problem becomes evident in the case of financial fraud, evidenced by the 2022 figures of the PricewaterhouseCoopers survey report revealing that 56% of companies globally have fallen victim to some form of fraud. In Latin America, 32% of companies have experienced fraud (PricewaterhouseCoopers, 2022). These alarming statistics align with the findings from Klynveld Peat Marwick Goerdeler (KPMG), indicating that 83% of the surveyed executives reported being targeted by cyber-attacks in the past 12 months. Furthermore, 71% had encountered some type of internal or external fraud (KPMG, 2022). These survey results reveal the higher risks of financial fraud faced by companies in Latin America, the United States, and Canada. In this context, traditional approaches, and techniques, as well as manual methods, have lost relevance and effectiveness because they cannot effectively address the complexity and scale of the information involved in detecting financial fraud.\n\nAs previously mentioned, despite the interest of organizations in detecting financial fraud using machine learning (ML), current knowledge in this field remains limited. After an initial research phase, specialized literature shows that most researchers have directed their efforts toward the analysis of credit card fraud using a supervised approach (Femila Roseline et al., 2022; Madhurya et al., 2022; Plakandaras et al., 2022; Saragih et al., 2019). In the studies of Ali et al. (2022), Hilal et al. (2022), and Ramírez-Alpízar et al. (2020), ML techniques employing the supervised approach were found to be the most widely used method for detecting financial fraud, compared to the unsupervised, deep learning, reinforcement, and semi-supervised approaches, among others. Moreover, scholars such as Whiting et al. (2012) have compared the performance of data mining models for detecting fraudulent financial statements using data from quarterly and annual financial indexes of public companies from the COMPUSTAT database.\n\nReurink (2018) has analyzed financial fraud resulting from false financial reports, scams, and misleading financial sales in the context of the financial market. Just like Wadhwa et al. (2020), he presented a wide variety of data mining methods, approaches, and techniques used in fraud detection, in addition to research addressing online banking fraud (Zhou et al., 2018; Moreira et al., 2022; Srokosz et al., 2023) and financial statement fraud (S. Chen, 2016; Ramírez-Alpízar et al., 2020). The abovementioned research works show that the accuracy of ML techniques in developing models for detecting financial fraud has increased (Al-Hashedi and Magalingam, 2021).\n\nThe effectiveness of financial fraud detection and prevention depends on the effective selection of appropriate ML techniques to identify new threats and minimize false fraud alarm warnings, responding to the negative impact of financial fraud on organizations (Ahmed et al., 2016). The use of ML techniques has made it possible to identify patterns and anomalies in large financial data sets. However, developments in detection tools, inaccurate classification, detection methods, privacy, computer performance, and disproportionate misclassification costs continue to hinder the accurate and timely detection of financial fraud (Dantas et al., 2022; Mongwe and Malan, 2020; Nicholls et al., 2021; West and Bhattacharya, 2016).\n\nRecently, several studies have reviewed financial statement fraud detection methods in data mining and ML (Gupta and Mehta, 2021; Shahana et al., 2023); however, the present study is different from these past works in the area. These authors established the types of financial fraud and the different data mining techniques and approaches used to detect financial statement fraud. In contrast, our study explains the trends in the use of ML approaches and techniques to detect financial fraud, and it presents the more frequently used datasets in the literature for conducting experiments.\n\nFraud detection mechanisms using machine learning techniques help detect unusual transactions and prevent cybercrime (Polak et al., 2020). Although each of these approaches uses different methods in their experimentation, a systematic literature review (SLR) shows that the application of each algorithm mirrors performance metrics to determine the accuracy with which it predicts that a financial transaction is fraud. Such metrics include Accuracy, Precision, F1 Score, Recall, and Sensitivity, among others.\n\nThe research presented uses a rigorous and well-structured methodology to expand current knowledge on financial fraud detection using machine learning (ML) techniques. Through the use of a systematic literature review that follows adaptations of PRISMA guidelines and Kitchenham’s methodology, the study ensures a carefully planned and transparent review process. The sources of information consulted include research articles published in reputable academic databases such as Scopus, IEEE Xplore, Taylor & Francis, SAGE, and ScienceDirect, ensuring that the review covers the most relevant and quality scientific literature in the field of financial fraud and machine learning. Moreover, the study includes a bibliometric analysis using VOSviewer software, which allows identifying trends and patterns within the literature both quantitatively and visually. Based on the 104 articles reviewed, which cover the period 2012–2023, we manage to describe the types of fraud, the models applied, the ML techniques used, the datasets employed, and the metrics of performance reported. These contribute to filling the existing gaps in the literature by providing a comprehensive and up-to-date synthesis of the evidence on the use of machine learning techniques for financial fraud detection, thus laying the groundwork for future research and practical applications in this field.\n\nOur responses to the initial research questions raised are four main contributions that justify this research. Thus, this study contributes to the literature on financial fraud detection by examining the relationship between the current literature on financial fraud detection and ML based on the scholars, articles, countries, journals, and trends in the area. Fraud has been classified as internal and external, with a focus on credit card loan fraud investigations and insurance fraud. The different ML techniques and their models applied to experiments were grouped. The most widely used datasets in financial fraud detection using ML are analyzed according to the 86 articles that contained experiments, highlighting that most of them involve real data. This paper is useful for researchers because it studies and presents the metrics used in supervised and unsupervised learning experiments, providing a clear view of their application in the different models.\n\nTherefore, this study is relevant because it presents in a consolidated and updated manner new contributions derived from experiment results regarding the use of ML, which helps address the problem when financial fraud occurs.\n\nThe research work is organized as follows: the section “Methods” comprehensively describes the research method and the questions addressed in the study. Section “Results of the data synthesis” presents the findings encompassing authors, articles, sources, countries, trends, financial fraud types, and datasets with their characteristics to which the detection models using ML techniques were applied, with the results of their metrics. Finally, the section “Discussion and conclusion” highlights the conclusions, including future lines of research in the field.\n\nMethods\n\nThe study focuses on SLR, which provides a comprehensive view of the great developments in financial fraud detection. Considering the purpose, scientific guidelines were followed in the literature review of the PRISMA and Kitchenham methods, which were adapted by the authors (Ashtiani and Raahemi, 2022; Kitchenham and Brereton, 2013; Kitchenham and Stuart, 2007; Kumbure et al., 2022; Moher et al., 2009; Roehrs et al., 2017; Saputra et al., 2023; Wohlin, 2014).\n\nThe method used in the SLR was developed with carefully planned and executed activities: (a) planning of the review, (b) definition of research questions, (c) description of the search strategy, (d) consultation concerning the search strategy, (e) selection of the inclusion/exclusion criteria and data selection, (f) description of the quality assessment, (g) investigation of the study topics, (h) description of data extraction, and (i) synthesis of the data.\n\nEach of the activities conducted in this study is explained below.\n\nPlanning of the review\n\nThe research purpose was established in accordance with the indicated research goals and questions. The analysis focused on research articles published between 2012 and 2023, particularly those using ML methods for financial fraud detection. Accordingly, the SLR procedure presented by Kitchenham and Stuart (2007) and Moher et al. (2009) was implemented following a series of steps adapted and modified by Ashtiani and Raahemi (2022) and Kumbure et al. (2022), as depicted in Fig. 1. Thus, it was possible to ensure a rigorous and objective analysis of the available literature in our field of interest.\n\nDescription of the general process used to review the literature in the study area. Authors’ own elaboration.\n\nThe procedures implemented in this review process are discussed in the following subsections.\n\nDefinition of research questions\n\nIn SLR, research questions are key and decisive for the success of the study (Kitchenham and Stuart, 2007). Therefore, analyzing the existing literature on financial fraud detection through ML techniques and its characteristics, problems, challenges, solutions, and research trends is crucial. Table 1 describes the research questions to provide a structured framework for the study.\n\nWithin the proposed systematic review, the questions were fine-tuned, achieving a better classification and thematic analysis. The research questions were categorized into two groups: general questions (GQ) and specific questions (SQ). GQs provide an overview of the current state of the art, that is, a general framework for future research. Meanwhile, SQs focus on specific matters emerging from the application areas of the topic, thereby improving the filtering process of the study.\n\nDescription of the search strategy\n\nThe search strategy was designed to identify a set of studies addressing the research questions posed. This strategy was to be implemented in two stages. In the first stage, a manual search was conducted by selecting a set of test documents through a defined database. Following the strategy proposed by Wohlin (2014), a snowball search was conducted. This approach involved choosing from a set of initial references (e.g., relevant articles or books addressing the subject matter) and searching for new related references relevant to the study based on these.\n\nIn the second stage, an automated search was performed using the technique described by Kitchenham and Brereton (2013), which included preparing a list of the main search terms to be applied in the queries in each database, as indicated in subsection “Search queries”.\n\nIn the study’s initial stage, nine journal articles were selected from the test set of papers (Ahmed et al., 2016; Ali et al., 2022; Bakumenko and Elragal, 2022; Gupta and Mehta, 2021; Hilal et al., 2022; Nicholls et al., 2021; Nonnenmacher and Marx Gómez, 2021; Ramírez-Alpízar et al., 2020; West and Bhattacharya, 2016). The manual literature search helped identify articles related to financial fraud detection through ML techniques, which were used as an initial set and were part of the final analysis. In the subsequent stage, a backward and forward snowball search was conducted. This approach involved using the initial set to select the relevant articles.\n\nThe backward snowball search process comprised reviewing article titles, including those meeting the inclusion and exclusion criteria. In the forward snowball search, the analysis was performed in the Scopus database to identify studies citing one or more of the articles in the initial set. This filtering method helped identify studies meeting the inclusion and exclusion criteria, eliminate duplicates from the previous set, and analyze articles answering the questions posed, which were retained in the final study set.\n\nThe research work mainly aimed to obtain a reliable set of relevant studies to minimize bias and increase the validity of the results. To this end, a manual search for articles meeting the inclusion and exclusion criteria was conducted by assessing the abstracts and other sections of articles. We decided to implement an automated search strategy using five databases: Scopus, IEEE Xplore, Taylor & Francis, SAGE, and ScienceDirect, known for their impartiality in the representation of research works, with inclusion and exclusion criteria already defined, thereby complementing the search. Thus, 104 related articles meeting the criteria established in the final set were identified.\n\nSearch queries\n\nStudies from 2012 onward were reviewed with keywords such as “financial fraud” and “machine learning” to identify model-based approaches and associated techniques. Table 2 presents a summary of the queries used in each data source.\n\nInclusion and exclusion criteria and study selection\n\nThe study established inclusion and exclusion criteria, a key process to select the most relevant articles. The exclusion criteria were documents published between 2012 and 2023 (until March), such as conference reviews, book chapters, editorials, and reviews. Further, the availability of the full text of the article was considered. We decided to exclude articles published before 2012 for the following reasons: (i) They were over 11 years old; (ii) Relevant publications prior to 2012 were scarce; and (iii) Sufficient number of articles were available between 2012 and 2023.\n\nFor the inclusion and exclusion criteria, appropriate filtering tools were applied to each data source during the search stage. This enabled the automated selection of the most relevant and appropriate studies based on the research goal.\n\nData processing strategies\n\nIn the data processing strategy used, databases were selected following strict inclusion and exclusion criteria to ensure the quality and relevance of the information collected (Table 3). Various databases initially identified the following number of relevant articles: Scopus (28), Taylor & Francis (80), SAGE (71), ScienceDirect (663), and IEEE Xplore (5132). This initial step provides a broad overview of the available literature in the field of financial fraud detection using ML models.\n\nSubsequently, a data removal phase was carried out so as to ensure data integrity, such that the following number of articles (given in parentheses) were removed from each database: Scopus (0), Taylor & Francis (63), SAGE (57), ScienceDirect (636), and IEEE Xplore (5114). This rigorous process ensures the integrity of the data collected and avoids redundancy.\n\nThe final step consisted of obtaining the consolidated number of articles included after the selection and exclusion of duplicates: Scopus (28), Taylor & Francis (17), SAGE (14), ScienceDirect (27), and IEEE Xplore (18). This methodological strategy ensured the relevance of the articles that carried out a complete analysis in the field of financial fraud detection using ML models.\n\nQuality assessment\n\nOnce the inclusion and exclusion criteria were applied, the remaining articles were assessed for quality. The evaluation criteria used included the purpose of the research; contextualization; literature review; and related works, methods, conclusions, and results. To minimize the empirical obstacles associated with full-text filtering, a set of questions proposed by Roehrs et al. (2017) (see Table 4) was used to validate whether the selected articles met the previously established quality criteria.\n\nResearch topics\n\nIn conducting the literature review to understand the current state of published research on the topic, a data orientation process was addressed, including preprocessing techniques and ML models and their metrics. Accordingly, four research topics were defined based on the research goals. They are presented in Table 5.\n\nData extraction\n\nFor data extraction, the necessary attributes were first defined and the information pertaining to the study goals was summarized. Next, the relevant information was identified and obtained through a detailed reading of the full text of each article. The information was then stored in a Microsoft Excel spreadsheet. Data were collected on the attributes specified in Table 6. In Table 6, the “Study” column corresponds to the identifiers of the research topics in Quality Assessment, and the “Subject” column refers to the category to which the different attributes belong. The names of the attributes and a brief description are presented in the last two columns of the table, including additional columns with relevant information.\n\nData synthesis\n\nData synthesis included analyzing and summarizing the information observed in the selected articles to address the research questions. To perform this task, a synthesis was conducted following the guidelines proposed by Moher et al. (2009) based on qualitative data. Further, a descriptive analysis was performed to obtain answers to the research questions. Consequently, a qualitative approach to data evidence was followed.\n\nResults of the data synthesis\n\nIn this section, the 104 finally selected articles have been considered. The data were synthesized to address the five research questions mentioned.\n\nGeneral questions (GQ)\n\nGQ1: Which were the most relevant authors, articles, sources, countries, and trends in the literature review on financial fraud detection based on the application of machine learning (ML) models?\n\nAuthors\n\nThe literature on financial fraud detection applying ML models has been studied by a large number of authors. However, some authors stood out in terms of the number of published papers and number of citations. Specifically, the most significant authors with two publications are Ahmed M. (with 318 citations), Ileberi E. (82 citations), Ali A. (20 citations), Chen S. (84 citations), and Domashova J and Kripak E. (each with 6 citations). Other relevant authors with one publication and who have been cited several times are Abdallah A. (with 333 citations), Abbasimehr H. (18 citations), Abd Razak S. (13 citations), Achakzai M. A. K. (5 citations), and Abosaq H. (2 citations). The aforementioned authors have contributed significantly to the development of research in financial fraud detection using ML models (Fig. 2).\n\nShows the analysis of the connections between authors based on co-authorship of publications. Produced with VOSviewer.\n\nCollectively, the researchers have contributed a solid knowledge base and have laid the foundation for future research in financial fraud detection using ML models. Although other researchers contributed to the field, such as Khan, S. and Mishra, B., both with 7 citations, among others, some have been more prominent in terms of the number of papers published. Their collective works have enriched the field and have promoted a greater understanding of the challenges and opportunities in this area.\n\nArticles\n\nAs depicted in Fig. 3, clusters 2 (green) and 4 (yellow) present the most relevant research articles on financial fraud detection using ML models. Cluster 2, comprising 9 articles with 357 citations and 32 links, is highlighted because of the significant impact of the articles by Sahin, Huang, and Kim. These articles have the highest number of citations and are deemed to be useful starting points for those intending to dive into this research field. Cluster 4, constituting 6 articles with 158 citations and 27 links, includes the works of Dutta and Kim, who have also been cited considerably.\n\nDepicts the connections between articles based on their bibliographic references. Produced with VOSviewer.\n\nArticles in clusters 1 (red) and 3 (dark blue) could be valuable sources of information; however, they were observed to have a lower number of citations and links than those in clusters 2 and 4, such as that of Nian K. (62 citations and 4 links) and Olszewski (92 citations and 4 links). However, some articles in these clusters have had a substantial number of citations.\n\nIn Cluster 10 (pink), the article by Reurink A. is prominent, with 38 citations. This is followed by the article by Ashtiani M.N. with 10 citations. In Cluster 11 (light green), the article by Hájek P. has 129 citations. In Cluster 12 (grayish blue), the articles by Blaszczynski J. and Elshaar S. have the greatest number of citations, indicating their influence in the field of financial fraud detection.\n\nIn Cluster 13 (light brown), the article by Pourhabibi T. has the greatest number of citations at 102, suggesting that he has been relevant in the research on financial fraud detection. Finally, in Cluster 14 (purple), the articles by Seera M. have 63 citations and 2 links. The article by Ileberi E. has 11 citations and 1 link. Both articles have a small number of citations, indicating a lower influence on the topic.\n\nIn conclusion, clusters 2, 4, and 11 are the most relevant in this literature review. The articles by Sahin, Huang, Kim, Dutta, and Pumsirirat are the most influential ones in the research on financial fraud detection through the application of ML models.\n\nSources\n\nThe information presented in Fig. 4 is the result of a clustering analysis of the articles resulting from the literature review on financial fraud detection by implementing ML models. In total, 48 items were identified and grouped into 12 clusters. The links between the items were 100, with a total link strength of 123.\n\nShows the relationship between different scientific journals based on bibliographic links. Produced with VOSviewer.\n\nThe following is a description of each cluster with its respective number of items, links, and total link strength (the number of times a link appears between two items and its strength):\n\nCluster 1 (6 articles—red): This cluster includes journals such as Computers and Security, Journal of Network and Computer Applications, and Journal of Advances in Information Technology. The total number of links is 27, and the total link strength is 32.\n\nCluster 2 (6 articles—dark green): This cluster includes articles from Technological Forecasting and Social Change, Journal of Open Innovation: Technology, Market, and Complexity, and Global Business Review. The total number of links is 18, and the total link strength is 19.\n\nCluster 3 (5 articles—dark blue): This cluster includes articles from the International Journal of Advanced Computer Science and Applications, Decision Support Systems, and Sustainability. The total number of links is 19, and the total link strength is 20.\n\nCluster 4 (4 articles—dark yellow): This cluster includes articles from Expert Systems with Applications and Applied Artificial Intelligence. The total number of links is 26, and the total link strength is 45.\n\nCluster 5 (4 articles—purple): This cluster includes articles from Future Generation Computer Systems and the International Journal of Accounting Information Systems. The total number of links is 15, and the total link strength is 16.\n\nCluster 6 (4 articles—dark blue): This cluster includes articles from IEEE Access and Applied Intelligence. The total number of links is 18, and the total link strength is 26.\n\nCluster 7 (4 articles—orange): This cluster includes articles from Knowledge-Based Systems and Mathematics. The total number of links is 23, and the total link strength is 29.\n\nCluster 8 (4 articles—brown): This cluster includes articles from the Journal of King Saud University—Computer and Information Sciences and the Journal of Finance and Data Science. The total number of links is 13, and the total link strength is 13.\n\nCluster 9 (4 articles—light purple): This cluster includes articles from the International Journal of Digital Accounting Research and Information Processing and Management. The total number of links is 2, and the total link strength is 2.\n\nThe clusters represent groups of related articles published in different academic journals. Each cluster has a specific number of articles, links, and total link strength. These findings provide an overview of the distribution and connectedness of articles in the literature on financial fraud detection using ML models. Further, clustering helps identify patterns and common thematic areas in the research, which may be useful for future researchers seeking to explore this field.\n\nClusters 1, 4, and 7 indicate a greater number of stronger articles and links. These clusters encompass articles from Computers and Security, Expert Systems with Applications, and Knowledge-Based Systems, which are important sources for the SLR on financial fraud detection through the implementation of ML models.\n\nCountries\n\nThe analysis presented indicates the number of documents related to research in different countries and territories. In this case, a list of 50 countries/territories and the number of documents related to the research conducted in each of them is presented. China leads with the highest paper count at 18, followed by India at 13 and Saudi Arabia and Canada at 9 each. Canada, Malaysia, Pakistan, South Africa, the United Kingdom, France, Germany, and Russia have similar research outputs with 4–9 papers. Sweden and Romania have 1 or 2 research papers, indicating limited scientific research output.\n\nThe presence of little-known countries such as Armenia, Costa Rica, and Slovenia suggests ongoing research in places less common in the academic world. From that point on, the number of papers has gradually decreased.\n\nThe production of papers is geographically distributed across countries from different continents and regions. However, more research exists on the subject from countries with developed and transition economies, which allows for a greater capacity to conduct research and produce papers.\n\nFigure 5, sourced from Scopus’s “Analyze search results” option, depicts countries with their respective number of published papers on the topic of financial fraud detection through ML models.\n\nRepresents the number of scientific publications in the study area classified by country. Produced with VOSviewer.\n\nThe above shows the diversity of countries involved in the research, where China leads the number of studies with 18 papers, followed by India with 13 and Saudi Arabia and Canada each with 9 papers. The other countries show little production, with less than 7 publications, which indicates an emerging topic of interest for the survival of companies that must prevent and detect different financial frauds using ML techniques.\n\nTrends\n\nThe most relevant keywords in the review of literature on financial fraud detection implementing ML models include the following:\n\nIn Cluster 1, the most relevant keywords are “decision trees” (13 repetitions), “support vector machine (SVM)” (11 repetitions), “machine-learning” (10 repetitions), and “credit card fraud detection” (9 repetitions). A special focus has been placed on the topic of artificial intelligence (ML), in addition to algorithms and/or supervised learning models such as decision trees, support vector machines, and credit card fraud detection.\n\nIn Cluster 2, the most relevant keywords are “crime” (46 repetitions), “fraud detection” (43 repetitions), and “learning systems” (13 repetitions). These terms reflect a broader focus on financial fraud detection, where the aspects of crime in general, fraud detection, and learning systems used for this purpose have been addressed.\n\nIn Cluster 3, the most relevant keywords are “Finance” (19 repetitions), “Data Mining” (18 repetitions), and “Financial Fraud” (12 repetitions). These keywords indicate a focus on the financial industry, where data mining is used to reveal patterns and trends related to financial fraud.\n\nIn Cluster 4, the most relevant keywords are “Machine Learning” (45 repetitions), “Anomaly Detection” (16 repetitions), and “Deep Learning” (11 repetitions). They reflect an emphasis on the use of traditional ML and deep learning techniques for anomaly detection and financial fraud detection.\n\nIn general, the different clusters indicate the most relevant keywords in the SLR on financial fraud detection through ML models. Each cluster presents a specific set of keywords reflecting the most relevant trends and approaches in this field of research (Fig. 6).\n\nShows the relationships between keywords based on their co-occurrence in the literature reviewed. Produced with VOSviewer.\n\nGQ2: What types of financial fraud have been identified in ML studies?\n\nFinancial fraud is generated by weaknesses in companies’ control mechanisms, which are analyzed based on the variables that allow them to materialize. These include opportunity, motivation, self-fulfillment, capacity, and pressure. Some of these are comprehensively analyzed by Donald Cressey through the fraud theory approach. The lack of modern controls has led organizations to use ML in response to this major problem. According to the findings of the Global Economic Crime and Fraud Survey 2022–2023, which gathered insights from 1,028 respondents across 36 countries worldwide, instances of fraud within these companies have caused a financial loss of approximately 10 million dollars (PricewaterhouseCoopers, 2022).\n\nReferring to the concept of fraud, as outlined in international studies (Estupiñán Gaitán, 2015; Márquez Arcila, 2019; Montes Salazar, 2019) and the guidelines of the American Institute of Certified Public Accountants, it is an illegal, intentional act in which there is a victim (someone who loses a financial resource) and a victimizer (someone who obtains a financial resource from the victim). Thus, the proposed classification includes corporate fraud and/or fraud in organizations, considering that the purpose is to misappropriate the capital resources of an entity or individual: cash, bank accounts, loans, bonds, stocks, real estate, and precious metals, among others.\n\nIn this SLR study, we have considered fraud classifications by authors of 86 articles, which encompass experiments. We have excluded the 18 SLR articles from our analysis. The types presented in Table 7 follow the holistic view of the authors of the research for a better understanding of the subject of financial fraud, considering whether it is internal or external fraud.\n\nTable 7 highlights the diverse types of frauds, and the research works on them. According to the classification, external frauds correspond to those performed by stakeholders outside the company. This study’s findings show that 54% of the analyzed articles investigate external fraud, among which the most important studies are on credit card loan fraud, followed by insurance fraud, using supervised and unsupervised ML techniques for their detection.\n\nIn research works (Kumar et al., 2022) analyzing credit card fraud, attention is drawn to the importance of prevention through the behavioral analysis of customers who acquire a bank loan and identifying applicants for bad loans through ML models. The datasets used in these fraud studies have covered transactions performed by credit card holders (Alarfaj et al., 2022; Baker et al., 2022; Hamza et al., 2023; Madhurya et al., 2022; Ounacer et al., 2018; Sahin et al., 2013), while other research works have covered master credit card money transactions in different countries (Wu et al., 2023) and fraudulent transactions gathered from 2014 to 2016 by the international auditing firm Mazars (Smith and Valverde, 2021).\n\nThe second major type of external fraud is insurance fraud, which is classified as fraud in health insurance programs involving practices such as document forgery, fraudulent billing, and false medical prescriptions (Sathya and Balakumar, 2022; Van Capelleveen et al., 2016) and automobile insurance fraud involving fraudulent actions between policyholders and repair shops, who mutually rely on each other to obtain benefits (Aslam et al., 2022; Nian et al., 2016; Subudhi and Panigrahi, 2020); as a result of the issues they face, insurance companies have developed robust models using ML.\n\nAs regards internal fraud, caused by an individual within the company, 46% of studies have analyzed this type, with financial statement fraud, money laundering fraud, and tax fraud standing out. The studies show that the investigations are based on information reported by the US Securities and Exchange Commission (SEC) and the stock exchanges of China, Canada, Tehran, and Taiwan, among others. To a considerable extent, the information taken is from the real sector, and very few studies have obtained synthetic information based on the application of different learning models.\n\nThe following is a summary of the financial information obtained by the researchers to apply AI models and techniques:\n\nStock market financial reports: Fraud in the Canadian securities industry (Lokanan and Sharma, 2022), companies listed on the Chinese stock exchanges (Achakzai and Juan, 2022; Y. Chen and Wu, 2022; Xiuguo and Shengyong, 2022), companies with shares according to the SEC (Hajek and Henriques, 2017; Papík and Papíková, 2022), companies listed on the Tehran Stock Exchange (Kootanaee et al. 2021), companies in the Taiwan Economic Journal Data Bank (TEJ) stock market (S. Chen, 2016; S. Chen et al., 2014), analysis of SEC accounting and auditing publications (Whiting et al., 2012)\n\nWrong financial reporting to manipulate stock prices (Chullamonthon and Tangamchit, 2023; Khan et al., 2022; Zhao and Bai, 2022)\n\nFinancial data of 2318 companies with the highest number of financial frauds (mechanical equipment, medical biology, media, and chemical industries; Shou et al., 2023), fraudulent financial restatements (Dutta et al., 2017)\n\nData from 950 companies in the Middle East and North Africa region (Ali et al., 2023), analyzing outliers in sampling risk and inefficiency of general ledger financial auditing (Bakumenko and Elragal, 2022), fraudulent intent errors by top management of public companies (Y. J. Kim et al., 2016), reporting of general ledger journal entries from an enterprise resource planning system (Zupan et al., 2020)\n\nSynthetic financial dataset for fraud detection (Alwadain et al., 2023).\n\nStudies have analyzed situations involving fraudulent financial statements. In these cases, instances of fraud have already occurred, leading to the creation of financial reports that contain statements with outliers that can be deemed fraudulent intent or errors in financial figures. This raises a reasonable doubt about whether an intent exists with regard to the reporting of unrealistic figures. Notably, once there are parties responsible for the financial information presented to stakeholders, such as organization owners, managers, administrators, accountants, or auditors, it is unlikely for it to be unintentional (an error). In this context, transparency and explainability are essential so as to ensure fairness in decisions, thus avoiding bias and discrimination based on prejudiced data (Rakowski et al., 2021).\n\nBecause of its significance, the information reported in financial statements is vital for investigations. Studies have indicated substantial amounts of data extracted from the financial reports of regulatory bodies such as stock exchanges and auditing firms. These entities use the data to establish the existence of fraud and its types through predictive models that use ML techniques. Thus, they require financial data such as dates, the third party affected, user, debit or credit amount, and type of document, among other aspects involving an accounting record. This information aids in identifying the possible impact in terms of lower profits and the perpetrator and/or perpetrators to gather sufficient evidence and file criminal proceedings for the financial damage caused.\n\nMoreover, investigations concerning money laundering fraud and/or money laundering, the second most investigated internal fraud type, encompass the reports of natural and legal persons exposed by the Financial Action Task Force in countries such as the Kingdom of Saudi Arabia (Alsuwailem et al., 2022), transactions from April to September 2018 from Taiwan’s “T” bank and the account watch list of the National Police Agency of the Ministry of Interior (Ti et al., 2022), money laundering frauds in Middle East banks (Lokanan, 2022), transactions of financial institutions in Mexico from January 2020 (Rocha-Salazar et al., 2021), and synthetic data of simulated banking transactions (Usman et al., 2023).\n\nConcerns regarding the entry of proceeds from money laundering into an organization have been articulated in relation to the financial damage it causes to the country. At the macroeconomic level, these activities negatively affect financial stability, distorting the prices of goods and services. Moreover, such activities disrupt markets, making it difficult to make efficient financial decisions. At the microeconomic level, legitimate businesses face unfair competition with companies using illegal money, which may lead to higher unemployment levels. Furthermore, money laundering has a social impact because it affects the security and welfare of society.\n\nThus, some research works (Alsuwailem et al., 2022) have indicated the need to implement ML models for promoting anti-money laundering measures. For instance, in Saudi Arabia, money from illicit drug trafficking, corruption, counterfeiting, and product piracy have entered the country. The measures to be taken are categorized according to the three stages of money laundering: placement, layering (also known as concealment), and integration. These include new legal regulations against money laundering, staff training, customer identification and validation, reporting of suspicious activities, and documentation and storage of relevant data (Bolgorian et al., 2023).\n\nRegarding the 7.5% incidence of internal fraud, specifically categorized as tax fraud resulting from tax evasion, the studies have analyzed tax returns on income and/or profits of legal persons and/or individuals from the Serbian tax administration during 2016–2017 (Savić et al., 2022). Studies have encompassed periodic value-added tax (VAT) returns, together with the anonymous list of clients for the tax year 2014 obtained from the Belgian tax administration (Vanhoeyveld et al., 2020) and income tax and VAT taxpayers registered and provided by the State Revenue Committee of the Republic of Armenia in 2018 (Baghdasaryan et al., 2022). These studies hold great relevance for tax administrations using different strategies to minimize the impact of fraud resulting from tax evasion. Tax evasion reduces the government’s ability to collect revenue, directly affecting government finances and causing budget deficits, thereby increasing public debt.\n\nGQ3: Which ML models were implemented to detect financial fraud in the datasets?\n\nGiven that ML is a key tool to extract meaningful information and make informed decisions, this study analyzes the most widely used ML techniques in the field of financial fraud detection. It takes as reference 86 experimental articles, excluding 18 SLR articles. In these articles, the most commonly used trends and approaches in the implementation of ML techniques in financial fraud detection were identified.\n\nFor the analysis, the pattern of frequency of use of ML models was observed. Several of them have been prominent because of their popularity and implementation in detecting financial fraud (Fig. 7). Some of the most widely used models include long-short term memory (LSTM) with 7 mentions, autoencoder with 10 mentions, XGBoost with 13 mentions, k-nearest neighbors (KNN) with 14 mentions, artificial neural network (ANN) with 17 mentions, NB with 19 mentions, SVM with 29 mentions, DT with 29 mentions, LR with 32 mentions, and RF with 34 mentions.\n\nIllustrates the most common machine learning models in financial fraud detection. Authors’ own elaboration.\n\nThe LSTM model is a recurrent neural network used for sequence processing, especially for tasks concerning natural language processing (Chullamonthon and Tangamchit, 2023; Esenogho et al., 2022; Femila Roseline et al., 2022). Moreover, autoencoders are models used for data compression and decompression. These models are useful in dimensionality reduction applications (Misra et al., 2020; Srokosz et al., 2023). XGBoost is a library combining multiple weak DT models, offering a scalable and efficient solution in classification and regression tasks (Dalal et al., 2022; Udeze et al., 2022).\n\nKNN and ANN are widely used models in various ML applications. KNN is based on neighbor closeness, and ANN is inspired by human brain functioning. NB is a probabilistic algorithm commonly used in text classification and data mining (Ashtiani and Raahemi, 2022; Lei et al., 2022; Shahana et al., 2023).\n\nSVM, DT, LR, and RF, the most commonly mentioned models, are used in a wide range of classification and regression applications. These models are prominent because of their effectiveness and applicability to different scenarios, such as credit card loan fraud (external fraud) and financial statement fraud (internal fraud).\n\nThe most frequently used ML techniques are supervised learning (56.73%); unsupervised learning (18.29%), a combination of supervised and unsupervised learning (15.38%), a combination of supervised and deep learning (2.88%), and mathematical approach, supervised, and semi-supervised learning (0.96%). Figure 8 presents the ML techniques in the literature reviewed and indicates the number of times each type of technique is applied. Some articles applied several ML methods, in which the algorithms are mainly classified according to the learning method. In this case, there are four main types: supervised, semi-supervised, unsupervised, and deep learning.\n\nShows the different experimental approaches used in the study. Authors’ own elaboration.\n\nSupervised learning is the most widely used technique, with 56.73% of citations in financial fraud studies. In this approach, labeled training data are used, where the expected outputs are known and a model is built that can make higher-accuracy predictions on new unlabeled data. Common examples of supervised learning techniques include the models of LR, SVM, DT, RF, KNM, NB, and ANN.\n\nMoreover, unsupervised learning constitutes 18.27% of the mentions. The technique focuses on discovering patterns in the data without knowing data with labels and/or types for training. Some of these include DBSCAN, autoencoder, and isolation forest (IF).\n\nThe combination of supervised, unsupervised, and semi-supervised learning is used with a frequency of 1.92%. This technique and/or approach combines elements of supervised and unsupervised learning, using both labeled and unlabeled data to train the models. It is also used when labeled data are scarce or expensive to obtain; thus, the aim is to take advantage of unlabeled information to improve model performance.\n\nFinally, supervised and deep learning represents 2.88% of the mentions. It is based on deep neural networks with multiple neurons and hidden layers to learn complex data representations. It has achieved remarkable developments in areas such as image processing, voice recognition, and machine translation.\n\nSpecific questions (SQ)\n\nSQ1: What datasets were used by implementing ML models for financial fraud detection?\n\nFirst, the data structure and fraud types may vary with the collection of datasets. The performance of fraud detection models may be affected by variations in the number of instances and attributes selected. Therefore, investigating the datasets and their characteristics is relevant, as data differ in terms of data type (number, text) and the data source from which they were obtained (synthetic and/or real), as can be observed in Fig. 9.\n\nDepicts the datasets used in the research on financial fraud detection. Authors’ own elaboration.\n\nCredit card fraud detection\n\nThe dataset was created by the Machine Learning group at Université Libre de Bruxelles. It encompasses anonymized credit card transactions labeled as fraudulent or genuine. The transactions were performed in September 2013 over two days by European cardholders; a record of only 492 frauds out of 284,807 transactions is highly unbalanced because the positive types (frauds) represent only 0.172% of all transactions (Machine Learning Group, 2018).\n\nThe characteristics of the set encompass numerical variables resulting from a principal component analysis (PCA) transformation. For confidentiality, the original features of the data have not been disclosed. Features V1, V2…, V28 have been the main components obtained through PCA. The only features that have not transformed with PCA include “Time,” which denotes the seconds elapsed between each transaction. “Amount” denotes the transaction amount. The “Class” feature is the response variable, taking 1 as the value in case of fraud and 0 (no fraud) otherwise.\n\nThis dataset has been used by 15 authors in their papers, who have applied different financial fraud detection techniques (Alarfaj et al., 2022; Baker et al., 2022; Fanai and Abbasimehr, 2023; Fang et al., 2019; Femila Roseline et al., 2022; Hwang and Kim, 2020; Ileberi et al., 2021, 2022; Khan et al., 2022; Misra et al., 2020; Ounacer et al., 2022).\n\nStatlog (German credit data)\n\nThe dataset was proposed by Professor Hofmann to the UC Irvine ML repository on November 16, 1994, for facilitating credit rating (Hofmann, 1994). It mainly aims to determine whether a person presents a favorable or unfavorable credit risk (binary rating). The set is multivariate, which implies that it contains many attributes used in credit rating. These attributes include information on existing current account status, credit duration, credit history, and credit purpose and amount, among others. In total, there are 20 attributes describing several characteristics of individuals and contains 1000 instances; it has been widely used in research related to credit rating (Esenogho et al., 2022; Fanai and Abbasimehr, 2023; Lee et al., 2018; Pumsirirat and Yan, 2018; Seera et al., 2021).\n\nStalog (Australian credit approval)\n\nThe dataset belongs to the UC Irvine ML repository and was created by Ross Quinlan in 1997. It focuses on credit card applications within the financial field (Quinlan, 1997). It has a total of 690 instances and 14 attributes of which 6 are numeric of type integer/actual and 8 are categorical; consequently, its data characteristics are multivariate—that is, it contains multiple variables and/or attributes. Several studies have used the ensemble data (Lee et al., 2018; Pumsirirat and Yan, 2018; Seera et al., 2021; Singh et al., 2022).\n\nChina Stock Market and Accounting Research\n\nThe China Stock Market and Accounting Research (CSMAR) Database contains financial reports and violations of CSMAR. It provides information on China’s stock markets and the financial statements of listed companies; the data were collected between 1998 and 2016 from publicly funded companies (CSMAR, 2022). It includes fraudulent and non-fraudulent companies committing several types of fraud, such as showing higher profits and/or earnings, fictitious assets, false records, and other irregularities in financial reporting.\n\nThe set comprises 35,574 samples, including 337 annual fraud samples of companies in the Chinese stock market. This is selected as a data source to illustrate the financial statement information of listed companies in three studies (Achakzai and Juan, 2022; Y. Chen and Wu, 2022; Shou et al., 2023).\n\nSynthetic financial datasets for fraud detection\n\nIt was generated by the PaySim mobile money simulator using aggregated data from a private dataset deriving from one month of financial records from a mobile money service in an African country (López-Rojas, 2017). The original records were provided by a multinational company offering mobile financial services in more than 14 countries worldwide. The dataset has been used in numerous studies (Alwadain et al., 2023; Hwang and Kim, 2020; Moreira et al., 2022).\n\nThe synthetic dataset provided is a scaled-down version, representing a quarter of the original dataset. It was made available for Kaggle. It constitutes 6,362,620 samples, with 8213 fraudulent transaction samples and 6,354,407 non-fraudulent transactions. It includes several attributes related to mobile money transactions: transaction type (cash-in, cash-out, debit, payment, and transfer); transaction amount in local currency; customer information (customer conducting the transaction and transaction recipient); initial balances before and after the transaction; and fraudulent behavior indicators (isFraud and isFlaggedFraud). These attributes indicate a binary classification.\n\nDefault of credit card clients\n\nIt was created by I-Cheng Yeh and introduced on January 25, 2016, and is available in the UC Irvine ML repository (Yeh, 2016). The dataset, which is used for classification tasks, focuses on the case of defaulted payments of credit card customers in Taiwan in the business area. Moreover, it is a multivariate dataset with 30,000 instances and 24 attributes. They include attributes such as the amount of credit granted, payment history, and statement records spanning April through September 2005. This data source is selected in studies such as those by Esenogho et al. (2022), Pumsirirat and Yan (2018), and Seera et al. (2021).\n\nEdgar Lopez Rojas created the dataset in 2017. The synthetic data were generated in the BankSim payment simulator. It is based on a sample of transactional data provided by a bank in Spain (López-Rojas, 2017). It includes the following characteristics: step, customer ID, age, gender, zip code, merchant ID, zip code of merchant, category of purchase, amount of purchase, and fraud status. It comprises 594,643 transactions, of which ~1.2% (7200) were labeled as fraud and the rest (587,443) were labeled as genuine, and it was processed as a binary classification problem. The dataset has been used in several investigations (Esenogho et al., 2022; Pumsirirat and Yan, 2018; Seera et al., 2021).\n\nCOMPUSTAT\n\nThis dataset is a financial and economic information and research database (Compustat, 2022). It contains characteristics related to various aspects of companies, such as asset quality, revenues earned, administrative and sales expenses, and sales growth, among others. COMPUSTAT collects and stores detailed information on listed companies in the United States and Canada. The set includes information on 61 characteristics and consists of 228 companies, of which half showed fraud in their information while the other half did not present fraud (binary classification), and it is used in studies (Dutta et al., 2017; Whiting et al., 2012).\n\nInsurance Company Benchmark (COIL 2000)\n\nThis dataset is used in the CoIL 2000 challenge, available at the UC Irvine Machine Learning Repository, created by Peter Van Der Putten. It consists of 9822 instances and 86 attributes containing information about customers of an insurance company and includes data on product use and sociodemographic data (Putten, 2000). It is characterized as multivariate and is used to perform regression/classification tasks by studies using the dataset (Huang et al., 2018; Sathya and Balakumar, 2022).\n\nBitcoin network transactional metadata\n\nThis dataset contains Bitcoin transaction metadata from 2011 to 2013. It was created by Omer Shafiq (Kaggle handle: OmerShafiq) and introduced to the Kaggle online community in 2019. The set comprises 11 attributes and 30,000 instances related to Bitcoin transactions, bitcoin flows, connections between transactions, average ratings, and malicious transactions (Omershafiq, 2019). It is efficient for investigating and analyzing anomalies and fraud detection in Bitcoin transactions (Ashfaq et al., 2022).\n\nSQ2: What were the metrics used to assess the performance of ML models to detect financial fraud?\n\nBased on previous studies (Nicholls et al., 2021; Shahana et al., 2023), the performance of the metrics used in ML models is the last step in determining whether the results align with the problem at hand. The metrics demonstrate the ability to do a specific task, such as classification, regression, or clustering quality, as they allow comparing the performance of models.\n\nMany evaluation metrics have been used in previous studies, such as precision, sensitivity, recall, accuracy, and area under the curve. These metrics can be calculated using the confusion matrix. Figure 10 compares the target and true values with the predicted ones based on the study by Torrano et al. (2018).\n\nPresents the confusion matrix generated during the evaluation of the financial fraud detection models. Authors’ own elaboration.\n\nAccording to previous studies (Shahana et al., 2023; Zhao and Bai, 2022), true positive (TP) projects a positive value (fraud) that matches the true value; true negative (TN) accurately predicts a negative outcome (no fraud); false positive (FP) denotes the predicted positive whose true value is negative (no fraud); and false negative (FN) represents the predicted negative whose true value is positive (fraud). FP and FN represent the misclassification cost, also known as classification model prediction error.\n\nThe metrics used to evaluate the effectiveness of supervised ML techniques are as follows. The accuracy metric is the most commonly used (Ramírez-Alpízar et al., 2020). It is defined as the total number or proportion of correct predictions/samples over the total number of records analyzed. Further, it is a method of evaluating the performance of a binary classification model distinguishing between true and false. In Eq. (1), it calculates the accuracy metric.\n\nThe sensitivity metric known as recall (TP or TPR rate) is the ratio of successfully identified fraudulent predictions to the total number of fraudulent samples. Equation (2) calculates the sensitivity metric.\n\nThe specificity metric (TN rate or TNR) is the percentage of non-fraudulent samples properly designated as non-fraudulent. It is represented in Eq. (3).\n\nAccuracy is the ratio of correctly classified fraudulent predictions to the total number of fraudulent predictions. Equation (4) calculates the precision metric.\n\nF1-score is a metric that combines accuracy and recall using a weighted harmonic mean (Bakumenko and Elragal, 2022). It is presented in Eq. (5).\n\nType I error (FP or FPR rate) is the number of legitimate predictions mistakenly labeled as fraudulent as a percentage of all legitimate predictions. The metric is defined in Eq. (6).\n\nType II error (FN or FNR rate) is the proportion of fraudulent samples incorrectly designated as non-fraudulent. Type I and II errors make up the overall error rate. It is defined in Eq. (7).\n\nThe area under the curve (AUC), or area under the receiver operating characteristic curve, represents a graphic of TPR versus FPR (Y. Chen and Wu, 2022). AUC values range from 0 to 1; the more accurate an ML model, the higher its AUC value. It is a metric that represents the model’s performance when differentiating between two classes.\n\nFollowing the guidelines in previous studies (Amrutha et al., 2023; García-Ordás et al., 2023; Palacio, 2019), some metrics used to evaluate the effectiveness of unsupervised ML techniques will be defined.\n\nThe silhouette coefficient identifies the most appropriate number of clusters; a higher coefficient means better quality with this number of clusters. Equation (8) calculates the metric.\n\nwhere x denotes the average of the distances of observation j with respect to the rest of the observations of the cluster to which j belongs. Furthermore, y denotes the minimum distance to a different cluster. The silhouette score takes values between −1 and 1. Based on the study by Viera et al. (2023), 1 (correct) represents the assignment of observation j to a good cluster, zero (0) indicates that observation j is between two distinct groups, and −1 (incorrect) indicates that the assignment of j to the cluster is a bad clustering.\n\nThe rand index is the similarity measure between two clusters considering all pairs and including those assigned to the same cluster in both the predictions and the true cluster. Equation (9) calculates the index.\n\nThe Davies–Bouldin metric is a score used to evaluate clustering algorithms. It is defined as the mean value of the samples, represented in Eq. (10).\n\nwhere k denotes the number of groups \\({c}_{i},{c}_{j}\\), k represents the centroids of cluster i and j, respectively, with \\(d\\left({c}_{i},{c}_{i}\\right)\\) as the distance between them, while \\({\\alpha }_{i}\\) and \\({\\alpha }_{j}\\) corresponds to the average distance of all elements in clusters i and j and the distance to their respective \\({c}_{i}\\) and \\({c}_{j}\\) centroids (Viera et al., 2023).\n\nThe Fowlkes–Mallows index is defined as the geometric mean between precision and recall, represented in Eq. (11).\n\nThe cophenetic correlation coefficient is a clustering method to produce a dendrogram (tree diagram). Equation (12) indicates the metric.\n\nwhere \\(x(i,j)=|{x}_{i}-{x}_{j}|\\) represents the Euclidean distance between the ith and jth points of \\(x\\). While \\(t(i,j)\\) is the height of the node at which the two points, \\({t}_{i}\\) and \\({t}_{j}\\), of the dendrogram meet and \\(\\bar{x}\\) and \\(\\bar{t}\\) are the mean value of \\(x(i,j)\\) and \\(t(i,j).\\)\n\nDiscussion and conclusion\n\nResearch on the detection of financial fraud by applying ML techniques is a significant topic. On the one hand, fraud directly affects the business world and, on the other hand, detecting it early involves great challenges; this has led to designing tools using AI, such as ML techniques. This study is an SLR using adaptations of the PRISMA and Kitchenham methods to critically analyze and synthesize the study results. Research articles published in Scopus, IEEE Xplore, Taylor & Francis, SAGE, and ScienceDirect were explored. The results were presented in two parts. The first one included a bibliometric study with the open-source software VOSviewer, followed by a discussion of the SLR results.\n\nThe bibliometric analysis presented the results of the authors, articles, sources, countries, and most important trends in the literature on financial fraud detection by applying ML, as well as an analysis of fraud types, ML models, and datasets. From the 104 articles dating from 2012 to 2023, several types of fraudulent activities are described, as well as external (e.g., credit cards, insurance) and internal (e.g., financial statements, money laundering) frauds, and a brief report on fraud, in general, is provided. Further, it was possible to extract supervised and unsupervised ML techniques, with the 10 most used models as RF in supervised techniques and autoencoder as an unsupervised technique.\n\nDuring the literature review on the detection of financial fraud using machine learning models, it became evident that several authors have made significant contributions. However, some stand out more in terms of the number of publications and citations. Some of the most notable ones, Ahmed M. with 318 citations, Ileberi E. with 82, and Chen S. with 84, have made important advances in the field. Others, such as Abdallah A., with only one publication, but with 333 citations, have also made a considerable impact. And although researchers such as Khan S. and Mishra B. have fewer citations, the combined work of all these authors has established a robust knowledge base, providing a deeper understanding of the challenges and opportunities present in financial fraud detection through machine learning techniques.\n\nConsistent with the analysis of the article clusters, clusters 2, 4 and 11 emerge as the most influential in this field with topics of interdisciplinary interest (artificial intelligence/machine learning, accounting, finance), among academics and auditing firms. The SLR evidences that authors in these domains often cooperate when it comes to publication, in turn, studies by (Huang et al., 2018; J. Kim et al., 2019; Sahin et al., 2013; Dutta et al., 2017) are highly cited articles.\n\nSimilarly, the leading countries in the research area include China, which has the largest number of published articles, followed by India and Saudi Arabia. The production of articles on the subject was found to be geographically distributed among countries whose economies are developing and are in transition, which indicates a greater capacity for the production of papers and research. In comparison to Ashtiani and Raahemi’s (2022) study highlighting the United States, leading with the largest number of papers (18) in the area, followed by China (8) and Greece (7), Al-Hashedi and Magalingam’s (2021) posit that India is the top producer of articles with 24, followed by China (14) and the United States (9).\n\nThe journals that have accepted the publication of these studies are specifically in the accounting and computer science domain. There is much literature on computers and security, expert systems with applications, and knowledge-based systems on financial fraud detection through ML models, as supported by Al-Hashedi and Magalingam (2021) and Ali et al. (2022). The keywords highlighted in the studies include crime, fraud detection, and ML. These words indicate a central focus on the financial industry, where learning and/or data mining systems help discover patterns or anomalies in financial data, in addition to attractive trends and approaches in the research field.\n\nThe literature has indicated articles investigating fraud types, particularly credit card loan fraud and insurance fraud, which are of great interest to the scientific community (Al-Hashedi and Magalingam, 2021; Ali et al., 2022; West and Bhattacharya, 2016). This study has classified the different types of fraud into internal and external, and sub-classifications have been derived. In both types, ML techniques have been used to detect financial fraud—supervised (59 articles), unsupervised (19 articles), supervised and unsupervised (16 articles), and deep learning (3 articles), among others. Most of the studies analyzed have developed binary classification models, that is, fraud or non-fraud. Supervised learning techniques require labeled data, and the most frequently used models are LR, RF, and SVM, among others. In the experiments, the prevalence of metrics such as accuracy, precision, sensitivity, and F1-score are highlighted. For unsupervised learning as a technique, the data do not have a label and focus on discovering new patterns with algorithms such as DBSCAN, autoencoder, and IF, among others. The evaluation with internal metrics was not made in detail. Few studies using semi-supervised learning and deep learning techniques have been highlighted because of the fact that they are novel.\n\nFurther, it is found in the trend through the keywords, as the research works address the subject of ML, learning algorithms, deep learning, SVM, fraudulent transactions, and anomaly detection, but it is evident that there is little research on unsupervised learning and deep learning. The scarce use of these techniques may be because of the complexity of the models and the high consumption of computational resources. In the analysis of the 86 experiment articles, few articles were found that used unsupervised techniques. Also, a large part of the datasets used is labeled, which requires further experimentation with models and unlabeled real-world datasets (Ounacer et al., 2018; Pumsirirat and Yan, 2018; Rubio et al., 2020; Van Capelleveen et al., 2016; Vanini et al., 2023). Meanwhile, labeled data are costly because an expert is required for their construction. Thus, more attention has been given to data origin, preprocessing, and feature extraction before training an ML model to increase detection accuracy. Accordingly, it should be emphasized that deep learning models require a thorough design and adjustment compared with previous models. They are quite sensitive to the architecture structure and choice of hyperparameters. Further, the data quality and quantity required is relatively high, so it should be considered in the design stage.\n\nThe studies show that the datasets for the experiments were taken from the stock exchanges of China, Canada, the United States, Taiwan, and Tehran, among others. The researchers used ML models to detect financial fraud in credit card loans, highlighting the use of the “Credit Card Fraud Detection” dataset, mentioned 15 times. Also, the performance of ML models can be affected because of the selected set by the number of selected attributes and instances. From the analysis, it was observed that most of the articles use real datasets obtained from existing databases, historical records, or other collection methods, and few studies use synthetic datasets (four articles), which are those generated by modeling or simulation techniques and try to mimic a real dataset.\n\nStill, the integration of real and synthetic datasets enables a comprehensive approach to the problem by providing a basis and complementary information for conclusions and comparisons with other studies on the performance of ML models. Specifically, the datasets used in recent studies and/or articles, spanning from 2012 to 2023, reveal concern related to obsolete data approximately from 1994, which, because of their age, do not provide effective and accurate results in the current context as a result of the new fraud modalities created day after day, with characteristics and behavior patterns that have evolved significantly over time.\n\nThe literature review and bibliometric analyses on financial fraud detection using machine learning and its various techniques conducted between 2012 and 2023 show a remarkable evolution in this field. Authors, including Ahmed M., Ileberi E., and Chen S. have made important contributions with a high number of citations. There has been fundamental interdisciplinary collaboration between areas such as artificial intelligence, accounting, finance, and information security, highlighting widely cited studies such as Huang et al. (2018), J. Kim et al. (2019), Sahin et al. (2013), and Dutta et al. (2017). Countries such as China, India and Saudi Arabia leading in publications can be seen, which reflects the global effort of emerging economies. Supervised learning techniques such as Random Forest, and unsupervised ones, like Autoencoder, are the most widely used. Furthermore, the effort and enthusiasm for the use of deep learning, despite its complexity and high computational resource requirements, are evident.\n\nResearch mainly uses real datasets such as those from the Chinese, Canadian, US, Taiwanese, and Tehran stock exchanges, with the “Credit Card Fraud Detection” dataset being the most important one. The journals that publish these studies belong both to the accounting area and to computer science, with extensive literature in Computers and Security, Expert Systems with Applications, and Knowledge-Based Systems. While it is true that the accuracy of fraud detection depends on the quality of the data and preprocessing with various algorithms, the need for robust and updated approaches to face new fraud modalities is particularly highlighted.\n\nLimitations and scope for future research\n\nThe study had limitations that affected the scope and interpretation of the results. Although a systematic review was performed, the lack of quantitative support in the data collected is acknowledged. From the 104 articles identified in the SLR, 18 correspond to systematic reviews, which limits the availability of studies with specific details or experiments. This affected the depth of the analysis and the comprehensiveness of the results obtained.\n\nThe literature review reveals a predominant emphasis on the banking sector, especially in relation to credit card fraud and insurance fraud. The narrow focus leads to a lack of diversity in the types of fraud studied, excluding internal fraud types such as embezzlement, racketeering, smurfing, defalcation, collusion, signature forgery, and manipulation of accounting documents, among others. The underrepresentation of these other fraud types compromises the generalization of the findings and the applicability of ML models to contexts beyond the banking sector.\n\nThe datasets analyzed show a significant deficiency in the representation of fraud types. It can be observed that most of these datasets originated from the main stock exchanges and, additionally, the information used to carry out the experiments is old. This scenario indicates the inclusion of non-contemporary fraud types in the analysis. The limited availability of information on the performance metrics of the unsupervised learning models made it difficult to count the evaluation metrics used to predict financial fraud.\n\nThe field of financial fraud detection using ML models offers promising prospects for future research. An area of potential improvement is experimentation with advanced techniques, such as reinforcement learning or deep neural network architectures, to improve the accuracy and efficiency of models, including unsupervised learning. This approach could enable the development of more sophisticated systems capable of identifying complex fraud patterns and dynamically adjusting to the changing strategies of criminals, who are constantly innovating new fraud methods.\n\nMoreover, it is suggested that the applicability of fraud detection systems in contexts other than banking be analyzed by adopting the anomaly approach, which would make it possible to move forward in the detection of fraud in real-time and minimize risks in organizations. It is also proposed that a dataset be created, containing real context information, which is freely accessible and includes new fraud methods to provide the scientific community with an updated dataset.\n\nData availability\n\nThe datasets generated and/or analyzed in this study are available in the Harvard Dataverse repository https://doi.org/10.7910/DVN/CM8NVY.\n\nReferences\n\nAbdallah A, Maarof MA, Zainal A (2016) Fraud detection system: a survey. J Netw Comput Appl 68:90–113. https://doi.org/10.1016/j.jnca.2016.04.007\n\nArticle\n  Google Scholar\n\nAchakzai MAK, Juan P (2022) Using machine learning meta-classifiers to detect financial frauds. Financ Res Lett 48:102915. https://doi.org/10.1016/j.frl.2022.102915\n\nArticle\n  Google Scholar\n\nAhmed M, Mahmood AN, Islam MdR (2016) A survey of anomaly detection techniques in financial domain. Future Gener Comput Syst 55:278–288. https://doi.org/10.1016/j.future.2015.01.001\n\nArticle\n  Google Scholar\n\nAl Ali A, Khedr AM, El-Bannany M, Kanakkayil S (2023) A powerful predicting model for financial statement fraud based on optimized XGBoost ensemble learning technique. Appl Sci 13(4):2272. https://doi.org/10.3390/app13042272\n\nArticle\n  CAS\n  Google Scholar\n\nAlarfaj FK, Malik I, Khan HU, Almusallam N, Ramzan M, Ahmed M (2022) Credit card fraud detection using state-of-the-art machine learning and deep learning algorithms. IEEE Access 10:39700–39715. https://doi.org/10.1109/ACCESS.2022.3166891\n\nArticle\n  Google Scholar\n\nAl-Hashedi KG, Magalingam P (2021) Financial fraud detection applying data mining techniques: a comprehensive review from 2009 to 2019. Comput Sci Rev 40:100402. https://doi.org/10.1016/j.cosrev.2021.100402\n\nArticle\n  Google Scholar\n\nAli A, Abd Razak S, Othman SH, Eisa TAE, Al-Dhaqm A, Nasser Tusneem ME, Elshafie H, Saif A (2022) Financial fraud detection based on machine learning: a systematic literature review. Appl Sci (Switz). https://doi.org/10.3390/app12199637\n\nAlsuwailem AAS, Salem E, Saudagar AKJ (2022) Performance of different machine learning algorithms in detecting financial fraud. Comput Econ. https://doi.org/10.1007/s10614-022-10314-x\n\nAlwadain A, Ali RF, Muneer A (2023) Estimating financial fraud through transaction-level features and machine learning. Mathematics 11(5):1184. https://doi.org/10.3390/math11051184\n\nArticle\n  Google Scholar\n\nAmrutha E, Arivazhagan S, Jebarani WSL (2023) Deep clustering network for steganographer detection using latent features extracted from a novel convolutional autoencoder. Neural Process Lett 55(3):2953–2964. https://doi.org/10.1007/s11063-022-10992-6\n\nArticle\n  Google Scholar\n\nArévalo F, Barucca P, Téllez-León I-E, Rodríguez W, Gage G, Morales R (2022) Identifying clusters of anomalous payments in the salvadorian payment system. Lat Am J Cent Bank. 3(1):100050. https://doi.org/10.1016/j.latcb.2022.100050\n\nArticle\n  Google Scholar\n\nAshfaq T, Khalid R, Yahaya A, Aslam S, Alsafari S, Hameed I (2022) A machine learning and blockchain bases efficient fraud detection mechanism. Sensors 22(19):7162. https://doi.org/10.3390/s22197162\n\nArticle\n  ADS\n  PubMed\n  PubMed Central\n  Google Scholar\n\nAshtiani MN, Raahemi B (2022) Intelligent fraud detection in financial statements using machine learning and data mining: a systematic literature review. IEEE Access 10:72504–72525. https://doi.org/10.1109/ACCESS.2021.3096799\n\nArticle\n  Google Scholar\n\nAslam F, Hunjra A, Ftiti Z, Louhichi W, Shams T (2022) Insurance fraud detection: evidence from artificial intelligence and machine learning. Res Int Bus Financ. https://doi.org/10.1016/j.ribaf.2022.101744\n\nBaghdasaryan V, Davtyan H, Sarikyan A, Navasardyan Z (2022) Improving tax audit efficiency using machine learning: the role of taxpayer’s network data in fraud detection. Appl Artif Intell 36(1). https://doi.org/10.1080/08839514.2021.2012002\n\nBaker MR, Mahmood ZN, Shaker EH (2022) Ensemble learning with supervised machine learning models to predict credit card fraud transactions. Rev Intell Artif. https://doi.org/10.18280/ria.360401\n\nBakumenko A, Elragal A (2022) Detecting anomalies in financial data using machine learning algorithms. Systems. https://doi.org/10.3390/systems10050130\n\nBekirev AS, Klimov VV, Kuzin MV, Shchukin BA (2015) Payment card fraud detection using neural network committee and clustering. Optical Mem. Neural Netw 24(3):193–200. https://doi.org/10.3103/S1060992X15030030\n\nArticle\n  Google Scholar\n\nBenchaji I, Douzi S, Ouahidi BEl (2021) Credit card fraud detection model based on LSTM recurrent neural networks. J Adv Inf Technol 12(2):113–118. https://doi.org/10.12720/jait.12.2.113-118\n\nArticle\n  Google Scholar\n\nBłaszczyński J, de Almeida Filho AT, Matuszyk A, Szeląg M, Słowiński R (2021) Auto loan fraud detection using dominance-based rough set approach versus machine learning methods. Expert Syst Appl 163:113740. https://doi.org/10.1016/j.eswa.2020.113740\n\nArticle\n  Google Scholar\n\nBolgorian M, Mayeli A, Ronizi NG (2023) CEO compensation and money laundering risk. J Econ Criminol 1:100007. https://doi.org/10.1016/j.jeconc.2023.100007\n\nArticle\n  Google Scholar\n\nChen S (2016) Detection of fraudulent financial statements using the hybrid data mining approach. SpringerPlus 5(1):89. https://doi.org/10.1186/s40064-016-1707-6\n\nArticle\n  PubMed\n  PubMed Central\n  Google Scholar\n\nChen S, Goo Y-JJ, Shen Z-D (2014) A hybrid approach of stepwise regression, logistic regression, support vector machine, and decision tree for forecasting fraudulent financial statements. Sci World J 2014:1–9. https://doi.org/10.1155/2014/968712\n\nArticle\n  Google Scholar\n\nChen Y, Wu Z (2022) Financial fraud detection of listed companies in China: a machine learning approach. Sustainability 15(1):105. https://doi.org/10.3390/su15010105\n\nArticle\n  Google Scholar\n\nChullamonthon P, Tangamchit P (2023) Ensemble of supervised and unsupervised deep neural networks for stock price manipulation detection. Expert Syst Appl 220:119698. https://doi.org/10.1016/j.eswa.2023.119698\n\nArticle\n  Google Scholar\n\nCompustat (2022) Compustat. S&P Global Market Intelligence. https://www.marketplace.spglobal.com/en/datasets?cq_cmp=9778467255&cq_plac=&cq_net=g&cq_pos=&cq_plt=gp&utm_source=google&utm_medium=cpc&utm_campaign=DMS_Marketplace_Search_Google&utm_term=&utm_content=586436401424&_bt=586436401424&_bk=&_bm=&_bn=g&_bg=133704002389&gclid=Cj0KCQjw4s-kBhDqARIsAN-ipH3TguUoVohfDZgD65fjvKomc6BBgJ3uA9zP95m6u4vOs5yG7_L7w2UaAnnvEALw_wcB\n\nCSMAR (2022) China Stock Market & Accounting Research (CSMAR). Wharton University of Pennsylvania. https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/china-stock-market-accounting-research-csmar/\n\nDalal S, Seth B, Radulescu M, Secara C, Tolea C (2022) Predicting fraud in financial payment services through optimized hyper-parameter-tuned XGBoost model. Mathematics 10(24):4679. https://doi.org/10.3390/math10244679\n\nArticle\n  Google Scholar\n\nDantas RM, Firdaus R, Jaleel F, Neves Mata P, Mata MN, Li G (2022) Systemic acquired critique of credit card deception exposure through machine learning. J Open Innov: Technol Mark Complex 8(4):192. https://doi.org/10.3390/joitmc8040192\n\nArticle\n  Google Scholar\n\nDomashova J, Kripak E (2021) Identification of non-typical international transactions on bank cards of individuals using machine learning methods. Procedia Comput Sci 190:178–183. https://doi.org/10.1016/j.procs.2021.06.023\n\nArticle\n  Google Scholar\n\nDomashova J, Kripak E (2022) Development of a generalized algorithm for identifying atypical bank transactions using machine learning methods. Procedia Comput Sci 213:101–109. https://doi.org/10.1016/j.procs.2022.11.044\n\nArticle\n  Google Scholar\n\nDutta I, Dutta S, Raahemi B (2017) Detecting financial restatements using data mining techniques. Expert Syst Appl 90:374–393. https://doi.org/10.1016/j.eswa.2017.08.030\n\nArticle\n  Google Scholar\n\nElshaar S, Sadaoui S (2020) Semi-supervised Classification of Fraud Data in Commercial Auctions. Appl Artif Intell 34(1):47–63. https://doi.org/10.1080/08839514.2019.1691341\n\nArticle\n  Google Scholar\n\nEsenogho E, Mienye ID, Swart TG, Aruleba K, Obaido G (2022) A neural network ensemble with feature engineering for improved credit card fraud detection. IEEE Access 10:16400–16407. https://doi.org/10.1109/ACCESS.2022.3148298\n\nArticle\n  Google Scholar\n\nEshghi A, Kargari M (2019) Introducing a new method for the fusion of fraud evidence in banking transactions with regards to uncertainty. Expert Syst Appl 121:382–392. https://doi.org/10.1016/j.eswa.2018.11.039\n\nArticle\n  Google Scholar\n\nEstupiñán Gaitán R (2015) Control interno y fraudes: análisis de informe COSO I, II y III con base en los ciclos transaccionales, Tercera edición (Niebel BW (ed)). Ecoe Ediciones\n\nFanai H, Abbasimehr H (2023) A novel combined approach based on deep autoencoder and deep classifiers for credit card fraud detection. Expert Syst Appl 217:119562. https://doi.org/10.1016/j.eswa.2023.119562\n\nArticle\n  Google Scholar\n\nFang Y, Zhang Y, Huang C (2019) Credit card fraud detection based on machine learning. Comput Mater Contin 61(1):185–195. https://doi.org/10.32604/cmc.2019.06144\n\nArticle\n  Google Scholar\n\nFemila Roseline J, Naidu G, Samuthira Pandi V, Alamelu alias Rajasree S, Mageswari N (2022) Autonomous credit card fraud detection using machine learning approach✰. Comput Electr Eng 102:108132. https://doi.org/10.1016/j.compeleceng.2022.108132\n\nArticle\n  Google Scholar\n\nGarcía-Ordás MT, Alaiz-Moretón H, Casteleiro-Roca J-L, Jove E, Benítez-Andrades JA, García-Rodríguez I, Quintián H, Calvo-Rolle JL (2023) Clustering techniques selection for a hybrid regression model: a case study based on a solar thermal system. Cybern Syst 54(3):286–305. https://doi.org/10.1080/01969722.2022.2030006\n\nArticle\n  Google Scholar\n\nGupta S, Mehta SK (2021) Data mining-based financial statement fraud detection: systematic literature review and meta-analysis to estimate data sample mapping of fraudulent companies against non-fraudulent companies. Global Bus Rev https://doi.org/10.1177/0972150920984857\n\nHajek P, Henriques R (2017) Mining corporate annual reports for intelligent detection of financial statement fraud—a comparative study of machine learning methods. Knowl-Based Syst 128:139–152. https://doi.org/10.1016/j.knosys.2017.05.001\n\nArticle\n  Google Scholar\n\nHamza C, Lylia A, Nadine C, Nicolas C (2023) Semi-supervised method to detect fraudulent transactions and identify fraud types while minimizing mounting costs. Int J Adv Comput Sci Appl 14(2). https://doi.org/10.14569/IJACSA.2023.0140298\n\nHilal W, Gadsden SA, Yawney J (2022) Financial fraud: a review of anomaly detection techniques and recent advances. Expert Syst Appl 193:116429. https://doi.org/10.1016/j.eswa.2021.116429\n\nArticle\n  Google Scholar\n\nHofmann H (1994) Statlog (German credit data). UCI Machine Learning Repository. https://doi.org/10.24432/C5NC77\n\nHuang D, Mu D, Yang L, Cai X (2018) CoDetect: financial fraud detection with anomaly feature detection. IEEE Access 6:19161–19174. https://doi.org/10.1109/ACCESS.2018.2816564\n\nArticle\n  Google Scholar\n\nHwang J, Kim K (2020) An efficient domain-adaptation method using GAN for fraud detection. Int J Adv Comput Sci Appl 11(11). https://doi.org/10.14569/IJACSA.2020.0111113\n\nIleberi E, Sun Y, Wang Z (2021) Performance evaluation of machine learning methods for credit card fraud detection using SMOTE and AdaBoost. IEEE Access 9:165286–165294. https://doi.org/10.1109/ACCESS.2021.3134330\n\nArticle\n  Google Scholar\n\nIleberi E, Sun Y, Wang Z (2022) A machine learning based credit card fraud detection using the GA algorithm for feature selection. J Big Data 9(1):24. https://doi.org/10.1186/s40537-022-00573-8\n\nArticle\n  Google Scholar\n\nKhan S, Alourani A, Mishra B, Ali A, Kamal M (2022) Developing a credit card fraud detection model using machine learning approaches. Int J Adv Comput Sci Appl 13(3). https://doi.org/10.14569/IJACSA.2022.0130350\n\nKim J, Kim H-J, Kim H (2019) Fraud detection for job placement using hierarchical clusters-based deep neural networks. Appl Intell 49(8):2842–2861. https://doi.org/10.1007/s10489-019-01419-2\n\nArticle\n  Google Scholar\n\nKim YJ, Baik B, Cho S (2016) Detecting financial misstatements with fraud intention using multi-class cost-sensitive learning. Expert Syst Appl 62:32–43. https://doi.org/10.1016/j.eswa.2016.06.016\n\nArticle\n  Google Scholar\n\nKitchenham B, Brereton P (2013) A systematic review of systematic review process research in software engineering. Inf Softw Technol 55(12):2049–2075. https://doi.org/10.1016/j.infsof.2013.07.010\n\nArticle\n  Google Scholar\n\nKitchenham B, Stuart C (2007) Guidelines for performing systematic literature reviews in software engineering. https://www.researchgate.net/publication/302924724_Guidelines_for_performing_Systematic_Literature_Reviews_in_Software_Engineering\n\nKootanaee AJ, Aghajan AAP, Shirvani MH (2021) A hybrid model based on machine learning and genetic algorithm for detecting fraud in financial statements. J Optim Ind Eng 14(2):183–201. https://doi.org/10.22094/JOIE.2020.1877455.1685\n\nArticle\n  Google Scholar\n\nKPMG (2022) Una triple amenaza en las Américas. KMPG. https://kpmg.com/co/es/home/insights/2022/01/kpmg-fraud-outlook-survey.html\n\nKumar S, Ahmed R, Bharany S, Shuaib M, Ahmad T, Tag Eldin E, Rehman AU, Shafiq M (2022) Exploitation of machine learning algorithms for detecting financial crimes based on customers’ behavior. Sustainability 14(21):13875. https://doi.org/10.3390/su142113875\n\nArticle\n  Google Scholar\n\nKumbure MM, Lohrmann C, Luukka P, Porras J (2022) Machine learning techniques and data for stock market forecasting: a literature review. Expert Syst Appl 197:116659. https://doi.org/10.1016/j.eswa.2022.116659\n\nArticle\n  Google Scholar\n\nLee H, Choi E, Kim I, Choi D, Go W, Lee K, Yim H, Lee T (2018) Feature selection practice for unsupervised learning of credit card fraud detection. J Theor Appl Inf Technol 96(2):408–417\n\nGoogle Scholar\n\nLei X, Mohamad UH, Sarlan A, Shutaywi M, Daradkeh YI, Mohammed HO (2022) Development of an intelligent information system for financial analysis depend on supervised machine learning algorithms. Inf Process Manag 59(5):103036. https://doi.org/10.1016/j.ipm.2022.103036\n\nArticle\n  Google Scholar\n\nLokanan M, Tran V, Vuong NH (2019) Detecting anomalies in financial statements using machine learning algorithm. Asian J Account Res 4(2):181–201. https://doi.org/10.1108/AJAR-09-2018-0032\n\nArticle\n  Google Scholar\n\nLokanan ME, Sharma K (2022) Fraud prediction using machine learning: The case of investment advisors in Canada. Mach Learn Appl 8:100269. https://doi.org/10.1016/j.mlwa.2022.100269\n\nArticle\n  Google Scholar\n\nLokanan ME (2022) Predicting money laundering using machine learning and artificial neural networks algorithms in banks. J Appl Secur Res 1–25. https://doi.org/10.1080/19361610.2022.2114744\n\nLópez-Rojas E (2017) Synthetic financial datasets for fraud detection. Kaggle. https://www.kaggle.com/datasets/ealaxi/paysim1\n\nMachine Learning Group (2018) Credit card fraud detection. Kaggle. https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n\nMadhurya MJ, Gururaj HL, Soundarya BC, Vidyashree KP, Rajendra AB (2022) Exploratory analysis of credit card fraud detection using machine learning techniques. Glob Transit Proc 3(1):31–37. https://doi.org/10.1016/j.gltp.2022.04.006\n\nArticle\n  Google Scholar\n\nMalik EF, Khaw KW, Belaton B, Wong WP, Chew X (2022) Credit card fraud detection using a new hybrid machine learning architecture. Mathematics 10(9):1480. https://doi.org/10.3390/math10091480\n\nArticle\n  Google Scholar\n\nMárquez Arcila RH (2019) Auditoría forense. Ecoe Ediciones\n\nMisra S, Thakur S, Ghosh M, Saha SK (2020) An autoencoder based model for detecting fraudulent credit card transaction. Procedia Comput Sci 167:254–262. https://doi.org/10.1016/j.procs.2020.03.219\n\nArticle\n  Google Scholar\n\nMoher D, Liberati A, Tetzlaff J, Altman DG (2009) Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement. PLoS Med 6(7):e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nArticle\n  PubMed\n  PubMed Central\n  Google Scholar\n\nMongwe W, Malan K (2020) A survey of automated financial statement fraud detection with relevance to the South African context. S Afr Comput J 32(1). https://doi.org/10.18489/sacj.v32i1.777\n\nMontes Salazar CA (2019) Riesgos de fraude en una auditoría de estados financieros (1.a ed.). Alfaomega. ISBN: 9789587782639. https://www.alfaomegacloud.com/reader/riesgos-de-fraude-en-una-auditoria-de-estados-financieros?location=3\n\nMoreira MÂL, Junior C, de SR, Silva DF, de L, de Castro Junior MAP, Costa IP, de A, Gomes CFS, dos Santos M (2022) Exploratory analysis and implementation of machine learning techniques for predictive assessment of fraud in banking systems. Procedia Comput Sci 214:117–124. https://doi.org/10.1016/j.procs.2022.11.156\n\nArticle\n  Google Scholar\n\nNarsimha B, Raghavendran CV, Rajyalakshmi P, Reddy GK, Bhargavi M, Naresh P (2022) Cyber defense in the age of artificial intelligence and machine learning for financial fraud detection application. Int J Electr Electron Res 10(2):87–92. https://doi.org/10.37391/ijeer.100206\n\nArticle\n  Google Scholar\n\nNian K, Zhang H, Tayal A, Coleman T, Li Y (2016) Auto insurance fraud detection using unsupervised spectral ranking for anomaly. J Financ Data Sci 2(1):58–75. https://doi.org/10.1016/j.jfds.2016.03.001\n\nArticle\n  Google Scholar\n\nNicholls J, Kuppa A, Le-Khac N-A (2021) Financial cybercrime: a comprehensive survey of deep learning approaches to tackle the evolving financial crime landscape. IEEE Access 9:163965–163986. https://doi.org/10.1109/ACCESS.2021.3134076\n\nArticle\n  Google Scholar\n\nNonnenmacher J, Marx Gómez J (2021) Unsupervised anomaly detection for internal auditing: Literature review and research agenda. Int J Digit Account Res 1–22. https://doi.org/10.4192/1577-8517-v21_1\n\nOlszewski D (2014) Fraud detection using self-organizing map visualizing the user profiles. Knowl Based Syst 70:324–334. https://doi.org/10.1016/j.knosys.2014.07.008\n\nArticle\n  Google Scholar\n\nOmershafiq (2019) Bitcoin network transactional metadata. Kaggle. https://www.kaggle.com/datasets/omershafiq/bitcoin-network-transactional-metadata\n\nOunacer S, Ait El Bour H, Oubrahim Y, Ghoumari MY, Azzouazi M (2018) Using isolation forest in anomaly detection: the case of credit card transactions. Period Eng Nat Sci 6(2):394. https://doi.org/10.21533/pen.v6i2.533\n\nArticle\n  Google Scholar\n\nPalacio SM (2019) Abnormal pattern prediction: detecting fraudulent insurance property claims with semi-supervised machine-learning. Data Sci J 18(1):35. https://doi.org/10.5334/dsj-2019-035\n\nArticle\n  Google Scholar\n\nPapík M, Papíková L (2022) Detecting accounting fraud in companies reporting under US GAAP through data mining. Int J Account Inf Syst 45:100559. https://doi.org/10.1016/j.accinf.2022.100559\n\nArticle\n  Google Scholar\n\nPlakandaras V, Gogas P, Papadimitriou T, Tsamardinos I (2022) Credit card fraud detection with automated machine learning systems. Appl Artif Intell 36(1). https://doi.org/10.1080/08839514.2022.2086354\n\nPolak P, Nelischer C, Guo H, Robertson DC (2020) Intelligent” finance and treasury management: what we can expect. AI Soc 35(3):715–726. https://doi.org/10.1007/s00146-019-00919-6\n\nArticle\n  Google Scholar\n\nPricewaterhouseCoopers (2022) Encuesta Global de Crimen y Fraude Económico de PwC Colombia 2022–2023. https://www.pwc.com/co/es/publicaciones/encuesta-crimen-fraude-economico.html\n\nPumsirirat A, Yan L (2018) Credit card fraud detection using deep learning based on auto-encoder and restricted Boltzmann machine. Int J Adv Comput Sci Appl 9(1). https://doi.org/10.14569/IJACSA.2018.090103\n\nPutten P (2000) Insurance Company Benchmark (COIL 2000). UCI Machine Learning Repository. https://doi.org/10.24432/C5630S\n\nQuinlan R (1997) Statlog (Australian credit approval). UCI Machine Learning Repository. https://doi.org/10.24432/C59012\n\nRakowski R, Polak P, Kowalikova P (2021) Ethical aspects of the impact of AI: the status of humans in the era of artificial intelligence. Society 58(3):196–203. https://doi.org/10.1007/s12115-021-00586-8\n\nArticle\n  Google Scholar\n\nRamírez-Alpízar A, Jenkins M, Martínez A, Quesada-López C (2020a) Use of data mining and machine learning techniques for fraud detection in financial statements: a systematic mapping study. Rev Ibér Sist Tecnol Inf Lousada No. E28:97–109\n\nReurink A (2018) Financial fraud: a literature review. J Econ Surv 32(5):1292–1325. https://doi.org/10.1111/joes.12294\n\nArticle\n  Google Scholar\n\nRocha-Salazar J-J, Segovia-Vargas M-J, Camacho-Miñano M-M (2021) Money laundering and terrorism financing detection using neural networks and an abnormality indicator. Expert Syst Appl 169:114470. https://doi.org/10.1016/j.eswa.2020.114470\n\nArticle\n  Google Scholar\n\nRoehrs A, da Costa CA, Righi R, da R, de Oliveira KSF (2017) Personal health records: a systematic literature review. J Med Internet Res 19(1):e13. https://doi.org/10.2196/jmir.5876\n\nArticle\n  PubMed\n  PubMed Central\n  Google Scholar\n\nRubio J, Barucca P, Gage G, Arroyo J, Morales-Resendiz R (2020) Classifying payment patterns with artificial neural networks: an autoencoder approach. Lat Am J Cent Bank 1(1–4):100013. https://doi.org/10.1016/j.latcb.2020.100013\n\nArticle\n  Google Scholar\n\nSahin Y, Bulkan S, Duman E (2013) A cost-sensitive decision tree approach for fraud detection. Expert Syst Appl 40(15):5916–5923. https://doi.org/10.1016/j.eswa.2013.05.021\n\nArticle\n  Google Scholar\n\nSaputra M, Santosa PI, Permanasari AE (2023) Consumer behaviour and acceptance in fintech adoption: a systematic literature review. Acta Inform Pragensia 12(2):468–489. https://doi.org/10.18267/j.aip.222\n\nArticle\n  Google Scholar\n\nSaragih MG, Chin J, Setyawasih R, Nguyen PT, Shankar K (2019) Machine learning methods for analysis fraud credit card transaction. Int J Eng Adv Technol 8(6S):870–874. https://doi.org/10.35940/ijeat.F1164.0886S19\n\nArticle\n  Google Scholar\n\nSathya M, Balakumar B (2022) Insurance fraud detection using novel machine learning technique. Int J Intell Syst Appl Eng 10(3):374–381\n\nGoogle Scholar\n\nSavić M, Atanasijević J, Jakovetić D, Krejić N (2022) Tax evasion risk management using a hybrid unsupervised outlier detection method. Expert Syst Appl 193:116409. https://doi.org/10.1016/j.eswa.2021.116409\n\nArticle\n  Google Scholar\n\nSeera M, Lim CP, Kumar A, Dhamotharan L, Tan KH (2021) An intelligent payment card fraud detection system. Ann Oper Res. https://doi.org/10.1007/s10479-021-04149-2\n\nShahana T, Lavanya V, Bhat AR (2023) State of the art in financial statement fraud detection: a systematic review. Technol Forecast Soc Change 192:122527. https://doi.org/10.1016/j.techfore.2023.122527\n\nArticle\n  Google Scholar\n\nShou M, Bao X, Yu J (2023) An optimal weighted machine learning model for detecting financial fraud. Appl Econ Lett 30(4):410–415. https://doi.org/10.1080/13504851.2021.1989367\n\nArticle\n  Google Scholar\n\nSingh A, Jain A, Biable SE (2022) Financial fraud detection approach based on firefly optimization algorithm and support vector machine. Appl Comput Intell Soft Comput 2022:1–10. https://doi.org/10.1155/2022/1468015\n\nArticle\n  Google Scholar\n\nSmith Q-J, Valverde R (2021) A perceptron based neural network data analytics architecture for the detection of fraud in credit card transactions in financial legacy systems. WSEAS Trans Syst Control 16:358–374. https://doi.org/10.37394/23203.2021.16.31\n\nArticle\n  Google Scholar\n\nSofy MA, Khafagy MH, Badry RM (2023) An intelligent Arabic model for recruitment fraud detection using machine learning. J Adv Informat Technol. https://doi.org/10.12720/jait.14.1.102-111\n\nSrokosz M, Bobyk A, Ksiezopolski B, Wydra M (2023) Machine-learning-based scoring system for antifraud CISIRTs in banking environment. Electronics 12(1):251. https://doi.org/10.3390/electronics12010251\n\nArticle\n  Google Scholar\n\nSubudhi S, Panigrahi S (2020) Use of optimized fuzzy C-Means clustering and supervised classifiers for automobile insurance fraud detection. J King Saud Univ— Comput Inf Sci 32(5):568–575. https://doi.org/10.1016/j.jksuci.2017.09.010\n\nArticle\n  Google Scholar\n\nTi Y-W, Hsin Y-Y, Dai T-S, Huang M-C, Liu L-C (2022) Feature generation and contribution comparison for electronic fraud detection. Sci Rep 12(1):18042. https://doi.org/10.1038/s41598-022-22130-2\n\nArticle\n  ADS\n  CAS\n  PubMed\n  PubMed Central\n  Google Scholar\n\nTingfei H, Guangquan C, Kuihua H (2020) Using variational auto encoding in credit card fraud detection. IEEE Access 8:149841–149853. https://doi.org/10.1109/ACCESS.2020.3015600\n\nArticle\n  Google Scholar\n\nTorrano C, Recuero P, Ramirez F, Hernández S, Torres J (2018) Machine learning aplicado a la ciberseguridad: técnicas y ejemplos en detección de amenazas. Zeroxword Computing\n\nUdeze CL, Eteng IE, Ibor AE (2022) Application of machine learning and resampling techniques to credit card fraud detection. J Niger Soc Phys Sci 769. https://doi.org/10.46481/jnsps.2022.769\n\nUsman A, Naveed N, Munawar S (2023) Intelligent anti-money laundering fraud control using graph-based machine learning model for the financial domain. J Cases Inf Technol 25(1):1–20. https://doi.org/10.4018/JCIT.316665\n\nArticle\n  Google Scholar\n\nVan Capelleveen G, Poel M, Mueller RM, Thornton D, Van Hillegersberg J (2016) Outlier detection in healthcare fraud: a case study in the Medicaid dental domain. Int J Account Inf Syst 21:18–31. https://doi.org/10.1016/j.accinf.2016.04.001\n\nArticle\n  Google Scholar\n\nVanhoeyveld J, Martens D, Peeters B (2020) Value-added tax fraud detection with scalable anomaly detection techniques. Appl Soft Comput 86:105895. https://doi.org/10.1016/j.asoc.2019.105895\n\nArticle\n  Google Scholar\n\nVanini P, Rossi S, Zvizdic E, Domenig T (2023) Online payment fraud: from anomaly detection to risk management. Financ Innov 9(1):66. https://doi.org/10.1186/s40854-023-00470-w\n\nArticle\n  Google Scholar\n\nVanneschi L, Horn DM, Castelli M, Popovič A (2018) An artificial intelligence system for predicting customer default in e-commerce. Expert Syst Appl 104:1–21. https://doi.org/10.1016/j.eswa.2018.03.025\n\nArticle\n  Google Scholar\n\nViera J, Aguilar J, Rodríguez-Moreno M, Quintero-Gull C (2023) Analysis of the behavior pattern of energy consumption through online clustering techniques. Energies 16(4):1649. https://doi.org/10.3390/en16041649\n\nArticle\n  Google Scholar\n\nWadhwa VK, Saini AK, Kumar SS (2020) Financial fraud prediction models: a review of research evidence. Int J Sci Technol Res 9(1):677–680\n\nGoogle Scholar\n\nWest J, Bhattacharya M (2016) Intelligent financial fraud detection: a comprehensive review. Comput Secur 57:47–66. https://doi.org/10.1016/j.cose.2015.09.005\n\nArticle\n  Google Scholar\n\nWhiting DG, Hansen JV, McDonald JB, Albrecht C, Albrecht WS (2012) Machine learning methods for detecting patterns of management fraud. Comput Intell 28(4):505–527. https://doi.org/10.1111/j.1467-8640.2012.00425.x\n\nArticle\n  MathSciNet\n  Google Scholar\n\nWohlin C (2014) Guidelines for snowballing in systematic literature studies and a replication in software engineering. In: Proceedings of the 18th international conference on evaluation and assessment in software engineering. pp. 1–10\n\nWu B, Lv X, Alghamdi A, Abosaq H, Alrizq M (2023) Advancement of management information system for discovering fraud in master card based intelligent supervised machine learning and deep learning during SARS-CoV2. Inf Process Manag 60(2):103231. https://doi.org/10.1016/j.ipm.2022.103231\n\nArticle\n  PubMed\n  Google Scholar\n\nXiong T, Ma Z, Li Z, Dai J (2022) The analysis of influence mechanism for internet financial fraud identification and user behavior based on machine learning approaches. Int J Syst Assur Eng Manag 13(S3):996–1007. https://doi.org/10.1007/s13198-021-01181-0\n\nArticle\n  Google Scholar\n\nXiuguo W, Shengyong D (2022) An analysis on financial statement fraud detection for Chinese listed companies using deep learning. IEEE Access 10:22516–22532. https://doi.org/10.1109/ACCESS.2022.3153478\n\nArticle\n  Google Scholar\n\nYeh I-C (2016) Default of credit card clients. UCI Machine Learning Repository. https://doi.org/10.24432/C55S3H\n\nZhang Z, Zhou X, Zhang X, Wang L, Wang P (2018) A model based on convolutional neural network for online transaction fraud detection. Secur Commun. Netw. 2018:1–9. https://doi.org/10.1155/2018/5680264\n\nArticle\n  Google Scholar\n\nZhao Z, Bai T (2022) Financial fraud detection and prediction in listed companies using SMOTE and machine learning algorithms. Entropy 24(8):1157. https://doi.org/10.3390/e24081157\n\nArticle\n  ADS\n  PubMed\n  PubMed Central\n  Google Scholar\n\nZhou H, Chai H, Qiu M (2018) Fraud detection within bankcard enrollment on mobile device based payment using machine learning. Front Inf Technol Electron Eng 19(12):1537–1545. https://doi.org/10.1631/FITEE.1800580\n\nArticle\n  Google Scholar\n\nZupan M, Budimir V, Letinic S (2020) Journal entry anomaly detection model. Intell Syst Account Financ Manag 27(4):197–209. https://doi.org/10.1002/isaf.1485\n\nArticle\n  Google Scholar\n\nDownload references\n\nAcknowledgements\n\nWe would like to express our gratitude to the Universidad Cooperativa de Colombia, Ibagué campus, Espinal. This research work was supported by Universidad Cooperativa de Colombia and derived from research project INV3456 entitled “Detection of anomalies in financial data in social economy organizations through machine learning techniques” associated with the PLANAUDI, AQUA and SINERGIA UCC group, from the Research Center of the Public Accounting and Systems Engineering program of the UCC Ibagué campus.\n\nAuthor information\n\nAuthors and Affiliations\n\nSchool of Public Accounting, Universidad Cooperativa de Colombia, 730001, Ibagué-Espinal campus, Ibagué, Colombia\n\nLudivia Hernandez Aros & John Johver Moreno Hernandez\n\nSchool of Systems Engineering, Universidad Cooperativa de Colombia, 730001, Ibagué-Espinal campus, Ibagué, Colombia\n\nLuisa Ximena Bustamante Molano & Fernando Gutierrez-Portela\n\nSchool of Business Administration, Universidad Cooperativa de Colombia, 730001, Ibagué-Espinal campus, Ibagué, Colombia\n\nMario Samuel Rodríguez Barrero\n\nContributions\n\nAll authors contributed to the creation and design of the study.\n\nCorresponding author\n\nCorrespondence to Ludivia Hernandez Aros.\n\nEthics declarations\n\nCompeting interests\n\nThe authors declare no competing interests.\n\nEthical approval and consent to participate\n\nThe authors declare that they have no human participants, human data, or human tissue.\n\nConsent to publish\n\nThe authors have no data from any individual person on any form.\n\nAdditional information\n\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nRights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nHernandez Aros, L., Bustamante Molano, L.X., Gutierrez-Portela, F. et al. Financial fraud detection through the application of machine learning techniques: a literature review. Humanit Soc Sci Commun 11, 1130 (2024). https://doi.org/10.1057/s41599-024-03606-0\n\nDownload citation\n\nReceived\n15 November 2023\n\nAccepted\n13 August 2024\n\nPublished\n03 September 2024\n\nDOI\nhttps://doi.org/10.1057/s41599-024-03606-0\n\nShare this article\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nSubjects\n\nHumanities and Social Sciences Communications (Humanit Soc Sci Commun)\n\nISSN 2662-9992 (online)\n\nAbout Nature Portfolio\n\nDiscover content\n\nPublishing policies\n\nAuthor & Researcher services\n\nLibraries & institutions\n\nAdvertising & partnerships\n\nProfessional development\n\nRegional websites\n\n© 2025 Springer Nature Limited\n\nFraud Detection: The Balance Between Accuracy and Efficiency in ML : \nFraud Detection: The Balance Between Accuracy and Efficiency in ML\n\nIntroduction\n\nFraud detection is a critical area in today's financial ecosystem, where the prowess of technology has increasingly become a double-edged sword. With the advent of digital transactions and the increase in online activities, the incidence of fraudulent behaviors has surged. The traditional methods of combating fraud have gradually shifted toward machine learning (ML) techniques that promise greater accuracy and efficiency in detecting anomalies and suspicious activities. However, striking the right balance between accuracy and efficiency remains a significant challenge for organizations relying on these sophisticated technologies.\n\nThis article aims to explore the nuances of fraud detection through machine learning, focusing on the pivotal aspects of accuracy and efficiency. We will delve into the various machine learning algorithms used in fraud detection, how they function, the trade-offs organizations face, and the importance of optimizing both parameters to foster a reliable detection system.\n\nUnderstanding Fraud Detection and Its Importance\n\nFraud detection involves identifying deceptive practices intended to secure unauthorized benefits or financial gains. As technology evolves, so do the tactics employed by fraudsters. This evolution necessitates a robust system that can detect and prevent fraudulent activities in real-time.\n\nThe increasing frequency of breaches, especially in sensitive areas such as financial services, insurance, and e-commerce, underscores the urgency for efficient fraud detection systems. A successful fraud detection strategy can minimize losses significantly, protect customer information, and maintain the integrity of the business. Additionally, the legal ramifications associated with fraud imply that organizations must invest in effective systems to avoid penalties and damage to their reputation.\n\nOrganizations are leveraging machine learning as a means to enhance their fraud detection capabilities. By employing predictive analytics and anomaly detection techniques, they can identify patterns indicative of fraudulent behavior. However, the reliance on machine learning brings forth complexities surrounding the balance between accuracy—correctly identifying fraud—and efficiency—processing transactions swiftly to maintain a seamless customer experience.\n\nThe Role of Machine Learning in Fraud Detection\n\nMachine learning employs algorithms that learn from historical data to make predictions about future outcomes. In fraud detection, these algorithms can analyze vast amounts of transaction data to identify unusual patterns that may indicate fraud. For instance, supervised learning models such as logistic regression, decision trees, and random forests are commonly used to classify transactions as either legitimate or fraudulent based on labeled training data.\n\nSupervised Learning Techniques\n\nSupervised learning techniques require labeled datasets, wherein the characteristics of both fraudulent and non-fraudulent transactions are known. This wealth of information is used to train models that can generalize to new, unseen data. One common method involves building a decision tree, which breaks down complex decisions into simpler, manageable parts, allowing organizations to assess various transaction features that may indicate fraud.\n\nThe advantage of using supervised learning is that it typically provides high accuracy rates due to the specificity of the labeled data, enhancing the algorithm's capacity to detect fraud effectively. However, it also presents challenges, such as the need for extensive, high-quality labeled data and the potential for overfitting—where an algorithm becomes too specific to the training data, failing to perform well on new cases.\n\nUnsupervised Learning Techniques\n\nGiven the limitations of labeled data, many organizations are turning to unsupervised learning techniques. In unsupervised learning, algorithms analyze datasets without pre-existing labels, searching for inherent structures and patterns. Methods such as clustering and anomaly detection allow for the identification of transactions that deviate significantly from the norm.\n\nFor instance, techniques like k-means clustering can group similar transactions together, shedding light on behavioral patterns that may arise during normal operations. Transactions that fall outside these established clusters may be flagged for further investigation. While unsupervised methods excel in discovering novel fraud techniques, they often come with a trade-off in accuracy since the models may not always correctly identify fraudulent transactions, leading to high rates of false positives—transactions wrongly flagged as fraudulent.\n\nThe Trade-offs Between Accuracy and Efficiency\n\nStriking a balance between accuracy and efficiency in fraud detection is a multifaceted challenge. While high accuracy is essential in minimizing financial losses, efficiency is equally crucial to ensure a smooth customer experience and reduce operational costs. In fraud detection contexts, this translates to speed of detection and the ability to process large volumes of transactions in real-time.\n\nFalse Positives and Their Impact\n\nOne of the most significant issues arises from false positives. If a fraud detection system erroneously flags legitimate transactions as fraudulent, it can lead to dissatisfied customers and potential revenue loss. This not only affects customer retention but can also tarnish the brand's image. Companies must invest effort into refining their algorithms to reduce false positives while maintaining high detection rates for actual fraud.\n\nAdditionally, businesses need to be mindful of the consequences of false negatives, where fraudulent activities go undetected. High false negative rates can expose organizations to financial losses, legal repercussions, and loss of trust among customers. Therefore, the challenge becomes creating a model that can efficiently strike a balance—catching as much fraud as possible while minimizing the number of legitimate transactions incorrectly flagged.\n\nReal-Time Transaction Analysis\n\nEfficiency in fraud detection does not solely hinge on how accurately a model distinguishes between fraudulent and legitimate transactions; it also involves the capability to analyze transactions in real-time. Customers today expect instant responses, particularly in financial transactions. Long processing times can result in abandoned purchases, lost revenue, and a deteriorating customer experience.\n\nAs a result, organizations are implementing streaming analytics and real-time scoring of transactions to enhance efficiency. The ability to analyze transactions and make decisions in real-time relies on the model's design and the underlying technology. However, improving speed can lead to compromises in accuracy if models are not well-optimized. This paradox necessitates continuous refinement of machine learning models to ensure that they remain effective as they process increasing volumes of transactional data.\n\nStrategies for Optimizing Accuracy and Efficiency\n\nOrganizations can combat the challenges inherent in balancing accuracy with efficiency through several strategies tailored to their unique needs. The following methodologies are instrumental in achieving optimal outcomes in fraud detection systems.\n\nEnsemble Learning Approaches\n\nOne effective strategy for improving accuracy is the utilization of ensemble learning methods. This technique combines multiple algorithms to create a more powerful predictive model. By aggregating predictions from various algorithms—such as decision trees, support vector machines, or gradient boosting techniques—organizations can benefit from a more holistic view of potential fraudulent behavior.\n\nEnsemble methods help reduce the biases that individual models may exhibit, thereby enhancing the overall accuracy of fraud detection systems. They can also serve to increase robustness against false positives and false negatives. However, the challenge with ensemble learning is that it can introduce complexity, making it necessary for organizations to ensure appropriate computational resources are available to maintain efficiency.\n\nContinuous Model Training and Evaluation\n\nThe dynamics of fraudulent behavior are constantly evolving, making it essential for machine learning models to evolve as well. Organizations should adopt a proactive approach by implementing continuous model training and evaluation. This ensures that fraud detection systems remain current with emerging trends and shifts in fraudulent tactics.\n\nBy utilizing techniques like incremental learning, organizations can update models in real-time as new data arrives. Moreover, establishing a feedback loop that assesses the effectiveness of detection methods in identifying real-world fraud can inform adjustments needed to optimize accuracy and efficiency further. This practice facilitates a data-driven approach to fraud detection that adapts to changing environments and minimizes operational costs.\n\nConclusion\n\nFraud detection is paramount in today’s fast-paced financial landscape, where digital transactions and online activities are the norm. Machine learning offers powerful tools for tackling fraudulent behavior; however, organizations must carefully navigate the intricate balance between accuracy and efficiency. An effective fraud detection system enhances profitability, fosters customer trust, and protects a company’s reputation.\n\nBoth supervised and unsupervised machine learning techniques have their pros and cons, necessitating the adoption of strategies like ensemble learning and continuous model training to enhance performance. By addressing the challenges of false positives and false negatives while ensuring a seamless operational flow, organizations can optimize their fraud detection capabilities.\n\nIn conclusion, as technology advances and fraud strategies evolve, organizations must remain vigilant and adaptable. Fostering a culture of continuous improvement within fraud detection systems is vital for staying ahead of fraudulent trends. Balancing accuracy and efficiency ultimately determines the success of these advanced systems in mitigating fraud and ensuring a secure financial ecosystem.\n\nIf you want to read more articles similar to Fraud Detection: The Balance Between Accuracy and Efficiency in ML, you can visit the Fraud Detection category.\n\nYou Must Read\n\nFrom Comprehensive Datasets to Realistic Image Generation Models\n\nThe Interplay Between Machine Learning and Ecological Risk Assessment\n\nEnsuring Observability of Machine Learning Models\n\nCategories\n\nRelated Posts\n\nExploring Multi-Modal Approaches in Face Recognition Applications\n\nThe Role of Machine Learning in Modern Anomaly Detection Systems\n\nAI and ML in Augmented Reality: Current Trends and Future Prospects\n\nTerms and Conditions\n\nPrivacy Policy\n\nCookie Policy\n\nAbout Us\n\nContact Us\n"
    },
    "AnalyzedArticles": {
        "Machine learning models employed in Credit Risk Assessment": {
            "Article_Summary": "The systematic review explores machine learning and deep learning techniques in credit risk assessment, highlighting the evolution from traditional statistical methods to advanced AI models. The research emphasizes how models like Random Forest, Neural Networks, and Support Vector Machines can leverage structured and unstructured data to improve credit risk prediction accuracy, while addressing challenges of model transparency, bias, and regulatory compliance.",
            "ML_Models": "Deep Belief Networks, Convolutional Neural Networks, Long Short-Term Memory Networks"
        },
        "Machine learning models employed in Customer Segmentation and Targeting": {
            "Article_Summary": "The articles discuss customer segmentation using machine learning techniques, focusing on how ML models can help businesses understand and categorize customers more effectively. The key emphasis is on using unsupervised learning algorithms to discover hidden patterns in customer data, enabling personalized marketing strategies and improved customer experiences.",
            "ML_Models": "Random Forest Clustering, Gaussian Mixture Models, Support Vector Machines for Segmentation"
        },
        "Machine learning models employed in Financial Performance Prediction": {
            "Article_Summary": "The paper compares nine machine learning models for stock market prediction using Tesla Inc. stock data, focusing on predicting stock market direction. The study introduces a novel 15-minute time interval strategy that improves model performance compared to traditional end-of-day approaches. The research demonstrates that classification metrics alone are insufficient for evaluating ML models in financial prediction, and a comprehensive financial simulation is necessary to assess true model performance.",
            "ML_Models": "Random Forest, Artificial Neural Network (ANN), Logistic Regression"
        },
        "Machine learning models employed in Fraud Detection and Anomaly Detection": {
            "Article_Summary": "The articles discuss the critical role of machine learning (ML) in fraud detection across various industries, highlighting the need for advanced techniques to identify complex fraud patterns. The focus is on balancing accuracy and efficiency in detecting fraudulent activities, with an emphasis on supervised and unsupervised learning approaches.",
            "ML_Models": "Long Short-Term Memory (LSTM), XGBoost, Random Forest (RF)"
        }
    },
    "Relationship": {
        "Machine learning models employed in Credit Risk Assessment": [
            "The ML models (Deep Belief Networks, CNNs, LSTMs) are not ideally suited for the credit risk assessment task with the given data. These are primarily deep learning architectures designed for complex pattern recognition in structured data like images or sequences, while the banking and financial data provided is tabular. Traditional models like Logistic Regression, Random Forest, or Gradient Boosting would be more appropriate for credit risk assessment using these financial indicators and customer attributes."
        ],
        "Machine learning models employed in Customer Segmentation and Targeting": [
            "Customer Segmentation and Targeting models (K-means, Hierarchical Clustering, DBSCAN, Random Forest Clustering, Gaussian Mixture Models, SVMs) can effectively utilize the banking dataset to group customers based on demographic information, financial behaviors, and response patterns. This enables the bank to develop targeted marketing campaigns, personalize product offerings, and improve customer engagement strategies."
        ],
        "Machine learning models employed in Financial Performance Prediction": [
            "The financial performance prediction models (Random Forest, ANN, Logistic Regression) align perfectly with the 'data' table which contains extensive financial indicators and ratios. These models can analyze patterns in financial metrics to predict bankruptcy risk, future performance, and financial health. The banking table provides complementary demographic and economic context that could enhance prediction accuracy."
        ],
        "Machine learning models employed in Fraud Detection and Anomaly Detection": [
            "The fraud and anomaly detection models (Isolation Forest, One-Class SVM, Autoencoders, Ensemble Methods) can leverage both datasets to identify unusual financial patterns. The banking dataset provides transaction-level data while the financial data contains company performance metrics that can reveal accounting irregularities or financial distress signals that might indicate fraudulent activities."
        ]
    },
    "Needs": {
        "Machine learning models employed in Credit Risk Assessment": [
            "For credit risk assessment, we need categorical and numerical features that indicate financial behavior and stability. The banking table provides customer demographics and loan status, while the data table offers financial health indicators. These would typically be used in a binary classification model to predict default probability or bankruptcy risk. The deep learning models mentioned would require significant feature engineering and possibly restructuring the data into sequential format to leverage their strengths in pattern recognition across time periods of financial performance."
        ],
        "Machine learning models employed in Customer Segmentation and Targeting": [
            "For customer segmentation, we need both categorical features (job, marital, education, housing, loan, poutcome) and numerical features (age, campaign, previous, duration). These will be used in unsupervised learning models like K-means and Hierarchical Clustering to identify natural groupings of customers. For targeting models (Random Forest, SVM), we'll use these features to predict the target variable 'y' (whether a customer subscribed to a term deposit), requiring classification algorithms. Categorical variables will need encoding, and numerical features may require normalization or standardization. The end goal is to create distinct customer segments and build predictive models to identify which customers are most likely to respond to specific marketing campaigns."
        ],
        "Machine learning models employed in Financial Performance Prediction": [
            "For financial performance prediction, we need numerical features from the data table as inputs and 'Bankrupt?' as the target variable for classification. The Random Forest and Logistic Regression models require properly scaled financial ratios to predict bankruptcy risk (binary classification). The ANN can handle both classification and regression tasks, potentially predicting continuous financial metrics. Banking data provides contextual features that could be encoded (categorical variables) or used directly (numerical variables) to enhance prediction accuracy by incorporating economic indicators like euribor3m."
        ],
        "Machine learning models employed in Fraud Detection and Anomaly Detection": [
            "For fraud detection, we need numerical features to train unsupervised models like Isolation Forest and One-Class SVM that identify outliers. Financial ratios and cash flow indicators from the data table will help detect accounting anomalies, while transaction patterns from the banking table can reveal unusual customer behavior. The models require normalized numerical inputs and will output anomaly scores or binary classifications (fraud/non-fraud). Time-series features should be engineered from the banking data to capture temporal patterns using LSTM, while XGBoost and Random Forest can handle the complex non-linear relationships in both datasets."
        ]
    },
    "ModelsPerTopic": {
        "Machine learning models employed in Credit Risk Assessment": "Deep Belief Networks, Convolutional Neural Networks, Long Short-Term Memory Networks",
        "Machine learning models employed in Customer Segmentation and Targeting": "Random Forest Clustering, Gaussian Mixture Models, Support Vector Machines for Segmentation",
        "Machine learning models employed in Financial Performance Prediction": "Random Forest, Artificial Neural Network (ANN), Logistic Regression",
        "Machine learning models employed in Fraud Detection and Anomaly Detection": "Long Short-Term Memory (LSTM), XGBoost, Random Forest (RF)"
    },
    "ML_Models1": [
        "Logistic Regression, Random Forest, Gradient Boosting, Neural Networks",
        "K-means Clustering, Hierarchical Clustering, DBSCAN, Decision Trees",
        "XGBoost, LSTM Networks, Support Vector Regression, Random Forest Regression",
        "Isolation Forest, One-Class SVM, Autoencoders, Ensemble Methods"
    ],
    "GPT_Columns": {
        "Machine learning models employed in Credit Risk Assessment": [
            [
                {
                    "banking": [
                        "default",
                        "loan",
                        "housing",
                        "age",
                        "job",
                        "income",
                        "education",
                        "y"
                    ]
                },
                {
                    "data": [
                        "Bankrupt?",
                        "Debt ratio %",
                        "Net Income to Total Assets",
                        "Cash Flow to Liability",
                        "Total debt/Total net worth",
                        "Interest Coverage Ratio (Interest expense to EBIT)",
                        "Net Income Flag"
                    ]
                }
            ]
        ],
        "Machine learning models employed in Customer Segmentation and Targeting": [
            [
                {
                    "banking": [
                        "age",
                        "job",
                        "marital",
                        "education",
                        "housing",
                        "loan",
                        "campaign",
                        "previous",
                        "poutcome",
                        "duration",
                        "y"
                    ]
                }
            ]
        ],
        "Machine learning models employed in Financial Performance Prediction": [
            [
                {
                    "data": [
                        "Bankrupt?",
                        "ROA(C) before interest and depreciation before interest",
                        "Operating Gross Margin",
                        "Cash flow rate",
                        "Debt ratio %",
                        "Net Income to Total Assets",
                        "Liability to Equity"
                    ]
                },
                {
                    "banking": [
                        "age",
                        "job",
                        "education",
                        "default",
                        "housing",
                        "loan",
                        "euribor3m"
                    ]
                }
            ]
        ],
        "Machine learning models employed in Fraud Detection and Anomaly Detection": [
            [
                {
                    "banking": [
                        "duration",
                        "campaign",
                        "previous",
                        "poutcome",
                        "emp_var_rate",
                        "euribor3m",
                        "y"
                    ]
                },
                {
                    "data": [
                        "Bankrupt?",
                        "Cash flow rate",
                        "Interest-bearing debt interest rate",
                        "Debt ratio %",
                        "Cash Flow to Liability",
                        "Net Income Flag",
                        "Liability-Assets Flag"
                    ]
                }
            ]
        ]
    },
    "AdjustedColumns": {}
}