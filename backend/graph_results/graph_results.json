{
    "tables": [
        {
            "banking": [
                "age",
                "job",
                "marital",
                "education",
                "default",
                "housing",
                "loan",
                "contact",
                "month",
                "day_of_week",
                "duration",
                "campaign",
                "pdays",
                "previous",
                "poutcome",
                "emp_var_rate",
                "cons_price_idx",
                "cons_conf_idx",
                "euribor3m",
                "nr_employed",
                "y"
            ]
        },
        {
            "data": [
                "Bankrupt?",
                " ROA(C) before interest and depreciation before interest",
                " ROA(A) before interest and % after tax",
                " ROA(B) before interest and depreciation after tax",
                " Operating Gross Margin",
                " Realized Sales Gross Margin",
                " Operating Profit Rate",
                " Pre-tax net Interest Rate",
                " After-tax net Interest Rate",
                " Non-industry income and expenditure/revenue",
                " Continuous interest rate (after tax)",
                " Operating Expense Rate",
                " Research and development expense rate",
                " Cash flow rate",
                " Interest-bearing debt interest rate",
                " Tax rate (A)",
                " Net Value Per Share (B)",
                " Net Value Per Share (A)",
                " Net Value Per Share (C)",
                " Persistent EPS in the Last Four Seasons",
                " Cash Flow Per Share",
                " Revenue Per Share (Yuan ¥)",
                " Operating Profit Per Share (Yuan ¥)",
                " Per Share Net profit before tax (Yuan ¥)",
                " Realized Sales Gross Profit Growth Rate",
                " Operating Profit Growth Rate",
                " After-tax Net Profit Growth Rate",
                " Regular Net Profit Growth Rate",
                " Continuous Net Profit Growth Rate",
                " Total Asset Growth Rate",
                " Net Value Growth Rate",
                " Total Asset Return Growth Rate Ratio",
                " Cash Reinvestment %",
                " Current Ratio",
                " Quick Ratio",
                " Interest Expense Ratio",
                " Total debt/Total net worth",
                " Debt ratio %",
                " Net worth/Assets",
                " Long-term fund suitability ratio (A)",
                " Borrowing dependency",
                " Contingent liabilities/Net worth",
                " Operating profit/Paid-in capital",
                " Net profit before tax/Paid-in capital",
                " Inventory and accounts receivable/Net value",
                " Total Asset Turnover",
                " Accounts Receivable Turnover",
                " Average Collection Days",
                " Inventory Turnover Rate (times)",
                " Fixed Assets Turnover Frequency",
                " Net Worth Turnover Rate (times)",
                " Revenue per person",
                " Operating profit per person",
                " Allocation rate per person",
                " Working Capital to Total Assets",
                " Quick Assets/Total Assets",
                " Current Assets/Total Assets",
                " Cash/Total Assets",
                " Quick Assets/Current Liability",
                " Cash/Current Liability",
                " Current Liability to Assets",
                " Operating Funds to Liability",
                " Inventory/Working Capital",
                " Inventory/Current Liability",
                " Current Liabilities/Liability",
                " Working Capital/Equity",
                " Current Liabilities/Equity",
                " Long-term Liability to Current Assets",
                " Retained Earnings to Total Assets",
                " Total income/Total expense",
                " Total expense/Assets",
                " Current Asset Turnover Rate",
                " Quick Asset Turnover Rate",
                " Working capitcal Turnover Rate",
                " Cash Turnover Rate",
                " Cash Flow to Sales",
                " Fixed Assets to Assets",
                " Current Liability to Liability",
                " Current Liability to Equity",
                " Equity to Long-term Liability",
                " Cash Flow to Total Assets",
                " Cash Flow to Liability",
                " CFO to Assets",
                " Cash Flow to Equity",
                " Current Liability to Current Assets",
                " Liability-Assets Flag",
                " Net Income to Total Assets",
                " Total assets to GNP price",
                " No-credit Interval",
                " Gross Profit to Sales",
                " Net Income to Stockholder's Equity",
                " Liability to Equity",
                " Degree of Financial Leverage (DFL)",
                " Interest Coverage Ratio (Interest expense to EBIT)",
                " Net Income Flag",
                " Equity to Liability"
            ]
        }
    ],
    "analyzed_topics": [
        {
            "topic": "Machine learning models employed in Credit Risk Assessment",
            "ML_Models": "Logistic Regression, Random Forest, Gradient Boosting, Neural Networks",
            "reasoning": "The banking dataset contains features like 'default', 'loan', 'housing', while the data table has bankruptcy indicators and financial ratios. ML models could predict loan defaults or bankruptcy using customer demographics (age, job, education) and financial health indicators (debt ratio, cash flow, asset turnover)."
        },
        {
            "topic": "Machine learning models employed in Customer Segmentation and Targeting",
            "ML_Models": "K-means Clustering, Hierarchical Clustering, DBSCAN, Decision Trees",
            "reasoning": "The banking dataset includes demographic information (age, job, marital status, education) and contact history (campaign, previous, poutcome). ML models could segment customers for targeted marketing campaigns by analyzing behavior patterns and demographic profiles to optimize conversion rates."
        },
        {
            "topic": "Machine learning models employed in Financial Performance Prediction",
            "ML_Models": "XGBoost, LSTM Networks, Support Vector Machines, Ensemble Methods",
            "reasoning": "The data table contains numerous financial ratios and performance metrics. ML models could forecast company financial performance using indicators like ROA, operating margins, growth rates, and asset turnover to predict future profitability and financial health."
        },
        {
            "topic": "Machine learning models employed in Market Trend Analysis",
            "ML_Models": "Time Series Analysis, ARIMA, Prophet, Recurrent Neural Networks",
            "reasoning": "The banking dataset includes economic indicators (emp_var_rate, cons_price_idx, cons_conf_idx, euribor3m) while the data table has market-related metrics. ML models could analyze market trends and economic conditions to optimize business strategies and investment decisions."
        }
    ],
    "csv_files": [
        "csv_test/banking.csv",
        "csv_test/data.csv"
    ],
    "topic": [
        "Machine learning models employed in Credit Risk Assessment",
        "Machine learning models employed in Customer Segmentation and Targeting",
        "Machine learning models employed in Financial Performance Prediction",
        "Machine learning models employed in Market Trend Analysis"
    ],
    "ScrapedArticles": {
        "Machine learning models employed in Credit Risk Assessment": "Credit Risk Modeling with Machine Learning | Towards Data Science : \nCredit Risk Modeling with Machine Learning\n\nWhat a real-world machine learning solution looks like – no background knowledge required\n\nNote from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.\n\nCredit risk modeling–the process of estimating the probability someone will pay back a loan–is one of the most important mathematical problems of the modern world. In this article, we’ll explore from the ground up how Machine Learning is applied to credit risk modeling. You don’t need to know anything about machine learning to understand this article!\n\nTo explain credit risk modeling with machine learning, we’ll first develop domain knowledge about credit risk modeling. Then, we’ll introduce four fundamental machine learning systems that can be used for credit risk modeling:\n\nBy the end of this article, you’ll understand how each of these algorithms can be applied to the real-world problem of credit risk modeling, and you’ll be well on your way to understanding the field of machine learning in general!\n\nLet’s begin learning about what credit risk modeling is by looking at a simple situation.\n\nThe Situation\n\nSay your buddy Ted needs ten bucks. You’ll want those bucks back, so he promises he’ll repay you tomorrow when you see him again.\n\nBut you’ve heard that Ted has a bit of a history with being difficult when it comes to repaying people. As a result, you know there’s a chance you’ll never see your ten dollars again.\n\nWould you lend him the money?\n\nTo make this decision, you need another piece of information.\n\nJust how difficult has Ted been in the past?\n\nIf he just forgot to pay back the wrong dude once, then you’re probably good. On the other hand, if he has a habit of getting deep into debt and fleeing to other countries, then you should probably keep your money to yourself.\n\nTed’s history tells us something about the risk of lending money to him.\n\nIf he’s been trustworthy in the past, then the risk of lending money to him is relatively low. If he’s been untrustworthy in the past, then the risk of lending money to him is relatively high.\n\nThe concept of risk gives you a logical way to make the decision about whether or not to lend Ted the money. First, determine how much risk you’re okay with. Then, figure out how risky Ted’s history makes lending money to him. If the risk of lending money to Ted isn’t greater than your maximum tolerated risk, then you can go ahead and lend him the money.\n\nIf it’s just Ted asking you for money, you’ll probably rely on your intuition and feelings to determine the risk associated with loaning money to Ted. This works well enough for just one person.\n\nBut what if you had hundreds of Teds asking you for money?\n\nYou can’t get to know every one of these people so that you can decide whether loaning to them feels like a good idea. But if you just loaned to them randomly, you might loan money to some people who are never going to pay you back. You also might reject some perfectly responsible people who would pay you back quickly. Instead, you’d want to use a more scientific process.\n\nThis scientific process is called credit risk modeling, and it’s what we’ll be exploring in this article.\n\nFormally speaking, credit risk modeling is the process of using data about a person to determine how likely it is that the person will pay back a loan. Based on the name of the process, it’s no surprise that credit card companies do credit risk modeling all the time. But credit risk modeling doesn’t necessarily have anything to do with credit cards, even though \"credit\" is in the name. Credit risk modeling applies to any loans, not just loans associated with credit cards.\n\nCountless organizations use credit risk modeling, including insurance companies, banks, investment firms, and government treasuries. Sometimes, individual people make a living using credit risk modeling to strategically loan away their own money. Credit risk modeling is ultra important anywhere people are borrowing money.\n\nEven more importantly, understanding the process of credit risk modeling will make it easier to understand any type of modeling that involves probabilities.\n\nBut for now, let’s come back to Ted so that we can get a solid grasp of the fundamentals of loans.\n\nWhat are interest rates?\n\nIf you’re lending money to Ted just to be nice, you’ll probably just ask him to give you back the same amount of money you gave him. But you could also take this opportunity to make a bit of extra money.\n\nYou could tell Ted that he has to give you back the same amount of money you gave him, plus ten percent.\n\nThis means if you give Ted $10, he’ll have to give you $11 back. The extra money he gives you back is called the interest. In this case, Ted is paying you $1 in interest, and the interest rate is ten percent. If you raised the interest rate to 20%, Ted would have to pay you $2 in interest, which means he’d have to pay you $12 in total.\n\nIf Ted really needs the money, then he’ll probably be okay with paying some interest. Paying interest on a loan is basically buying the ability to spend money that you’ll have in the future.\n\nSpending future money in the present is a convenient thing to do, and like all convenient things in the business world, it comes with a price.\n\nWith interest rates, you can see how it could be pretty useful to have 500 Teds asking you for money at the same time. If you charged all of them 20% interest, then loaning $10 to each of them could make you a thousand dollars.\n\nBut we get to that figure of $1000 by making a pretty big–and invalid–assumption: that everyone will pay you back.\n\nIf somebody doesn’t pay you back, then you permanently lose the $10 you gave them. You also miss out on the $2 they would have paid you in interest. This means that for each person who doesn’t pay you back, you miss out on $12.\n\nIf just 84 out of the 500 people don’t pay you back, then you’ll actually lose money from this whole lending process. It’s amazing that it takes less than 17% of the people to screw this whole system up.\n\nTo make a living off of this, you need a better system.\n\nWhat is default risk?\n\nFortunately, credit risk modeling lets you estimate the probability that each person will fail to pay you back. Defaulting on a loan means failing to pay it back, so each person’s probability that they’ll fail to pay back their loan is called their default risk.\n\nDetermining somebody’s default risk is important. Once you know somebody’s default risk, there’s a way to calibrate their interest rate to mitigate the risk of lending money to them.\n\nBut before we can understand how to use a default risk to calibrate somebody’s interest rate, it’s important to understand a fundamental statistics concept: the Law of Large Numbers.\n\nThe Law of Large Numbers\n\nLet’s start with a simple situation: flipping a coin.\n\nWe’ve all heard that there’s a 50% chance of getting a heads and a 50% chance of getting a tails. This is called a theoretical probability.\n\nIf you flip the coin once, then you’re going to get either heads or tails. If you flip the coin 10 times, you could get 5 heads and 5 tails (50% heads). But it’s also totally possible that you get 6 heads and 4 tails (60% heads), or even 9 heads and 1 tails (90% heads).\n\nThese observed results are called the experimental probability. The experimental probability doesn’t necessarily equal the theoretical probability.\n\nSo what does a 50% chance of getting heads actually mean?\n\nIt means that if you flip a coin a lots and lots and lots of times, the overall result will be that about 50% of the flips come of up heads.\n\nLet’s look at this step by step.\n\nLet’s flip a coin 5 times. The number of heads we got is the number of red dots, and the number of tails we got is the number of blue dots.\n\nThis looks pretty imbalanced so far, with 80% of our flips coming up heads. That’s an 80% experimental probability, which is pretty far from our 50% theoretical probability. But let’s keep flipping. Here’s what we’ve gotten after flipping the coin 15 times.\n\nNow we’ve had a total of 60% of our results come up as heads, which is a lot closer to 50%. If we keep flipping until we’ve flipped the coin 50 times, here’s what we get.\n\nThat’s 46% of our total flips now coming up heads, which is even closer to 50%. If we keep flipping the coin over and over again, then we’re going to see this percentage of heads get closer and closer to the theoretical probability of 50%. It might not get closer to 50% with every single flip, but it will generally tend to get closer to 50%.\n\nIf we keep flipping, until we get to 100 flips, the blue line represents the cumulative percentage of heads over time. The red line is 50%.\n\nThe blue line starts off really high because our first four flips were heads, which means that 100% of our cumulative tosses came out heads for a little bit. But as we get some tails in the mix, the blue line tends to get closer to the red line.\n\nAs you can see, as we flip the coin more and more times, the blue line, which represents our experimental probability, approaches the red line, which is our theoretical probability.\n\nThis is how we define a theoretical probability, like a 50% chance of getting a heads or a 16.7% chance of getting a 1 when rolling a 6-sided die.\n\nThe Law of Large Numbers is that the experimental probability tends to approach the theoretical probability as we do a large number of trials. The more trials we do, the closer the experimental probability tends to get to the theoretical probability.\n\nTo illustrate this concept, here’s a coin flip simulation graph with fifty thousand flips:\n\nBy the end, the experimental probability gets so close to the theoretical probability that the blue and red line are almost on top of each other.\n\nThis means that when we’re dealing with a ton of trials, we can assume that the experimental probability will basically equal the theoretical probability. In this case, if we flip a coin fifty thousand times, the experimental probability will be super close to 50%.\n\nIt’s important to highlight that each time you __ flip the coin, _the probability of getting a heads is always 50%, no matter what came before i_t. Even if you’ve gotten 10 heads in a row, the probability of getting another heads on the next flip is _stil_l 50%.\n\nThis helps explain why the blue line in the graph above doesn’t always go straight to the red line. Even near flip 30,000, you can see that the blue line still wiggles just a little. But over time, the blue line generally tends to approach the red line.\n\nThe important thing to take away from the Law of Large Numbers is that a large number of trials gets the experimental probability really close to the theoretical probability.\n\nIf you think this idea is making a bit of sense, you’re ready to understand the process of calibrating interest rates based on default risk.\n\nUsing default risk to calibrate interest rates\n\nLet’s say you’re thinking about making loans of $10 to a large number of people, such as 10,000 people. Using credit risk modeling, you discover that on average, each person’s theoretical default risk is 15%. The Law of Large Numbers tells you that the experimental probability approaches the theoretical probability for a large number of trials. Given that you have a large number of people here (10,000 people), it’s safe to assume that about 15% of the people you loan to really will default, and 85% will pay you back.\n\nIf you know that about 15% of people probably won’t pay you back, then you know that you’re going to permanently lose 15% of the $10 loans you make. Because 15% of 10,000 people is 1500 people, and you’re loaning each person $10, you know you’ll probably lose $15,000.\n\nBut you don’t need to just be okay with losing $15,000. There are a few things you can do here.\n\n1. Increase everyone’s interest rate to make up for that $15,000.\n\nThis way, the 85% of people who actually do pay back their loan will end up paying a total of $15,000 extra. Because 85% of 10,000 people is 8,500 people, that’s about an extra $1.76 of interest per person. In this case, that’s equal to raising the interest rate by 17.6 percentage points.\n\nHere’s the issue: if the interest rate is already set at 20% so that you can make some profit, then raising the interest rate another 17.6 percentage points makes the total interest rate 37.6%. That’s a big increase, and increasing the interest rate like that might deter some potential borrowers.\n\n2. Only accept the safest borrowers.\n\nCredit risk modeling lets you see each individual’s default risk separately. Some people might have a 40% default risk, while others might have just a 1% default risk. If you only give loans to people whose default risk is below a certain threshold, say 2%, then the percentage of people who will pay you back will be much higher. This means that fewer people will default, and you’ll lose less money.\n\nAs a result, you can charge a lower interest rate, because you won’t have to make up for as much lost money due to defaulting. The problem with this strategy is that you might have to turn away a big portion of people who want a loan, which would decrease your profits.\n\n3. Charge people interest rates that are proportional to their default risk.\n\nThis is the most important strategy in the real world. As we go through all the borrowers, we’ll call each individual borrower \"borrower x\". For each borrower x, you charge borrower x an interest rate such that you wouldn’t lose any money if every borrower had borrower x’s default risk.\n\nThat’s a dense idea, so let’s unpack it.\n\nWhen analyzing the first strategy, we discovered that we should add 17.6 percentage points to each borrower’s interest rate if each borrower has a 15% default risk. (This is the same as 15% being the average default risk.) This way, the extra interest paid by the 85% who don’t default will make up for the 15% of people who default.\n\nIf everybody you’re loaning money to has the same default risk, then there’s always a certain amount that should be added to each borrower’s interest rate to offset the people who default.\n\nIn fact, the amount that should be added to each borrower’s interest rate can be calculated with the following formula:\n\nThere’s no need to delve into this formula if you don’t want to, but if you want to learn how it works, this paragraph will show you how. The risk term on top represents the share of people who are going to default. Defaulting is a binary outcome; this means that every person is either going to default or not default. There’s no in-between. The number 1 represents 100% of the people, so the term 1 – risk (one minus risk) is the share of people who don’t default. You divide the defaulters by the non-defaulters to determine what percentage of each defaulter’s loan the non-defaulters need to pay off.\n\nEven if each borrower’s default risk is different, you can use this formula to calculate exactly how much you should add to each borrower’s interest rate to counterbalance their risk. If you add this amount to every borrower’s interest rate, and you have a large number of borrowers, the Law of Large Numbers shows us that you almost certainly won’t lose money in the long run.\n\nJust to recap, here’s a breakdown of the money somebody might pay you back after you loan money to them.\n\nIn the real world, these strategies are combined. In accordance with strategy 1, lenders often increase everyone’s interest rate by small fixed rate, regardless of default risk, to make up for uncertainty in their models. In accordance with strategy 2, lenders often have a risk threshold, so they won’t accept everyone. And finally, in accordance with strategy 3, lenders use each borrower’s default risk to add a certain number of percentage points to each individual’s interest rate.\n\nIf you’re getting the hang of these three strategies, then you have a solid grasp of the fundamentals of loaning money.\n\nAt this point, we’ve explored why finding somebody’s default risk is so useful. Now, we’re ready to dive into the machine learning models used in credit risk modeling to calculate these default risks.\n\nWhat is machine learning?\n\nPeople toss around the term \"machine learning\" a lot nowadays, and sometimes it’s hard to know what it means.\n\nSimply put, a machine learning model is a mathematical predictive model that gets better as it sees more data.\n\nTo do machine learning, you need two things: a model, and data.\n\nThere are tons of different types of machine learning models. A machine learning model is the name for the set of steps that are used to make predictions based off the data. Below, we’ll explore four fundamental machine learning models that are important in credit risk modeling.\n\nYou also need historical data, and lots of it. This data needs to contain both the thing you’re trying to predict (called the target) and other characteristics that are related to the thing you’re trying to predict (called the features). In the case of credit risk modeling, you need historical data for lots of individual people. The target data here is whether or not each person defaulted. The feature data here consists of statistics about each person, such as their income, age, and employment status.\n\nOnce you have your data, you need to train the model by feeding it historical data. Machine learning models are built to connect the feature data to the target data. After training, the model can look at cases where the feature data is known and the target data isn’t, and the model can make predictions.\n\nThis situation is common in real life. When you’re trying to decide whether or not to give someone a loan, you usually have all their feature data, such as income and age. You just don’t know the target data: the probability that they’ll default on their loan.\n\nCalculating this probability is where machine learning models shine.\n\nLet’s start with a simple, but surprisingly powerful model.\n\nThe K-Nearest Neighbors Model\n\nLet’s say we have a bunch of historical lending data with two features: each person’s income and their age. We also know whether each person defaulted on their loan. If they defaulted, then their dot is colored red on the graph. If they paid back their loan, their dot is colored blue on the graph.\n\nNow let’s say Ted is asking again for a loan. We want to know how likely it is that he’ll pay back his loan, and we now have a much more powerful tool than our feelings alone. If we know Ted’s income and age, then we can plot him in green on the same graph.\n\nWe then select a number of \"nearest neighbors\" that we’ll look at. This number is called K. If K = 5, then we’ll select the 5 dots that are closest to Ted’s dot and figure out what percentage of them defaulted.\n\nIn this case, 80% of people nearest to Ted paid back their loans (4 people), and 20% of them defaulted (1 person). This means that we tentatively estimate Ted’s default risk as 20%.\n\nOf course, you’re probably thinking that setting K equal to 5 seems pretty arbitrary. There is a way to optimize our K value, and it lies in testing the model.\n\nTo test the model, we split our historical data into two parts: the training data and the testing data. We then set our K value to an arbitrary value, say 10, and we feed the model our training data to generate a graph like the one above. Then, for each person in the testing data, we make our trained model guess whether or not the person defaulted on their loan. The model doesn’t get to see the target column of the testing data; it only sees the features. During testing, the model has to make predictions based on the features alone, just like it would in real life.\n\nEach time the model makes a prediction, we record whether the model predicted correctly. We then calculate the model’s accuracy score by finding the percentage of cases it predicted correctly.\n\nTo optimize our K value, we test the model over and over again with different K values until we find one that works best. Sometimes the ideal K value is 1, while other times the ideal K value is huge. (Of course, for credit risk modeling, a K value of 1 would be silly, because this would either give us a 0% or 100% default risk.)\n\nIn the 2D graph above, we only had 2 feature variables: income and age. But the k-nearest neighbors algorithm works great with more than 2 feature variables as well. If we had 3 feature variables, we could plot the points in a 3D box instead of a 2D square.\n\nWith 3 variables, the same set of steps applies. Training the model gives us a 3D graph with a bunch of blue and red dots on it. When we have a person whose default risk we’re trying to predict, we plot them on the graph. To calculate their default risk, we find this person’s K nearest neighbors, and we calculate the percentage of these neighbors who defaulted. We say that this percentage is the new person’s default risk.\n\nOnce we get to 4 variables, the graph becomes hard to visualize. But the computer can deal with a nearly infinite number of variables. In fact, it’s not too difficult to algebraically calculate nearest neighbors by hand with 4 or more variables using the Pythagorean Theorem. (Google it if you’re interested!)\n\nk-nearest neighbors might seem like quite a simplistic model, but with thousands of data points and many features, it can be quite powerful.\n\nIn some cases, however, other models are more effective. Let’s discuss another classic machine learning model: logistic regression.\n\nThe Logistic Regression Model\n\nThe simplest form of logistic regression involves a dataset with a target column and a single feature column. For instance, a historical dataset that contains borrowers’ incomes and whether or not they defaulted on their loan would be very simple to use for logistic regression.\n\nFor logistic regression, the target data must be binary, which means that it can only have two outcomes. In credit risk modeling, the target data is indeed binary: a person can either default or not default on a loan.\n\nIn order to make a graph, we can represent this binary outcome as the probability that a person in the past paid back their loan. If somebody defaulted on a loan in the past, then we can say that there was a 0 probability that they paid back their loan. If somebody actually did pay back their loan, then we can say that the probability that they paid back their loan was 1. It might feel weird to assign probabilities to past events, but probabilities of 0% or 100% are just another way of saying that we know something either definitely didn’t happen or definitely did happen.\n\nHere’s what a graph of income vs historical default probability might look like:\n\nBecause we’re dealing with definite past data, all the dots are either on the top, or bottom. A blue dot on the top represents a person who did pay back their loan, while a red dot on the bottom represents a person who defaulted.\n\nThis is where things get cool. Using some interesting math that we won’t dive into here, the logistic regression algorithm calculates a line that looks something like this:\n\nThis line tells us the probability that a new person with a given income level will pay back their loan.\n\nPredicting a new person’s default risk with logistic regression is pretty simple. You just find the point on the income axis that corresponds to the new person’s income, and the logistic regression line’s y-value that corresponds to this point gives you the probability that they’ll pay back the loan. Subtracting this value from 1 gives you their default risk.\n\nLet’s say that Ted’s income is right here:\n\nTo find the probability that Ted will pay back his loan, we go up and meet the logistic regression trend line.\n\nBased on this model, it looks like he has about a 85% chance of paying back his loan. This means that his default risk is 15%.\n\nIt’s worth noting that this version of logistic regression put Ted’s default risk at 15%, while our k-nearest neighbors model put Ted’s default risk at 20%. Getting different predictions from different models is totally normal in the world of machine learning. To decide which model to use, we would have to do a bunch of train/test cycles with historical data to see which model consistently performs best in this scenario.\n\nA 2D version of the logistic regression model is pretty simplistic, just like 2D k-nearest neighbors. But just like K-Nearest-Neighbors, a computer (or a person with a pencil!) can extend logistic regression to an infinite number of dimensions without any problem.\n\nK-nearest neighbors and logistic regression are both great models for making approximations, especially if the data follows a linear pattern. But sometimes, it’s nice to have a model that picks up on subtle, non-linear patterns in the data. The decision tree model is a simple model that’s excellent at finding such patterns.\n\nDecision Trees\n\nTo understand decision trees, let’s go back to the graph we used to illustrate how k-nearest neighbors works.\n\nAs we’ve seen, one way to make sense of this graph would be to use the k-nearest neighbors algorithm. However, we could also make some simple rules for how we classify points based on their location. For instance, we could draw a line that splits the graph in half, like this:\n\nDrawing this line splits the data into two halves. One way to represent this is using a tree diagram, as we can see below.\n\nThen, we can split each of these splits again, so that they end up looking like this on the graph:\n\nAnd here’s what this second split looks like in tree form:\n\nWe can keep making smaller and smaller splits like these until we’re left with just one outcome–default or no default–on the bottom of each branch. Another way to look at this is that we keep making the boxes on the graph smaller and smaller until each box only contains one color of dots.\n\nFor each split, the computer finds the optimal value to split on by using an interesting mathematical process that we won’t delve into here.\n\nTo test a new case using the decision tree algorithm, you simply start at the beginning and follow the branches down.\n\nLet’s say Ted makes $50,000 a year and is 30 years old. We start at the top of the tree.\n\nThen, we move down and right, because Ted makes more than $20,000 per year.\n\nFrom here, we move to the right because Ted is older than 28.\n\nThis process continues, until we get all the way to the bottom of the tree, where there is only one possible outcome on a leaf. In a real decision tree, there would be many more branches than the two layers shown here.\n\nThe issue with this algorithm is that it returns a binary result: either Ted will default, or Ted won’t default. Given that any model is inherently imperfect, this binary result isn’t that useful for credit modeling. We can’t use a binary result to set Ted’s interest rate.\n\nThe solution is to use random forests. A random forest is simply a collection of decision trees, all structured slightly differently. To make a prediction, each tree in the forest \"votes\" for whether or not it thinks Ted will default on the loan. Ted’s credit risk is the percentage of the trees that say he will default.\n\nLike the other algorithms we discussed, the decision tree algorithm can be extended to as many dimensions as we want. One advantage of the decision tree algorithm is that it’s very good at capturing non-linear data. However, decision trees sometimes look too far into noisy data, thinking that they see patterns where statistically significant patterns don’t exist.\n\nThere’s one more type of machine learning algorithm that we’ll briefly touch on here.\n\nNeural Networks\n\nNeural networks are considerably more complex than the three algorithms we’ve already discussed. They’re a fascinating algorithm that’s modeled off of the human brain, and they’re at the core of most systems that can be considered \"AI\".\n\nNeural networks are usually overkill for simple credit risk modeling problems, but in certain cases, they can be tremendously useful.\n\nIf you want to learn more about neural networks, you can read a detailed explanation of them here.\n\nSummary of the risk modeling process\n\nThese four classes of algorithms (k-nearest neighbors, logistic regression, decision tress, and neural networks) are just the beginning of the machine learning used in credit risk modeling. But understanding the basics of these algorithms gives you a bit of insight into the whole credit risk modeling process, and this understanding gives you the perfect springboard to learn more about machine learning.\n\nFirst, we developed a bit of domain knowledge about how loans and interest rates are used in the real world. Then, we looked at splitting the data into train and test sets to analyze various models. Finally, we analyzed several models that we could test out, and we know that we’d use the most effective models’ outputs to make our predictions.\n\nA similar process is applicable to almost any problem that involves machine learning.\n\nFirst, it’s important to develop some domain knowledge about the problem you’re dealing with so that you know how to ask the right questions. Then, you have to clean and prepare your data (a topic we didn’t delve into here). Next, you train and test a variety of models, honing the models to maximize prediction accuracy. Finally, you present your results to others and apply them to real world problems, repeating this process continuously.\n\nIf you have a better understanding of this general process, then you’re well on your way to better understanding credit risk modeling and machine learning as a whole!\n\nWRITTEN BY\n\nTopics:\n\nShare this article:\n\nRelated Articles\n\nImplementing Convolutional Neural Networks in TensorFlow\n\nStep-by-step code guide to building a Convolutional Neural Network\n\nHow to Forecast Hierarchical Time Series\n\nA beginner’s guide to forecast reconciliation\n\nHands-on Time Series Anomaly Detection using Autoencoders, with Python\n\nHere’s how to use Autoencoders to detect signals with anomalies in a few lines of…\n\n3 AI Use Cases (That Are Not a Chatbot)\n\nFeature engineering, structuring unstructured data, and lead scoring\n\nSolving a Constrained Project Scheduling Problem with Quantum Annealing\n\nSolving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM)\n\nBack To Basics, Part Uno: Linear Regression and Cost Function\n\nAn illustrated guide on essential machine learning concepts\n\nMust-Know in Statistics: The Bivariate Normal Projection Explained\n\nDerivation and practical examples of this powerful concept\n\nYour home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\n\nBank credit risk prediction using machine learning model : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nBank credit risk prediction using machine learning model\n\n153 Accesses\n\nExplore all metrics\n\nAbstract\n\nCredit risk is one of the most prevalent risks in the banking sector. For credit risk management, digital transformation brings greater clarity to the risk profiles. Typically, datasets include various features, and many of them may be irrelevant. Removing relevant features and keeping irrelevant ones in the dataset may be harmful and lead to ruining the decision-making process. In this paper, we investigate the interesting decision variables that can be used to create a credit risk prediction model. This model will help the bank staff evaluate the loan applications of customers and classify the loan applicants as “defaulter” and “non-defaulter” customers. In addition, the loan applicants can decide on the loan acceptance status online without physical visits to the bank. In this work, we build a machine learning model by using a multilayer perceptron (MLP) algorithm based on the Australian and Taiwan banking dataset. Besides, we deploy the SMOTE technique to balance the data and overcome the suboptimal results. For statistical calculations, we use the R computing environment. The results show that demographic, financial, economic, and ‘behavioral indicators have significant effects on credit risk and the efficiency of the proposed model, with an average accuracy of 89.543% for the Australian dataset and 88.345% for the Taiwan dataset.\n\nThis is a preview of subscription content, log in via an institution to check access.\n\nAccess this article\n\nSubscribe and save\n\nBuy Now\n\nPrice includes VAT (Lebanon)\n\nInstant access to the full article PDF.\n\nInstitutional subscriptions\n\nSimilar content being viewed by others\n\nCredit Risk Assessment with Madaline and Multilayer Perceptrons\n\nBanking Credit Risk Analysis using Artificial Neural Network\n\nMachine Learning Based Consumer Credit Risk Prediction\n\nExplore related subjects\n\nData availability\n\nThe dataset used in the current study is available on the UC Irvine Machine Learning Repository and is accessible via https://archive.ics.uci.edu/dataset/143/statlog+australian+credit+approval..\n\nReferences\n\nLin S, Lin J (2023) How organizations leverage digital technology to develop customization and enhance customer relationship performance: an empirical investigation. Technol Forecast Social Change 188:122254\n\nArticle\n  MATH\n  Google Scholar\n\nMartínez-Peláez R, Ochoa-Brust A, Rivera S, Félix VG, Ostos R, Brito H, Félix RA, Mena LJ (2023) Role of digital transformation for achieving sustainability: mediated role of stakeholders, key capabilities, and technology. Sustainability 15(14):11221\n\nArticle\n  Google Scholar\n\nOzili PK, Iorember PT (2023) Financial stability and sustainable development. Int J Financ Econ 29(3):2620–2646\n\nArticle\n  MATH\n  Google Scholar\n\nKarim S, Akhtar MU, Tashfeen R, Rabbani MR, Abdul Rahman AA, AlAbbas A (2022) Sustainable banking regulations pre and during coronavirus outbreak: the moderating role of financial stability. Econ Res-Ekonomska Istraživanja 35(1):3360–3377\n\nArticle\n  Google Scholar\n\nBuehler K, Freeman A, Hulme R (2008) The tools-the new arsenal of risk management. Harvard Bus Rev 86(9):92\n\nMATH\n  Google Scholar\n\nNaili M, Lahrichi Y (2022) The determinants of banks’ credit risk: review of the literature and future research agenda. Int J Financ Econ 27(1):334–360\n\nArticle\n  MATH\n  Google Scholar\n\nLamba SS, Kaur N (2022) A literature survey on machine learning in banking risk management. Resmilitaris 12(6):522–531\n\nMATH\n  Google Scholar\n\nBouteille S, Coogan-Pushner D (2021) The handbook of credit risk management: originating, assessing, and managing credit exposures. John Wiley & Sons, HOboken\n\nGoogle Scholar\n\nStuart T (2005) New players, new landscape. the banker, special supplement. Financial Times, April\n\nBacham D, Zhao J (2017) Machine learning: challenges, lessons, and opportunities in credit risk modeling. Moody’s Anal Risk Perspect 9:30–35\n\nMATH\n  Google Scholar\n\nLaryea E, Ntow-Gyamfi M, Alu AA (2016) Nonperforming loans and bank profitability: evidence from an emerging market. African J Econ Manag Stud 7(4):462–481\n\nArticle\n  Google Scholar\n\nKou G, Chao X, Peng Y, Alsaadi FE, Herrera Viedma E et al (2019) Machine learning methods for systemic risk analysis in financial sectors. Technol Econ Dev Econ 25(5):716–742\n\nArticle\n  MATH\n  Google Scholar\n\nRehman ZU, Muhammad N, Sarwar B, Raz MA (2019) Impact of risk management strategies on the credit risk faced by commercial banks of balochistan. Financ Innov 5:1–13\n\nArticle\n  MATH\n  Google Scholar\n\nIncekara A, Çetinkaya H (2019) Credit risk management: a panel data analysis on the islamic banks in turkey. Proce Comput Sci 158:947–954\n\nArticle\n  MATH\n  Google Scholar\n\nSoui M, Gasmi I, Smiti S, Ghédira K (2019) Rule-based credit risk assessment model using multi-objective evolutionary algorithms. Expert systems with applications 126:144–157\n\nArticle\n  Google Scholar\n\nAlam TM, Shaukat K, Hameed IA, Luo S, Sarwar MU, Shabbir S, Li J, Khushi M (2020) An investigation of credit card default prediction in the imbalanced datasets. IEEE Access 8:201173–201198\n\nArticle\n  Google Scholar\n\nElhoseny M, Metawa N, El-hasnony IM (2022) A new metaheuristic optimization model for financial crisis prediction: towards sustainable development. Sust Comput Inf Syst. https://doi.org/10.1016/j.suscom.2022.100778\n\nArticle\n  MATH\n  Google Scholar\n\nZhou Y, Shamsu Uddin M, Habib T, Chi G, Yuan K (2021) Feature selection in credit risk modeling: an international evidence. Economic research-Ekonomska istraživanja 34(1):3064–3091\n\nArticle\n  Google Scholar\n\nYap BW, Ong SH, Husain NHM (2011) Using data mining to improve assessment of credit worthiness via credit scoring models. Expert Syst Appl 38(10):13274–13283\n\nArticle\n  MATH\n  Google Scholar\n\nKarimi A (2014) Evaluation of the credit risk with statistical analysis. Int J Acad Res Account Financ Manage Sci 4(3):206–211\n\nMATH\n  Google Scholar\n\nHall MJ, Muljawan D, Suprayogi Moorena L (2009) Using the artificial neural network to assess bank credit risk: a case study of indonesia. Appl Financ Econ 19(22):1825–1846\n\nArticle\n  Google Scholar\n\nMitei A (2017) Determinants of loan default by savings and credit co-operative societies’ members in baringo county, kenya. PhD thesis, Egerton University\n\nBenítez-Peña S, Blanquero R, Carrizosa E, Ramírez-Cobo P (2019) Cost-sensitive feature selection for support vector machines. Comput Operat Res 106:169–178\n\nArticle\n  MathSciNet\n  MATH\n  Google Scholar\n\nTripathi D, Edla DR, Cheruku R, Kuppili V (2019) A novel hybrid credit scoring model based on ensemble feature selection and multilayer ensemble classification. Comput Intell 35(2):371–394\n\nArticle\n  MathSciNet\n  Google Scholar\n\nMunkhdalai L, Lee JY, Ryu KH (2019) A hybrid credit scoring model using neural networks and logistic regression. In: advances in intelligent information hiding and multimedia signal processing: Proceedings of the 15th international conference on IIH-MSP in conjunction with the 12th international conference on FITAT, July 18-20, Jilin, China, Volume 1, pp. 251–258 . Springer\n\nJadhav S, He H, Jenkins K (2018) Information gain directed genetic algorithm wrapper feature selection for credit rating. Appl Soft Comput 69:541–553\n\nArticle\n  MATH\n  Google Scholar\n\nTripathi D, Edla DR, Kuppili V, Bablani A, Dharavath R (2018) Credit scoring model based on weighted voting and cluster based feature selection. Proce Comput Sci 132:22–31\n\nArticle\n  MATH\n  Google Scholar\n\nDownload references\n\nFunding\n\nNot applicable.\n\nAuthor information\n\nAuthors and Affiliations\n\nUniversity of Manouba, Manouba, Tunisia\n\nInes Gasmi\n\nSchool of Innovation and Technology, University of Michigan-Flint, Flint, Michigan, USA\n\nSana Neji & Makram Soui\n\nSchool of Engineering and Computer Science, Oakland University, Rochester Hills, Michigan, USA\n\nNesrine Mansouri\n\nContributions\n\nIG and SN conceived of the presented idea. NM and MS developed the theory and performed the computations. IG and SN verified the analytical methods. All authors discussed the results. IG and MS investigated and supervised the findings of this work. All authors wrote, read, and approved the final manuscript.\n\nCorresponding author\n\nCorrespondence to Nesrine Mansouri.\n\nEthics declarations\n\nConflict of interest\n\nThe authors have no Conflict of interest to declare.\n\nEthical approval\n\nNot applicable.\n\nInformed consent\n\nNot applicable.\n\nHuman and animals rights\n\nNot applicable.\n\nConsent to participate\n\nNot applicable.\n\nConsent to publish\n\nNot applicable.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nRights and permissions\n\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nGasmi, I., Neji, S., Mansouri, N. et al. Bank credit risk prediction using machine learning model. Neural Comput & Applic (2025). https://doi.org/10.1007/s00521-025-11044-5\n\nDownload citation\n\nReceived\n30 May 2024\n\nAccepted\n22 January 2025\n\nPublished\n17 March 2025\n\nDOI\nhttps://doi.org/10.1007/s00521-025-11044-5\n\nKeywords\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nArtificial Intelligence and Machine Learning in Credit Risk Assessment ... : \nAbstract\n\nCredit risk assessment has become one of the major concerns in modern finance regarding informed lending decisions. Although several studies have used traditional logistic regression and linear discriminant analysis techniques, these have increasingly become inadequate tools in today’s complex and data-rich environment. Such models often struggle with large datasets and nonlinear relationships, thus reducing their predictive power and adaptability. Artificial Intelligence (AI) and Machine Learning (ML) provide two of the most innovative approaches to credit risk modeling. This paper reviews a few ML models applied to improve the accuracy and efficiency of credit risk assessment, from Random Forests and Support Vector Machines to Neural Networks. Compared to the more traditional models, AI models can enhance predictive accuracy by using a wealth of structured and unstructured information, including alternative information sources such as social media activities and transaction history. However, despite noticeable advantages, there are some challenges concerning the use of AI in credit risk assessment, including model opaqueness, bias, and regulatory compliance. The nature of such a “black box”, especially for deep learning algorithms, can limit their interpretability and complicate regulatory compliance and decision rationalization. To solve problems induced by this “black box” nature, explainable AI techniques, namely Shapley values and LIME, have been implemented to enhance the transparency of models and raise stakeholder trust in support systems for decision-making. This review aims to evaluate the current applications of AI and ML in credit risk assessment, weigh the strengths and limitations of various models, and discuss the ethical considerations and regulatory challenges linked to their adoption by credit institutions.\n\nKeywords\n\nArtificial Intelligence, Machine Learning, Credit Risk Assessment, Data Bias\n\nShare and Cite:\n\n1. Introduction\n\nAccurate and fair assessment of credit risks is rudimentary in modern finance for informed lending decisions. Whereas the traditional models included logistic regression and linear discriminant analysis, these have recently become inadequate in today’s complex and data-rich environments. Though transparent and regulator-friendly, these methods usually cannot capture borrowers’ nuanced nonlinear behaviors, compromising predictive accuracy.\n\nThe change in credit risk modeling heralds AI and ML. Predictive power has radically improved by accessing large datasets and the ability to discern patterns not accessible to traditional analytics. However, interpretability, ethical issues, and regulatory compliance become issues with the translucent nature of “black box” models such as deep neural networks.\n\nThis review critically analyzes various state-of-the-art AI and ML models in credit risk assessment by comparing them with traditional methods and discussing the potential of such models to reshape financial decision-making. We analyze different models, such as Random Forests, Support Vector Machines, and Neural Networks, to discuss how these technologies can improve accuracy by considering ethical and regulatory considerations proper for widespread adoption. The paper will help achieve a balanced understanding of the responsible implementation of AI, hoping that technological advancements will translate into improvements in the consistency and transparency of credit risk assessment.\n\n1.1. Traditional Credit Risk Modeling\n\nFor years, traditional models such as logistic regression and linear discriminant analysis have formed the foundation of credit risk assessment, a core function of the financial industry. These models conventionally rely on structured data, including borrowers’ income, credit scores, and debt levels, to predict the likelihood of default (Breeden, 2020). Although these methods have served well for many decades, they are based on assumptions of linearity, which often fail to capture the complexity of borrowers’ behavior in today’s data-rich environment.\n\nFor instance, logistic regression assumes a linear relationship between the input variables and the outcome default or non-default, which can oversimplify real-world credit risk (Bussmann et al., 2020). Furthermore, traditional models often require significant human intervention to extract key features, potentially overlooking meaningful nonlinear interactions between variables (Addo et al., 2018).\n\nDespite these deficiencies, traditional models remain popular, because they are transparent and acceptable to many regulators. Their interpretability advantages over more complex model types, neural networks are a good example of them appealing to regulators explicitly required to explain credit decisions. However, as data becomes increasingly complex and voluminous, these models struggle to keep pace, highlighting the need for more advanced techniques such as AI and ML.\n\n1.2. Credit Risk Assessment Models Using Artificial Intelligence and Machine Learning\n\nRecently, AI and ML have emerged as powerful tools for credit risk assessment. These methods offer several advantages over traditional models, mainly due to their ability to process large and complex datasets and uncover nonlinear relationships. Examples of such models include Random Forests, Support Vector Machines (SVMs), and Neural Networks, which can learn from vast amounts of data and recognize patterns that traditional models might miss.\n\nRandom Forest, an ensemble learning model, builds multiple decision trees and then combines their outputs for better accuracy. It is highly effective in credit risk assessment because the relationship between borrower attributes and default probability is often complex and nonlinear. Research has shown that Random Forests outperform conventional models in predictive accuracy, especially when the model is enhanced with data sources such as social media activities and transaction history.\n\nNeural Networks have also been shown to reveal complex nonlinear relationships among variables. They are instrumental when managing vast amounts of unstructured data, such as transaction histories or mobile phone usage data (Zhang & Yu, 2024). However, interpretability is considered one of the main disadvantages of neural networks, referred to as the “black box” problem-which makes it difficult for financial institutions to justify their credit decisions.\n\nSVMs also show exciting potential for credit risk modeling, especially when dealing with unbalanced datasets, as is often the case where the number of defaulters is less than that of non-defaulters. Support Vector Machines seek an optimal hyperplane that separates the two classes: defaulters versus non-defaulters. In situations where generalizing from traditional models is difficult, SVMs are particularly robust.\n\n1.3. Incorporation of Alternative Information Providers\n\nOne of the most crucial advantages of AI and ML models in evaluating credit risk is their ability to incorporate alternative data. Traditional models have relied heavily on structured data such as credit scores and income levels, which are inaccessible to all borrowers, particularly in emerging markets or those with incomplete credit histories (Breeden, 2020). In contrast, AI models can use unstructured data from social media activity, mobile phone usage, and transaction history to better understand a borrower’s behavior.\n\nFor instance, Kou et al. (Kou et al., 2019) highlighted the contributions of social media data in enhancing credit risk assessment. AI models can better predict financial reliability based on factors like the size of borrowers’ social networks, posting frequencies, and sentiment analysis from their online activities. Mobile usage data, such as call logs and location information, can provide in-depth insights into a borrower’s stability and predict their likelihood of default.\n\nThese alternative data sources are particularly useful for assessing borrowers with minimal traditional credit records, making them an integral part of modern credit risk models. However, the use of alternative data raises ethical concerns regarding data privacy and potential discrimination. Financial organizations should ensure that their alternative data practices comply with data protection regulations, such as the GDPR in Europe, and that their models are not discriminatory across different demographic groups.\n\n1.4. Explainability and Transparency in Artificial Intelligence Models\n\nOne of the biggest challenges posed by AI and ML models in credit risk assessment is their lack of transparency. Traditional models, such as logistic regression, are comparatively easier to explain, allowing lending and regulatory bodies to understand how decisions are made. In contrast, many AI models, particularly neural networks, function as an impenetrable “black box”, making it difficult to explain how they derive their predictions.\n\nThis lack of transparency creates serious problems in regulated industries like finance, where institutions must demonstrate the reasonableness of their lending decisions. Consequently, researchers have developed techniques to make AI models more explainable. For example, Shapley values provide a method for quantifying the contribution of each feature to model predictions, thereby helping financial institutions more easily explain their decisions to regulatory bodies and customers.\n\nAnother explainability technique is LIME, which generates interpretable models to approximate complex AI models for individual predictions. These methods are paramount for enhancing trust in AI credit assessment models, especially when a borrower is denied credit and an explanation is required (Angelini et al., 2008).\n\n1.5. Bias and Fairness of Artificial Intelligence Models\n\nWhile credit risk estimates have improved, biases inherent in AI models remain a concern. AI models are trained on historical data, which can be biased and lead to discriminatory outcomes. For example, a model trained on data from discriminatory lending practices may still be biased against certain demographic groups, even if sensitive attributes like race or gender are excluded.\n\nSome methods incorporate fairness constraints during model training to prevent sensitive attributes from affecting predictions. Other biases may be reduced by approaches like “fairness through unawareness”, which removes sensitive variables from the model. However, Kou et al. (Kou et al., 2019) demonstrated that information like geographical location or employment status can introduce biases even without sensitive variables.\n\nFinancial institutions must ensure that their AI models comply with regulatory standards that prevent discrimination, such as the FCRA in the United States and the Equality Act in the United Kingdom. According to Fernandez (Fernandez, 2019), failure to meet these regulations can generate serious legal and reputational risks for financial institutions.\n\n1.6. Performance Metrics for Artificial Intelligence Models in Credit Risk\n\nBeyond accuracy, various performance metrics are used for AI models in credit risk assessment. In imbalanced datasets-with few defaulters compared to non-defaulters-metrics like precision, recall, their harmonic mean (F1 score), and the area under the ROC curve (AUC-ROC) are crucial.\n\nAccuracy is defined as the ratio of correctly classified instances to all instances. While accuracy is useful, it can be misleading in imbalanced datasets where one class (the majority class) dominates. Precision is the ratio of true positive predictions (correctly predicted defaulters) to all positive predictions made by the model. Precision is relevant in credit risk assessment because a false positive-classifying a borrower as a defaulter when they are not-can lead to lost lending opportunities.\n\nRecall measures the proportion of true positives out of all actual positive instances (defaulters). A high recall ensures that a high proportion of high-risk borrowers are correctly identified. The F1 score, the harmonic mean of precision and recall, provides a balanced metric that considers both false positives and false negatives.\n\nThe AUC-ROC is a measure indicating the model’s capability to differentiate between defaulters and non-defaulters. A higher AUC indicates better model performance. Utilizing these performance metrics helps in understanding the strengths and weaknesses of different AI models in credit risk assessment.\n\n2. Methodology\n\n2.1. Data Collection and Preprocessing\n\nThis research is based on a comprehensive dataset from various sources that integrates both structured and unstructured data to enhance the validity of models when assessing credit risk. The structured data includes conventional financial variables related to borrowers’ income, debt-to-income ratio, credit history, and loan amounts-definite elements in any traditional credit risk assessment model (Breeden, 2020; Addo et al., 2018). Additionally, it leverages data from social media activities, mobile transactions, and geolocation to capture more depth in borrower behavior than conventional data may miss.\n\nThe dataset consists of over 200,000 records of individual borrowers spanning from 2017 to 2022. This large sample size provides high statistical power and makes the model dependable. All personal information is anonymized in accordance with strict standards, such as the European Union’s General Data Protection Regulation (GDPR), which demands that organizations maintain stringent data handling and protection.\n\nData preprocessing involves several important steps. Missing values are imputed using mean and mode strategies for numerical and categorical variables, respectively. Outliers are detected and removed to avoid bias in the results. For unstructured data, natural language processing techniques such as sentiment analysis and keyword extraction are employed to transform text data into analyzable features. Sentiment analysis, also known as sentiment analysis or opinion mining, is the process of analyzing, processing, summarizing, and inferring subjective texts with emotional connotations. By utilizing the ability of sentiment analysis, it is possible to automatically determine the positive and negative emotional tendencies of natural language texts with subjective descriptions and provide corresponding results. Keyword extraction is the process of extracting the most relevant words from the text that are most relevant to the meaning of the article. It has important applications in literature search, automatic summarization, text clustering, and text classification. Transaction data is summarized to reveal financial behaviors, including transaction frequency and volume.\n\nReferring to the commonly used classification standards for training and testing sets in deep learning networks, this article divides the preprocessed data into an 80% training set and a 20% testing set. To avoid overfitting, 10-fold cross-validation is used to tune the models and measure their performance.\n\n2.2. Model Selection and Architecture\n\nIn this study, we will evaluate the effectiveness of several AI and ML models in credit risk assessment and compare them with the baseline logistic regression model:\n\nRandom Forests: As an integrated learning model, random forests improve prediction accuracy by constructing multiple decision trees and merging their outputs. They are excellent at preventing overfitting and can provide an ordering of the importance of features that affect prediction.\n\nSupport Vector Machines (SVMs): SVMs are excellent at handling unbalanced datasets and are able to find optimal hyperplanes in high-dimensional spaces to separate different classes. This makes SVMs particularly well suited to deal with class imbalance.\n\nNeural networks: These models can handle complex nonlinear relationships between input variables. Deep learning models with a multi-layer structure make neural networks particularly effective at handling structured and unstructured data. They are especially good at recognizing patterns that may not be detected by traditional models.\n\nLogistic regression: used as a benchmark model for performance comparisons, although logistic regression is widely used for credit risk modeling and its linear nature has limitations when dealing with interactions between variables.\n\n2.3. Feature Engineering and Selection\n\nThe predictive power of a model can be significantly enhanced through efficient feature engineering. New features derived from raw data can more accurately reflect borrower behavior and risk profile, such as calculating debt-to-income ratios, default rates, and loan maturities.\n\nSentiment analysis of social media posts can reveal borrower sentiment and risk tolerance, information hidden in unstructured data. Variables such as transaction frequency, average transaction amount, and stability derived from geolocation data further enrich the database for risk assessment models. These features demonstrate details of financial behavior that are difficult to capture in traditional data.\n\n2.4. Evaluation Metrics\n\nTherefore, a range of metrics was considered to comprehensively evaluate the model performance: accuracy, precision, recall, F1 score, and area under the receiver operator characteristic curve (AUC-ROC). Since credit risk datasets are often classically unbalanced, relying on accuracy alone may be misleading.\n\nAccuracy rate: The ratio of correct classifications to the total number of instances.\n\nPrecision rate: These are the number of instances correctly predicted as defaulters to all instances predicted as defaulters, with the importance of avoiding opportunity costs due to incorrect lending decisions.\n\nRecall rate: the ratio of correctly identified true defaulters to highlight high-risk borrowers accordingly.\n\nF1-score: Harmonic mean of the precision and recall rates; it allows for a balanced view to find the model’s score, considering the effect of false positives and false negatives.\n\nAUC-ROC: It describes the model performance in terms of the capability to distinguish between defaulter and non-defaulter classes where higher values are better.\n\n2.5. Cross-Validation and Hyperparameter Tuning\n\nIn this thesis, 10-fold cross-validation will be applied to make sure that the model is generalizable for unseen data. One dataset for this purpose is further divided into ten subsets and is, in turn, trained on nine subsets and evaluated on the remaining one. In this manner, overfitting can be avoided, and the model can be generalized well to various subsets of data. The hyperparameter tuning was done through a grid search in which different combinations of hyperparameters that yield an optimum configuration were systematically tried. It ranges from modifying the number of trees in a random forest and the maximum depth of each tree to adjusting the number of hidden layers, the number of neurons per layer, and the learning rate in a neural network, making fits of model performance.\n\n2.6. Explainability and Interpretability\n\nGiven the complexity of AI models, especially neural networks, interpretability is an important challenge for financial institutions. Shapley values and LIME are two techniques that help to solve the “black box” problem of AI models by clarifying the contribution of each feature to the model prediction.\n\nShapley value: inspired by cooperative game theory, it is used to quantify the marginal contribution of each feature to the model output.\n\nLIME: approximates the behavior of complex models by constructing simplified interpretable models that revolve around individual predictions.\n\nThe integration of these interpretable techniques not only improves the accuracy of the model, but also ensures that the model is transparent and interpretable, enabling GSEs to make lending decisions that are both informed and interpretable, with results that are easily understood by regulators and customers.\n\n3. Results and Analysis\n\n3.1. Model Performance Comparison\n\nThis section introduces the comparative performance of the Random Forest, Support Vector Machine, Neural Networks, and baseline Logistic Regression models for credit risk assessment. Each model was evaluated for accuracy, precision, recall, F1 score, and AUC-ROC, which refers to the Area Under the Receiver Operating Characteristic Curve.\n\nDuring the analysis, the Random Forest model yielded the best performance with 93% accuracy and a mean AUC-ROC score of 0.94. Its high capability to deal with nonlinear relationships in big, complicated datasets and its robustness when structured data, such as social media activities or transaction history, give it a special place in credit default prediction.\n\nThe neural network models fared equally well by achieving the accuracy of 91% with an AUC-ROC score of 0.92. The strong points of the methods are treating high volume data to capture nonlinear interactions among features and allowing both structured and unstructured data inclusion.\n\nSupport Vector Machine showed worse results when the accuracy was 89%, and the AUC-ROC score equaled 0.88. It could be useful in a case if there is a small portion of defaulters compared with the overall population, and its strong classification capabilities will be helpful to identify a high-risk class of borrowers (Goodell et al., 2021).\n\nWhile the traditional credit risk assessments applied the LR baseline model, it resulted in the poorest performance among the models, where its accuracy was 84% and AUC-ROC was 0.79. Some of the limitations of LRs include reliance on linear assumptions, making it not that effective to capture the complexity in borrower behaviors.\n\nTable 1 is the summary of key performance metrics in a tabular form.\n\nTable 1. Model performance comparison table.\n\nModel\n\nAccuracy\n\nAUC-ROC\n\nPrecision\n\nRecall\n\nF1 Score\n\nRandom Forest\n\n93%\n\n0.94\n\n0.91\n\n0.88\n\n0.90\n\nNeural Networks\n\n91%\n\n0.92\n\n0.89\n\n0.86\n\n0.87\n\nSupport Vector Machine\n\n89%\n\n0.88\n\n0.85\n\n0.82\n\n0.84\n\nLogistic Regression\n\n84%\n\n0.79\n\n0.78\n\n0.74\n\n0.76\n\nPerformance of Visualization:\n\nROC Curves: The ROC curve of each model has been plotted to visually compare the strength of each model in classifying between defaulters and non-defaulters. Of these, the Random Forest had the maximum AUC, thus becoming the most powerful predictive model.\n\nPrecision-Recall Curves: Since credit risk datasets are usually imbalanced, precision-recall curves will give insight into how each model will perform to minimize false positives and false negatives. Random Forests and Neural Networks performed particularly well in maintaining high precision while finding the right balance in terms of recall.\n\nConfusion Matrix: The confusion matrix for each model provides different values of true positives, those respective defaulters who were correctly identified as true negatives, false positives, and false negatives that give detailed insight into model performance in a real-world setting.\n\nThese analyses pinpoint the potential of AI models, especially Random Forest and Neural Networks, which, by treating complex borrower behavior and using sources of structured and unstructured data, can outperform traditional methods such as Logistic Regression.\n\n3.2. Alternative Data Sources—Effectiveness\n\nThe key insight from this research is the exceptional performance gain achieved by incorporating additional data sources, such as social media activity, transaction history, and geolocation data. These alternative data sources gave a better context on how borrowers behave; this was especially true for those who did not have any formal credit history in the first place (Fernandez, 2019). In the case of social media analysis, for instance, good sentiments reflected a minimal risk of default. Other indicative factors showed that borrowers with stable high-frequency transactions have a lower risk of default, again proving the predictive power of alternative data.\n\nThe Random Forest and Neural Network models benefited the most from all the alternative data sources (Zhang & Yu, 2024). Because both can manage structured and unstructured data, they can notice complex patterns in borrower behavior, which the Logistic Regression model couldn’t (Addo et al., 2018). In contrast, the RF model used the frequency and volume of transactions as a significant measure of stability, while NN models leveraged social media activities and transaction behavior to build far more accurate default predictions.\n\n3.3. Explainability and Transparency of Artificial Intelligence Models\n\nWhile the models Random Forest and Neural Network outperformed Logistic Regression on predictive performance, their opaqueness is yet a big concern in highly regulated sectors like finance. According to Fernandez (Fernandez, 2019), financial institutions should be able to explain the rationale behind credit decisions; therefore, explainability is a crucial factor when considering AI model adoption. This has been accomplished through Shapley values and LIME in this work, Local Interpretable Model-Agnostic Explanations, by Goodell et al. (Goodell et al., 2021).\n\nShapley values provided insights into the contributions of individual features, such as debt-to-income ratio, payment history, and transaction frequency, to model predictions. For instance, debt-to-income ratio was one of the most influential variables in predicting default with continuity in most applications. LIME was used in specific instances to explain, for instance, why a particular borrower had been classified as high-risk. This at least allows more insight into the decision-making process.\n\nWhile these explanation techniques brought much-needed transparency, they added even more complexity. The trade-off between model complexity and explainability remains among the most important challenges that financial institutions need to address to ensure that regulatory bodies are satisfied and trust is instilled in AI-driven credit assessments.\n\n3.4. Bias and Fairness in Artificial Intelligence Models\n\nHowever, their use in credit risk management has raised several concerns regarding algorithmic bias and fairness. In general, AI models are subject to learning potential biases from the data used for training to disadvantage one demographic group or another. For instance, even though discriminatory on lending issues, AI models could practice it if they had been biased in the beginning against some groups, even if sensitive attributes like race or gender are excluded from the dataset.\n\nThe principle of FTU compliance in this work eliminated the bias by not considering sensitive variables in the models. However, even with FTU, biases are still possible, like proxy variables of location or employment sector. In line with this, the calculations of fairness metrics were performed for demographic parity and equal opportunity, which are used to determine whether models make equitable predictions across different demographic groups. Fairness constraints increased equity but slightly reduced performance in the Neural Network model (Thakkar, & Chaudhari, 2021). This reveals that fairness is a competing factor against accuracy.\n\n3.5. Comparison of Artificial INTELLIGENCE and Traditional Model\n\nOverall, the findings of this study confirm that AI models outperform traditional logistic regression in credit risk evaluation. The performance of the Random Forest and Neural Network models was consistently better than logistic regression in all the individual metrics, especially when exploiting nonlinear relationships and alternative data sources.\n\nWhile logistic regression is still appreciated for its simplicity and transparency, its reliance on linear relationships limits its performance on more complex, real-world credit risk scenarios. The Support Vector Machine model balances accuracy and interpretability well without being as complex as Neural Networks, thus making it implementable for financial institutions that want better performance without going all the way to black-box models.\n\nThe findings reveal that it is high time financial institutions adopt more advanced AI-driven systems; otherwise, they would likely be excluded from contemporary finance. Nevertheless, the problems of model transparency and fairness, together with assurance of conformation with regulatory requirements, must be addressed so that AI can deploy its duties responsibly in credit risk assessment.\n\n4. Discussion\n\n4.1. Implication of the Findings for Credit Risk Assessment\n\nThese results indicate benefits of using AI and ML models in evaluating credit risk compared to traditional approaches. The two AI models, Forests and Neural Networks, are significantly better and can improve the accuracy of the predictions in identifying complex nonlinear relationships that are easily ignored by traditional techniques, such as those based on logistic regression. Indeed, prior studies have established that these AI models can use alternative data, such as how active a credit-seeker has been on social media, his transaction history, and geolocation, to arrive at more accurate risk assessments (Fernandez, 2019).\n\nOne key takeaway is that those financial institutions using AI-based credit risk models gain a much deeper understanding of the behavior of borrowers. Integrating unstructured data offers better predictions of creditworthiness for people who either have a thin or no formal credit history. This is particularly important in developing markets and among gig economy workers, who typically do not have access to traditional credit systems because of a lack of structured financial data.\n\n4.2. Challenges and Limitations of Artificial Intelligence Models\n\nConversely, several challenges dominate deploying AI and ML models to mainstream adoption in the financial industry. The most important of these is the inherent complexity of deep learning models, which requires several computational resources and technical skills for modeling, deployment, and maintenance substantial enough to make smaller institutions struggle to compete with large organizations able to invest more energy in state-of-the-art AI solutions.\n\nAnother critical issue with these AI models is their lack of transparency. That is why traditional models, like Logistic Regression, have remained preferable because they are easy to interpret and explain the decisions, both to a regulator and a customer who has been rejected. It is true that deep learning models have emerged as “black boxes”, and it becomes challenging to justify credit decisions, especially in cases of denial to applicants. This causes severe complications regarding regulatory issues, such as adhering to a framework that needs to be set by the General Data Protection Regulation for explainability when a decision is made.\n\nThis paper tried to overcome these challenges by incorporating Shapley values to explain model predictions in a transparent manner without sacrificing predictive power. Goodell et al. (Goodell et al., 2021) have proposed LIME as a method to derive locally interpretable models for individual predictions, increasing the potential for transparency such that institutions may give clear justifications for their credit decisions, as stated by (Bussmann et al., 2020). In any case, implementing these explainability techniques requires additional computational effort, adding to the complexity of adopting AI.\n\n4.3. Some Ethics to Consider: Bias and Fairness in Artificial Intelligence Models\n\nCredit risk assessment always faces the algorithmic bias issue from deployed AI models. The model can inherit biases leading to disparity in outcomes, as most models are trained with historical data. For instance, a model that has been trained on some discriminatory practices of the past might keep discriminating against certain groups, even when sensitive attributes like race or gender are removed. This will be especially detrimental in regulated industries where equity and fairness are core ends.\n\nThe researchers in this work have excluded sensitive variables from the dataset, using a principle called Fairness Through Unawareness. However, there is still a chance that proxy features might encode sensitive information; therefore, fairness constraints during model training were necessary to make the predictions equal among demographic groups.\n\nFairness constraints imposed to enforce equity came at some cost in predictive accuracy, especially for the Neural Network model. This depicts the challenge of trading off fairness against performance, an issue now at the forefront of research. Organizations must ensure that their AI models comply with the legislation enacted to eradicate discrimination, such as the U.S. Fair Credit Reporting Act and the Equality Act in the UK.\n\n4.4. Future Research Directions\n\nResults from this work have also pointed out some directions for future research and development concerning the applications of AI models in credit risk assessment. Hybrid model development will be one very promising avenue that combines the predictive power of AI with the interpretability of traditional models. By integrating AI techniques with Logistic Regression or decision trees, financial institutions can have the best of both worlds: accuracy from AI and transparency for regulatory compliance. Hybrid systems could allow real practicality for financial institutions that might be uneasy about thoroughly implementing black-box models like Neural Networks.\n\nAnother exciting direction is improving fair-aware AI models. Current approaches, such as Fairness Through Unawareness, are a good starting point for which much future research needs to develop sophisticated techniques that tend to reduce bias without performance compromise. Techniques such as adversarial de-biasing or representation learning for fairness might hold promise for mitigating bias with at least loss in model accuracy.\n\nThen, there is a valid ethical basis upon which this research into the effects of alternative data sources in credit risk assessment can be pursued. Although social media activities and transaction histories present quite valuable advantages concerning improvement in credit risk predictions, data privacy and regulatory compliance issues implicate essential considerations, especially under rigid data protection laws such as the GDPR, as identified by (Goodell et al., 2021). Projects soon must be considered considering the impact of more privacy-preserving AI techniques, federated learning, and differential private ways to train AI models from decentralized data without touching borrower privacy.\n\nFinally, future studies need to be directed at enhancing the explainability of AI models, since they are still opaque despite the high accuracy of deep learning models. New developments in explainability techniques which can make complex models more interpretable would thus be crucial in developing AI-driven credit assessments that are transparent, trustworthy, and adherent to regulatory standards.\n\n5. Conclusions\n\nThe present study has explored the transformative power of AI and ML in credit risk assessment by comparing various state-of-the-art techniques with conventional models like Logistic Regression. The results show more significant improvements in predictive accuracy and identification of risk features for AI models, particularly Random Forest and Neural Networks. These models outperform conventional ones by processing and integrating a large volume of structured and unstructured information to attain much deeper insights into the behavior of borrowers (Breeden, 2020; Addo et al., 2018).\n\nAnother key takeaway is that incorporating alternative data sources, such as social media activity, transaction histories, and geolocation data, improves the performance of AI models. Moreover, a better breakthrough of predictive power is achieved that assists institutions in making more accurate estimates of credit risk, especially for people who have limited formal credit records. These AI models synthesize these sources of data in a way that would not otherwise be available for evaluating borrower risk, hence making the models valuable for underrepresented or emerging market borrowers.\n\nHowever, there are some issues that AI models need to go through before they can earn inconspicuous acceptance. First, transparency in the decision-making process, such as Neural Networks, is imperative due to its lack. Their black-box nature antagonizes explanations of how the model derived a particular decision, which is inappropriate for regulated industries. This research used Explainable AI techniques, including Shapley values and LIME, for enhanced interpretability according to feature importance and the building of overall trust in AI-driven decisions.\n\nIt is also important to consider bias and equity. Models built on historical data run the risk of perpetuating bias, leading to discriminatory outcomes. Fairness Through Unawareness and fairness constraints are among the various techniques applied in this study, but more research needs to be done to develop methods that reduce bias with minimal impact on performance. Showing compliance with regulatory standards goes hand in hand with responsible AI adoption.\n\n5.1. Future of AI in Credit Risk Assessment\n\nWhile the future of AI in credit risk assessment has a promising direction, several key considerations must be met to ensure that this technology is used responsibly and effectively. One auspicious direction is the development of hybrid models that take advantage of the predictive power of AI but combine such powers with the interpretability of traditional models. Adding AI techniques to the models with Logistic Regression or decision trees may give some accuracy of AI but retain the transparency to satisfy regulatory compliance. Hybrid models can thus provide a practical alternative for financial institutions that may be quite apprehensive about their use.\n\nAnother interesting path might be investigating the development of eventual fairness-aware AI models that can reduce bias without sacrificing accuracy. Techniques like adversarial de-biasing and fair representation learning are promising methods for mitigating bias with minimal model performance compromise. Future research should investigate such techniques within the context of credit risk assessment, especially regarding their effectiveness in reducing bias in real-world applications.\n\nAnother important direction of research is related to the ethical impact of using alternative data sources in credit risk assessment. While alternative data offers advantages in making credit risk predictions, it also raises major concerns about data privacy and regulatory issues, especially with strict data protection laws such as GDPR. Further research is warranted, which would apply newly developed AI privacy-preserving techniques, including federated learning and differential privacy, to enable the training of an AI model on decentralized data without violation of the borrower’s privacy.\n\nExplainability in AI models is a key element of any responsible use of AI within credit risk evaluation. While the Shapley values and LIME are indeed especially useful tools to explain the decisions of a model, much remains to be undertaken regarding the development of more sophisticated techniques of explainability that may render complex models, such as Neural Networks, more transparent. As AI continues to develop, it will be relevant to ensure that models remain interpretable, trustworthy, and comply with regulatory standards (Breeden, 2020).\n\n5.2. Realistic Recommendations to Financial Institutions\n\nBased on the findings of this study, the following are some practical recommendations that any financial institution willing to adopt AI in its credit risk assessment processes can consider. First, institutions should consider the adoption of hybrid models that provide a balancing act between the accuracy of AI and the interpretability of traditional models. These models can be a workable solution for organizations that have to comply with regulatory requirements while leveraging the predictive power of AI.\n\nSecond, fairness and equity should be inherent concerns of any AI model within financial institutions. This means periodic auditing of models for bias elimination, constraining models at development for fairness, and adherence to relevant regulations on the subject. Institutions should also be open towards borrowers regarding data considered during credit assessment and give meaningful reasons if adverse decisions are being passed.\n\nThirdly, institutions should consider the use of alternative data sources to populate their credit risk profiles. This will be particularly important in the case of borrowers with limited or no formal credit history. Nonetheless, this must be pursued while giving full respect to borrowers’ privacy and keeping in mind the needs of data protection regulations such as the GDPR. Privacy-preserving AI techniques, such as federated learning, should be considered to mitigate the risks associated with using alternative data.\n\nThe second is that it must make investments in explainable AI techniques, which shall help and work towards more transparency of the AI models that one uses. These techniques can include Shapley values and LIME in enabling institutions to explain their credit decisions to regulators and customers and therefore further build trust and ensure that regulatory standards are met. With increasing complexity, AI models would require the clear and interpretable development of institutions that can be trusted by all stakeholders.\n\n5.3. Future Directions and Considerations\n\nAI and ML represent the future of credit risk assessment: more enabling correct decisions, lower default rates, and increased credit accessibility (Milojević & Redzepagic, 2021). Minimizing transparency, bias, and fairness challenges, along with compliance issues, paves the path for successful adoption. Successful implementation of hybrid models, fairness-aware approaches, and techniques for explainable AI will enable institutions to harness all the benefits of AI while never compromising on ethical grounds or regulatory compliance.\n\nWhile much fair, private, and explainable AI models are developed today, with the rapid development of AI, future research in this area should be channeled to make AI models fairer, more private, more explainable for a truly more equitable financial system that exists between borrowers and lenders.\n\nConflicts of Interest\n\nThe author declares no conflicts of interest regarding the publication of this paper.\n\nReferences\n\nCopyright © 2025 by authors and Scientific Research Publishing Inc.\n\nThis work and the related PDF file are licensed under a Creative Commons Attribution 4.0 International License.\n",
        "Machine learning models employed in Customer Segmentation and Targeting": "Implementing Customer Segmentation Using Machine Learning ... - Neptune : \nShow off your best (or worst!) learning curve for a chance to win a custom-made gift box 🎁 → Learn more\n\nImplementing Customer Segmentation Using Machine Learning [Beginners Guide]\n\nThese days, you can personalize everything. There’s no one-size-fits-all approach. But, for business, this is actually a great thing. It creates a lot of space for healthy competition and opportunities for companies to get creative about how they acquire and retain customers.\n\nOne of the fundamental steps towards better personalization is customer segmentation. This is where personalization starts, and proper segmentation will help you make decisions regarding new features, new products, pricing, marketing strategies, even things like in-app recommendations.\n\nBut, doing segmentation manually can be exhausting. Why not employ machine learning to do it for us? In this article, I’ll tell you how to do just that.\n\nWhat is customer segmentation\n\nCustomer segmentation simply means grouping your customers according to various characteristics (for example grouping customers by age).\n\nIt’s a way for organizations to understand their customers. Knowing the differences between customer groups, it’s easier to make strategic decisions regarding product growth and marketing.\n\nThe opportunities to segment are endless and depend mainly on how much customer data you have at your use. Starting from the basic criteria, like gender, hobby, or age, it goes all the way to things like “time spent of website X” or “time since user opened our app”.\n\nThere are different methodologies for customer segmentation, and they depend on four types of parameters:\n\nGeographic customer segmentation is very simple, it’s all about the user’s location. This can be implemented in various ways. You can group by country, state, city, or zip code.\n\nDemographic segmentation is related to the structure, size, and movements of customers over space and time. Many companies use gender differences to create and market products. Parental status is another important feature. You can obtain data like this from customer surveys.\n\nBehavioral customer segmentation is based on past observed behaviors of customers that can be used to predict future actions. For example, brands that customers purchase, or moments when they buy the most. The behavioral aspect of customer segmentation not only tries to understand reasons for purchase but also how those reasons change throughout the year.\n\nPsychological segmentation of customers generally deals with things like personality traits, attitudes, or beliefs. This data is obtained using customer surveys, and it can be used to gauge customer sentiment.\n\nAdvantages of customer segmentation\n\nImplementing customer segmentation leads to plenty of new business opportunities. You can do a lot of optimization in:\n\nLet’s discuss these benefits in more depth.\n\nNobody likes to invest in campaigns that don’t generate any new customers. Most companies don’t have huge marketing budgets, so that money has to be spent right. Segmentation enables you to target customers with the highest potential value first, so you get the most out of your marketing budget.\n\nCustomer segmentation helps you understand what your users need. You can identify the most active users/customers, and optimize your application/offer towards their needs.\n\nProperly implemented customer segmentation helps you plan special offers and deals. Frequent deals have become a staple of e-commerce and commercial software in the past few years. If you reach a customer with just the right offer, at the right time, there’s a huge chance they’re going to buy. Customer segmentation will help you tailor your special offers perfectly.\n\nThe marketing strategy can be directly improved with segmentation because you can plan personalized marketing campaigns for different customer segments, using the channels that they use the most.\n\nBy studying different customer groups, you learn what they value the most about your company. This information will help you create personalized products and services that perfectly fit your customers’ preferences.\n\nIn the next section, we’re going to discuss why machine learning for customer segmentation.\n\nMachine Learning for customer segmentation\n\nMachine learning methodologies are a great tool for analyzing customer data and finding insights and patterns. Artificially intelligent models are powerful tools for decision-makers. They can precisely identify customer segments, which is much harder to do manually or with conventional analytical methods.\n\nThere are many machine learning algorithms, each suitable for a specific type of problem. One very common machine learning algorithm that’s suitable for customer segmentation problems is the k-means clustering algorithm. There are other clustering algorithms as well such as DBSCAN, Agglomerative Clustering, and BIRCH, etc.\n\nWhy would you implement machine learning for customer segmentation?\n\nMore time\n\nManual customer segmentation is time-consuming. It takes months, even years to analyze piles of data and find patterns manually.  Also if done heuristically, it may not have the accuracy to be useful as expected.\n\nCustomer segmentation used to be done manually and wasn’t too precise. You’d manually create and populating different data tables, and analyze the data like a detective with a looking glass. Now, it’s much better (and relatively easy thanks to rapid progress in ML) to just use machine learning, which can free up your time to focus on more demanding problems that require creativity to solve.\n\nEase of retraining\n\nCustomer Segmentation is not a “develop once and use forever” type of project. Data is ever-changing, trends oscillate, everything keeps changing after your model is deployed. Usually, more labeled data becomes available after development, and it’s a great resource for improving the overall performance of your model.\n\nThere are many ways to update customer segmentation models, but here are the two main approaches:\n\nBetter scaling\n\nMachine learning models deployed in production support scalability, thanks to cloud infrastructure. These models are quite flexible for future changes and feedback. For example, consider a company that has 10000 customers today, and they’ve implemented a customer segmentation model. After a year, if the company has 1 million customers, then ideally we don’t need to create a separate project to handle this increased data. Machine Learning models have the inherent capability to handle more data and scale in production.\n\nHigher accuracy\n\nThe value of an optimal number of clusters for given customer data is easy to find using machine learning methods like the elbow method. Not only the optimal number of clusters but also the performance of the model is far better when we use machine learning.\n\nRead also\n\nF1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?\n\nExploring customer dataset and its features\n\nLet’s analyze a customer dataset. Our dataset has 24,000 data points and four features. The features are:\n\nIn the upcoming section, we’ll pre-process this dataset.\n\nPre-processing the dataset\n\nBefore feeding the data to the k-means clustering algorithm, we need to pre-process the dataset. Let’s implement the necessary pre-processing for the customer dataset.\n\nMoving on, we’ll implement our k-means clustering algorithm in Python.\n\nMight be useful\n\nA Comprehensive Guide to Data Preprocessing\n\nImplementing K-means clustering in Python\n\nK-Means clustering is an efficient machine learning algorithm to solve data clustering problems. It’s an unsupervised algorithm that’s quite suitable for solving customer segmentation problems. Before we move on, let’s quickly explore two key concepts\n\nUnsupervised Learning\n\nUnsupervised machine learning is quite different from supervised machine learning. It’s a special kind of machine learning algorithm that discovers patterns in the dataset from unlabelled data.\n\nUnsupervised machine learning algorithms can group data points based on similar attributes in the dataset. One of the main types of unsupervised models is clustering models.\n\nNote that, supervised learning helps us produce an output from the previous experience.\n\nClustering algorithms\n\nA clustering machine learning algorithm is an unsupervised machine learning algorithm. It’s used for discovering natural groupings or patterns in the dataset. It’s worth noting that clustering algorithms just interpret the input data and find natural clusters in it.\n\nSome of the most popular clustering algorithms are:\n\nIn the following section, we’re going to analyze the customer segmentation problem using the k-means clustering algorithm and machine learning. However, before that, let’s quickly discuss why we’re using the k-means clustering algorithm.\n\nWhy use K-means clustering for customer segmentation?\n\nUnlike supervised learning algorithms, K-means clustering is an unsupervised machine learning algorithm. This algorithm is used when we have unlabelled data. Unlabelled data means input data without categories or groups provided. Our customer segmentation data is like this for this problem.\n\nThe algorithm discovers groups (cluster) in the data, where the number of clusters is represented by the K value. The algorithm acts iteratively to assign each input data to one of K clusters, as per the features provided. All of this makes k-means quite suitable for the customer segmentation problem.\n\nGiven a set of data points are grouped as per feature similarity. The output of the K-means clustering algorithm is:\n\nAt the end of implementation, we’re going to get output such as a group of clusters along with which customer belongs to which cluster.\n\nFirst, we need to implement the required Python libraries as shown in the table below.\n\nWe’ve imported the pandas, NumPy sklearn, plotly and matplotlib libraries. Pandas and NumPy are used for data wrangling and manipulation, sklearn is used for modelling, and plotly along with matplotlib will be used to plot graphs and images.\n\nAfter importing the library, our next step is to load the data in the pandas data frame. For this, we’re going to use the read_csv method of pandas.\n\nAfter loading the data, we need to define the K- means model. This is done with the help of the KMeans class that we imported from sklearn, as shown in the code below.\n\nAfter defining the model, we want to train is using a training dataset. This is implemented with the use of the fit method, as shown in the code below.\n\nNote that we’re passing three features to the fit method, namely products_purchased, complains, and money_spent.\n\nThough we have trained a K-means model up to these points, we haven’t found the optimal number of clusters required in this case of customer segmentation. Finding the optimal number of clusters, for the given dataset is important for producing a high-performant k-means clustering model.\n\nIn the upcoming section, we’re going to find the optimal number of clusters of the given dataset and then re-train the k-means clustering model with these optimal values of k. This will produce our final model.\n\nFinding the optimal number of clusters\n\nFinding the optimal number of clusters is one of the key tasks when implementing a k-means clustering algorithm. It’s worth noting that a k-means clustering model might converge for any value of K, but at the same time, not all values of K will produce the best model.\n\nFor some datasets, data visualization can help understand the optimal number of clusters, but this doesn’t apply to all datasets. We have a few methods, such as the elbow method, gap statistic method, and average silhouette method, to assess the optimal number of clusters for a given dataset. We’ll discuss them one by one.\n\nWe’re going to use the elbow method. The K-means clustering algorithm clusters data by separating given data points in k groups of equal variances. This effectively minimizes a parameter named inertia. Inertia is nothing but within-cluster sum-of-squares distances in this case.\n\nWhen we use the elbow method, we gradually increase the number of clusters from 2 until we reach the number of clusters where adding more clusters won’t cause a significant drop in the values of inertia.\n\nThe stage at this number of clusters is called the elbow of the clustering model. We’ll see that in our case it’s K =5.\n\nFor implementing the elbow method, the below function named “try_different_clusters” is created first. It takes two values as input:\n\nThe method try_different_clusters is called using the below code, where we pass the values of K from 1 to 12 and calculate the inertia for each value of k.\n\nUsing the below code, we plot the value of K (on the x-axis) against corresponding values of inertia on the Y-axis.\n\nWe can generate the below plot using the above code. The elbow of the code is at K=5. We have chosen 5 as if we increase the number of clusters to more than 5, there is very small change in the inertia or sum of the squared distance.\n\nOptimal value of K = 5\n\nThe stage at which the number of clusters is optimal is called the elbow of the clustering model. For example, in the below image, the elbow is at five clusters (K =5). Adding more than 5 clusters will cause the creation of an inefficient or less performant clustering model.\n\nAs discussed before, we need to train the k-means clustering model again with the optimal number of clusters found.\n\nNote that we’re using the fit_predict method to train the model.\n\nIn the next section, we’re going to discuss how to visualize customer segmentation clusters in three dimensions.\n\nVisualizing customer segments\n\nIn this section, we’ll be implementing some code using plotly express. This way we’ll visualize the clusters in three dimensions, formed by our k-means algorithm. Plotly express is a library based on plotly that works on several types of datasets and generates highly-styled plots.\n\nFirst, let’s add a new column named ‘clusters’ to the existing customer data dataset. This column will be able to tell which customer belongs to what cluster.\n\nNote that we’re using NumPy expm1 methods here. NumPy expm1 function returns the exponential value of minus one for each element given inside a NumPy array as output. Therefore, the np.expm1 method accepts arr_name and out arguments and then returns the array as outputs.\n\nAfter adding the new column, named clusters, the customer data dataset will look as below.\n\nFinally, we’re going to use the below code to visualize the five clusters created. This is done using plotly with the express library.\n\nPlotly is a Python library used for graphing, statistics, plotting, and analytics. This can be used along with Python, R, Julia, and other programming languages. Plotly is a free and open-source library.\n\nPlotly Express is a high-level interface over Plotly, that works on several types of datasets and generates highly-styled plots.\n\nThe plotly.express class has functions that can produce entire figures in one go. Generally, it’s referred to as px. It’s worth noting plotly express is the built-in module of the plotly library. This is the starting point of creating the most common plots as recommended. Note that each plotly express function creates graph objects internally and returns plotly.graph_objects.\n\nA graph created by a single method call using plotly express can be also created using graph objects only. However, in that case, it takes around 5 to 100 times as much code.\n\nAs the 2D scatter plot, px.scatter plots individual data in a two-dimensional space, and the 3D method px.scatter_3d plots individual data in a three-dimensional space.\n\nVisualization of clusters of data points is very important. Various edges of the graph provide a quick view of the complex input data set.\n\nConclusion\n\nIt’s not wise to serve all customers with the same product model, email, text message campaign, or ad. Customers have different needs. A one-size-for-all approach to business will generally result in less engagement, lower-click through rates, and ultimately fewer sales. Customer segmentation is the cure for this problem.\n\nFinding an optimal number of unique customer groups will help you understand how your customers differ, and help you give them exactly what they want. Customer segmentation improves customer experience and boosts company revenue. That’s why segmentation is a must if you want to surpass your competitors and get more customers. Doing it with machine learning is definitely the right way to go.\n\nIf you made it this far, thanks for reading!\n\nWas the article useful?\n\nMore about Implementing Customer Segmentation Using Machine Learning [Beginners Guide]\n\nExplore more content topics:\n\nTop articles, case studies, events (and more) in your inbox every month.\n\nGet Newsletter\n\nCopyright © 2025 Neptune Labs. All rights reserved.\n\nCreating Customer Segmentation Using Machine Learning: A Complete Guide ... : \nCreating Customer Segmentation Using Machine Learning: A Complete Guide 2024\n\nIn today’s competitive market, understanding your customers is crucial for delivering personalized experiences and maximizing growth. One of the most effective ways to achieve this is through customer segmentation, which involves dividing your customer base into distinct groups based on common characteristics. With the rise of machine learning, this process has become more sophisticated, enabling businesses to gain deeper insights and optimize their strategies.\n\nIn this article, we’ll explore customer segmentation using machine learning, focusing on how it works, the different types of segmentation, and how to create effective models.\n\nWhat is Customer Segmentation?\n\nCustomer segmentation is the practice of categorizing customers into groups that share similar traits or behaviors.\n\nThe goal is to tailor marketing strategies, product offerings, and communications to each segment, leading to more effective targeting and improved customer satisfaction. Traditionally, customer segmentation was based on simple criteria such as demographics or purchase history.\n\nHowever, with the advent of machine learning, segmentation can now be data-driven, uncovering more complex patterns and customer behaviors.\n\nTypes of Customer Segmentation\n\nThere are several types of customer segmentation, each offering unique insights:\n\nEach type of segmentation serves a different purpose, allowing you to tailor your customer segmentation strategy based on specific goals and data availability.\n\nCustomer Segmentation Models Using Machine Learning\n\nCustomer segmentation models automatically leverage machine learning algorithms to automatically group customers based on shared attributes. One of the most popular unsupervised learning techniques used for this purpose is clustering. Clustering allows the algorithm to group similar data points together without predefined categories. A common algorithm for this is K-Means Clustering.\n\nBy using machine learning algorithms, you can automatically discover customer segments based on various factors such as purchasing behavior, engagement levels, or product preferences.\n\nCustomer Segmentation Examples\n\nTo illustrate the power of machine learning in customer segmentation analysis, let’s consider a few practical examples:\n\nHow to Create a Customer Segmentation Strategy with Machine Learning\n\nBuilding a customer segmentation strategy using machine learning involves several key steps:\n\nBenefits of Machine Learning-Based Customer Segmentation\n\nLeveraging Machine Learning for Better Segmentation\n\nCustomer segmentation using machine learning offers a data-driven approach to understanding and targeting your customers more effectively. By blending data, normalizing it, and applying unsupervised learning algorithms like K-Means, you can uncover valuable insights that traditional methods might miss. This results in more precise marketing strategies, improved customer experiences, and ultimately, higher ROI.\n\nReady to optimize your customer segmentation strategy? Integrating machine learning into your approach allows you to create highly personalized campaigns that resonate with each unique customer segment and drive better business results.\n\nWant bang-up emails in your inbox? ⚡️\n\nPropensity to Buy Algorithms: Technical Foundations and Applications in E-Commerce\n\nPropensity to Buy Algorithms: Transforming Retail and Fashion with Data-Driven Insights\n\nAbout\n\nWork With Me\n\nClients\n\nBlog\n\nWorkflows\n\nContact\n\nServices\n\nFree RAG & Workflows\n\nFree Template Looker Studio for Data Analysis\n\nPower Hours\n\nEmail Marketing Automation & AI\n\nAI Growth Program\n\nPropensity to Buy Algorithms: Technical Foundations and Applications in E-Commerce\n\nPropensity to Buy Algorithms: Transforming Retail and Fashion with Data-Driven Insights\n\nAlgoritmi di Propensity to Buy: Una Panoramica Completa\n\nEmbedding customized product recommendations in E-Commerce\n\nReal-World applications of RAG in Retail: drive customized product recommendation system\n\nWhat is RAG pipeline?\n\nHow does RAG personalize customer experiences in retail\n\nCome incrociare i dati degli acquisti con le email dei clienti per migliorare il ROAS\n\nCookie Policy\n\nPrivacy Policy\n\nNotice\n\nWe and selected third parties use cookies or similar technologies for technical purposes and, with your consent, for experience, measurement and “marketing (personalised ads)” as specified in the cookie policy.\n\nYou can freely give, deny, or withdraw your consent at any time by accessing the preferences panel. Denying consent may make related features unavailable.\n\nUse the “Accept” button to consent. Use the “Reject” button or close this notice to continue without accepting.\n\nCustomizing Marketing Strategies with ML-Driven Customer Segmentation : \nCustomizing Marketing Strategies with ML-Driven Customer Segmentation\n\nIntroduction\n\nThe rise of machine learning (ML) technologies has revolutionized the landscape of marketing strategies. Companies today have access to vast amounts of data that lead to insights far beyond traditional marketing methods. Customer segmentation, the practice of dividing a customer base into distinct groups based on various criteria, is now enhanced significantly through machine learning. By leveraging ML algorithms, businesses can gain a deeper understanding of their customers, uncover hidden patterns, and tailor marketing efforts uniquely suited to each segment.\n\nThis article aims to delve into the importance and mechanics of using ML-driven customer segmentation to customize marketing strategies effectively. We will explore how machine learning models analyze vast datasets to identify characteristics and behaviors of different customer segments, refocusing marketing strategies based on these insights. We will discuss key methodologies, benefits, challenges, and future trends related to this advanced approach to customer-driven marketing.\n\nUnderstanding Customer Segmentation\n\nCustomer segmentation is a critical marketing strategy that allows businesses to categorize customers based on specific criteria. Traditionally, companies segmented their audiences based on demographic data such as age, gender, location, and purchase history. However, this approach often fails to capture the nuanced behaviors and preferences of individual customers. Herein lies the role of machine learning, which offers unprecedented opportunities for refining and revolutionizing how businesses approach segmentation.\n\nTypes of Customer Segmentation\n\nThere are multiple types of customer segmentation, and adopting a machine learning framework enables businesses to tackle complex segmentation more effectively. The commonly recognized categories include:\n\nDemographic Segmentation: This is the most basic form and involves segmenting customers based on easily identifiable metrics such as age, gender, income level, education, and occupation. While these metrics are essential, they often do not reveal the underlying motivations driving customer behavior.\n\nGeographic Segmentation: This focuses on segmenting customers based on their geographical location. Understanding regional preferences allows marketers to customize product offerings and messaging to resonate specifically with local clientele. However, geographic segmentation may overlook other pertinent factors like interests and behaviors.\n\nBehavioral Segmentation: Perhaps one of the most critical segments, behavioral segmentation categorizes customers based on their interactions and behaviors with the brand—such as purchase history, product usage, and brand loyalty. This type can help identify high-value customers and tailor experiences based on their behavioral patterns. By employing ML algorithms, businesses can analyze this data at scale, using advanced analytics to understand behaviors dynamically.\n\nPsychographic Segmentation: This dives deeper into the personalities, values, interests, and lifestyles of customers. Although it may be less common, psychographic segmentation plays a vital role, particularly in creative marketing campaigns that resonate emotionally with customers. Combining psycho-graphic data with ML can uncover consumer motivations that traditional methods might miss.\n\nThe Role of Machine Learning in Customer Segmentation\n\nMachine learning introduces advanced analytical techniques that remove the limitations of traditional segmentation approaches. ML algorithms, particularly clustering algorithms (e.g., K-Means, hierarchical clustering), can identify customer segments based on multiple attributes and behaviors concurrently. By processing vast datasets, these algorithms can discern intricate patterns and relationships that are otherwise undetectable.\n\nA prime example of a machine learning model's application in customer segmentation is the RFM (Recency, Frequency, Monetary) approach. By analyzing how recently customers made a purchase, how frequently they buy, and the amount they spend, businesses can cluster their audience into different segments with tailored strategies. Moreover, as new data comes in, machine learning allows for continual refinement of segments without manual oversight, ensuring that marketing strategies remain relevant and effective.\n\nBenefits of ML-Driven Customer Segmentation\n\nThe application of ML-driven customer segmentation offers numerous advantages over traditional methods. Enhanced customer understanding is at the forefront, enabling businesses to offer personalized marketing experiences. With precise segments in place, companies can create targeted marketing campaigns that resonate more profoundly with different groups.\n\nEnhanced Targeting and Personalization\n\nIn a marketplace saturated with homogenized marketing messages, standing out is imperative. Utilizing ML-powered segmentation achieves extensive precision in targeting. Marketing campaigns tailored to specific customer characteristics tend to yield higher engagement rates. For instance, an online retail store can identify which segments respond best to discounts for various products and adjust their offerings accordingly, ensuring they reach the right audience at the right time.\n\nPersonalization extends beyond mere discounts; it encompasses delivering customized content, suggestions, and recommendations based on individual preferences. By examining user behavior patterns, machine learning algorithms can suggest relevant products, which not only drives sales but also enhances customer satisfaction and loyalty.\n\nIncreased Efficiency in Marketing Strategies\n\nAdopting a machine-learning approach to customer segmentation significantly increases marketing efficiency. In traditional segmentation, marketers often relied on a trial-and-error methodology to identify and reach their target audience. However, through the automatic, data-backed identification of customer segments, businesses can reduce wasted ad spend and allocate resources more effectively.\n\nMoreover, ML-driven segmentation enables ongoing optimization. As customer preferences evolve or market trends shift, machine learning models can process new data and revise segments accordingly, allowing businesses to adapt swiftly without undergoing tedious manual analysis. This agility ensures brands remain competitive and engaged with customers in a rapidly changing marketplace.\n\nImproved Customer Retention and Loyalty\n\nOrganizations that effectively employ ML-driven customer segmentation can expect to see notable improvements in customer retention and loyalty. By developing a solid understanding of their customers’ needs and tailoring marketing messages accordingly, brands foster deep emotional connections with their audience. Personalization makes customers feel valued—leading to higher purchase frequency and loyalty.\n\nAdditionally, ML algorithms can help identify customers at risk of churning by analyzing behavioral signals indicating dissatisfaction or disengagement. Here, proactive engagement strategies can be implemented, such as personalized outreach, special promotions, or opportunities for feedback, ultimately preventing customer loss.\n\nChallenges in ML-Driven Customer Segmentation\n\nWhile the potential of ML-driven customer segmentation is immense, there are challenges and hurdles that organizations must navigate. Notably, complications can arise in data collection, model training, and the ethical implications surrounding customer data privacy.\n\nData Quality and Accessibility\n\nOne of the foremost challenges is ensuring data quality and accessibility. Machine learning relies heavily on quality data, as inaccuracies or missing data points can skew insights. Organizations must invest in solid data governance practices, ensuring datasets are cleaned, normalized, and updated consistently. Furthermore, obtaining comprehensive data from disparate sources can be cumbersome, particularly for smaller businesses with limited resources.\n\nEnsuring data privacy and compliance with regulations such as GDPR is crucial, as companies need to balance effective data collection strategies with customer trust. When machine learning practices infringe on privacy, consumer backlash can occur, resulting in damaged brand reputation.\n\nExpertise and Model Complexity\n\nImplementing ML-driven segmentation also requires a level of expertise that not all companies possess. Developing effective machine learning models demands specialized skills in data science and analytics, along with familiarity with machine learning concepts and algorithms. This can create a barrier for some organizations, particularly small to medium-sized enterprises, lacking the technical expertise or budget to invest in such capabilities.\n\nMoreover, the complexity of certain machine learning algorithms might lead to challenges in interpretability. Organizations need to understand the fundamentals of how models derive conclusions to take appropriate actionable steps. Creating transparency in these processes can build trust both internally and with consumers.\n\nIntegrating with Existing Systems\n\nFinally, integrating machine learning models into existing customer relationship management (CRM) or marketing systems may prove difficult. Legacy systems may lack the infrastructure necessary to support advanced analytics, necessitating potential upgrades or replacements.\n\nSeamless integration of data sources and aligning marketing strategies with insights gleaned from ML algorithms require careful planning and implementation. Companies must also train their personnel on how to leverage these technologies effectively to witness maximum returns on investment.\n\nConclusion\n\nCustomizing marketing strategies through ML-driven customer segmentation stands as one of the most transformative approaches businesses can adopt in today's competitive landscape. Through a nuanced understanding of customer behaviors, preferences, and needs, machine learning empowers organizations to create tailored marketing campaigns that resonate meaningfully with their target audiences. This not only enhances customer engagement but also drives sales, retention, and brand loyalty.\n\nHowever, as we explored, the journey to effective segmentation is not without its challenges. Ensuring data quality, navigating technical barriers, and addressing privacy and integration concerns are all essential facets that organizations must consider. By approaching these obstacles thoughtfully and strategically, companies can fully harness the potential of ML-driven customer segmentation, positioning themselves for success in an increasingly data-centric marketing world.\n\nThe future of marketing lies in the adoption of advanced technologies, and machine learning will be at the forefront of this revolution. Businesses that embrace these innovations will not only improve their marketing efforts but also create meaningful connections with customers, setting the stage for unmatched growth and success in the years to come.\n\nIf you want to read more articles similar to Customizing Marketing Strategies with ML-Driven Customer Segmentation, you can visit the Customer Segmentation category.\n\nYou Must Read\n\nAnalyzing Factors Affecting Machine Learning Model Sizes\n\nMachine Learning: A Comprehensive Analysis of Data-driven Learning\n\nAnalyzing the Quality of AI-Generated Music: Research Insights\n\nCategories\n\nRelated Posts\n\nExploring the Role of AI in Satellite Imagery for Urban Planning\n\nExploring Ensemble Learning Methods in Clinical Applications\n\nExploring Multi-Modal Approaches in Face Recognition Applications\n\nTerms and Conditions\n\nPrivacy Policy\n\nCookie Policy\n\nAbout Us\n\nContact Us\n",
        "Machine learning models employed in Financial Performance Prediction": "Machine learning for financial forecasting, planning and analysis ... : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nMachine learning for financial forecasting, planning and analysis: recent developments and pitfalls\n\nYou have full access to this\nopen access\narticle\n\n40k Accesses\n\n1 Altmetric\n\nExplore all metrics\n\nAbstract\n\nThis article is an introduction to machine learning for financial forecasting, planning and analysis (FP&A). Machine learning appears well suited to support FP&A with the highly automated extraction of information from large amounts of data. However, because most traditional machine learning techniques focus on forecasting (prediction), we discuss the particular care that must be taken to avoid the pitfalls of using them for planning and resource allocation (causal inference). While the naive application of machine learning usually fails in this context, the recently developed double machine learning framework can address causal questions of interest. We review the current literature on machine learning in FP&A and illustrate in a simulation study how machine learning can be used for both forecasting and planning. We also investigate how forecasting and planning improve as the number of data points increases.\n\nSimilar content being viewed by others\n\nIntegrating Decision Analytics and Advanced Modeling in Financial and Economic Systems Through Artificial Intelligence\n\nApplication of Machine Learning in Financial Asset Price Prediction and Allocation\n\nData Mining in Finance: Current Advances and Future Challenges\n\nExplore related subjects\n\nAvoid common mistakes on your manuscript.\n\n1 Introduction\n\nAccurate financial forecasts and plans for effective and efficient resource allocation are core deliverables of the finance function in modern companies. Particularly, in volatile or fast-evolving market environments, fast and reliable forecasting and planning are crucial (Becker et al. 2016). High-quality forecasting is among the defining characteristics of strong finance functions (Roos et al. 2020). It is therefore hardly surprising that most larger companies have dedicated teams for financial planning and analysis (FP&A) within their finance function.\n\nThe increasing availability of big data, coupled with new analysis techniques, provides an opportunity for FP&A to generate more and better insights at a faster pace, generating more value for the company. Machine learning is a set of techniques developed in computer science and statistics that appear particularly well suited to this context. The aim of our paper is to show how machine learning can be used for FP&A and which pitfalls can arise in the process. Machine learning has been applied successfully to a variety of predictive tasks, including fraud detection and financial forecasting. Planning and resource allocation, however, represent tasks of a different nature, because they require understanding the effect of an active intervention in a system, such as the market for a product. For this reason, they are causal problems, which are harder to model with machine learning. A large field within machine learning revolves around pattern recognition. Patterns in data, based on correlations, are learned and then used for predictions. In causal tasks, an understanding of the underlying (causal) mechanisms is important when evaluating the effects of interventions (e.g., the implementation of a new business strategy). The emerging field of causal machine learning uses machine learning algorithms for such questions. For instance, the recently developed double machine learning framework reduces the impact of imperfect model specifications, which are hard to avoid in practice in the context of causal analysis.\n\nWe structure this paper as follows. In Sect. 2, we briefly review the role of FP&A. Section 3 provides a short, focused overview of machine learning. In particular, we highlight the pitfall of not distinguishing between forecasting and planning. In Sect. 4, we present the results of our literature review, which finds surprisingly few publications of machine learning applications in FP&A. In Sect. 5, we describe and provide the results from a simulation study. We compare a machine learning technique, the lasso, to a linear regression based on the ordinary least squares (OLS) method. In our analysis, we refer back to the distinction between forecasting and planning from Sect. 3, and show how the results differ between the lasso and OLS for both tasks. Finally, we also quantify the benefit of additional data in this simulation.\n\n2 The role of FP&A\n\nGiven the importance of financial forecasting, planning and analysis (FP&A) in modern corporations, most larger companies have dedicated teams for these tasks within their finance function, even though the exact organizational design and naming of the department may vary.\nFootnote\n1\n\nThe overarching goal of FP&A is to inform and support decisions of management and the board of directors (Oesterreich et al. 2019). FP&A pursues this goal via different routes, helping determine which projects in the company portfolio create value (and are consequently worth funding), and preparing company-wide forecasts and financial plans to ensure that the company can reach its financial goals in the short and long term (Roos et al. 2019). Investments in research and development (R&D) or the expansion of production capacity are balanced with financial obligations to debt holders or equity investors and tax authorities (Brealey et al. 2020). Financial plans are also an important step in the translation of a company’s strategic priorities into concrete operational actions. These actions contribute to focusing the organization and the deployed resources behind common goals.\n\nAnalyzing the business environment and business dynamics is an integral part of the work performed by FP&A. The insights generated through such analysis can inform the development of forecasts and plans and help in the assessment of how likely these plans are to succeed. During the the execution phase of plans, these insights allow FP&A to understand why actual results may deviate from the plan and to recommend corrective actions. This need for business acumen is likely to continue, even when advanced forecasting methods like those described in this article are used (Möller et al. 2020).\n\nThe time horizons considered for financial forecasts and plans usually range from 1 month to several years (Roos et al. 2019; Fischer 2009). The choice of time horizon depends on company-specific circumstances and objectives; for instance, stock-market listed companies typically put additional weight on quarterly figures. In practice, most companies create forecasts and plans for the next fiscal year (sometimes called a budget), which additionally can serve as a management control mechanism (Strauß and Zecher 2013). Rolling forecasts are another form of plan. These are characterized by regular updates, which are typically performed on a monthly or quarterly basis (Hansen 2011).\n\nFP&A relies in large part on quantitative analysis to generate forecasts and plans. Accounting systems are a major internal data source for FP&A (Garrison et al. 2006; Gray and Alles 2015), covering items related to sales (turnover), expenses, and balance sheet positions, which are especially important for cash flow analysis. Other important internal sources of data include those related to human resources (employee numbers, wage costs), supply chain and production (manufacturing costs at various levels of granularity), and R&D (product development costs, success rates, timelines).\n\nExternal data sources include market- or product-specific information, such as the size and development of the market and market shares. The exact nature and granularity of these data depends largely on the product or question under analysis, as well as the investment required to access the relevant data (Gray and Alles 2015). For instance, it is not uncommon in the consumer goods industry to have access to transaction-level data (Taddy 2019), covering one’s own and competitor products. However, information at this level of specificity is typically used by the marketing and sales department for product-specific tactics. In contrast, FP&A often uses macroeconomic indicators, including GDP, inflation and currency rates.\n\nThe development and spread of comprehensive, company-wide IT systems in recent decades has increased the amount and variety of data readily available to FP&A. Increasing digitalization will further accentuate this development, with big data as the crystallizing term. The “Three V’s”, a common framework to define big data (Laney 2001), allow us to look at the different dimensions that drive this development. First, the amount of information generated, captured and thus accessible for FP&A activities is growing (volume). Second, the speed of information creation and its accessibility is accelerating (velocity); as a consequence, the speed at which information must be analyzed and acted upon increases, too. This calls for automated, real-time analytics and evidence-based planning (Gandomi and Haider 2015). Third, more and more types of information are being gathered or generated and can be analyzed (variety); for instance, stock-market analysts apply sentiment analysis to extract information relevant to stock prices from text documents.\n\nIn addition, other dimensions of big data have been proposed (Gandomi and Haider 2015). In the context of FP&A, the additional dimensions of veracity and complexity appear especially relevant. Thanks to the more widespread use of digital tools, the need for data transparency and scrutiny within many companies is increasing, as well. In turn, the need to ensure data quality and reliability is growing (veracity). Moreover, (big) data are generated through multiple sources, both from inside and outside the company. This requires data cleaning, data matching and, ideally centralized storage, which facilitates accessibility (complexity).\n\nAs mentioned above, a key output of FP&A is financial forecasts and plans. For data that are more numerous, available more quickly, and are more diverse and of better quality than in the past, FP&A needs to choose adequate tools, such as those provided by machine learning.\n\n3 Introduction to machine learning\n\nWhile there is no uniform definition of machine learning, it can be described as a collection of methods that automatically build predictions from complex data (Taddy 2019). In essence, machine learning deploys a function-fitting strategy aiming to find a useful approximation of the function that underlies the predictive relationship between input and output data (Hastie et al. 2009). In this search for patterns in data (Bishop et al. 2006), which, to a large extent, is executed autonomously, machine learning draws on statistical tools and algorithmic approaches from computer science. In particular, machine learning aims to cope with the situation of high-dimensional data. High dimensionality occurs when the number of input variables (independent variables, features) used to predict the output (dependent) variable is large compared to the number of observations available. Classical statistical techniques do not work in this setting (Taddy 2019).\n\nThe three broad categories of machine learning are supervised learning, unsupervised learning and reinforcement learning. Supervised learning is concerned with predicting the value of an output variable based on the values of a set of input variables. For this, supervised learning relies on a set of input and output variables that are jointly observed for each data point (Hastie et al. 2009). A practical example is to predict the sales of a product using input variables such as time of the year, price level, advertising expenditures and availability of competitor products. In contrast, unsupervised learning consists only of a set of input observations for which the joint distribution is known. However, there is no observed output (response). The goal is to directly infer the properties of these observations (Hastie et al. 2009). Classifying customers into (previously unknown) customer archetypes based on their observed characteristics such as buying behavior, age, gender and socio-economic status is an example of unsupervised learning. In reinforcement learning, the algorithm performs a trial-and-error search to maximize a numeric reward signal, in direct interaction with its environment (Sutton and Barto 2018). By interacting with its environment, the algorithm creates its own data from which it can learn. Games such as checkers, chess and go are classical examples in which reinforcement learning is applied. Sometimes cited as a fourth category, semi-supervised learning falls between supervised and unsupervised learning, combining a small amount of fully labeled data as in supervised learning and a large amount of unlabeled data as in unsupervised learning. The objective is to improve supervised learning in situations in which labeled data are scarce (Zhu and Goldberg 2009). For the purposes of FP&A objectives, which mostly revolve around producing forecasts from a set of inputs and assumptions, the predominant choice is typically supervised methods.\n\nMachine learning methods appear especially suitable for the core FP&A task of forecasting because of their focus on predictive performance. These methods manage to identify generalizable patterns that work well on new data, i.e., data outside of the training sample (Mullainathan and Spiess 2017). Through their ability to identify complex structures that have not been specified in advance, they lend themselves to support a high degree of automation in the generation of forecasts. This flexibility has the additional advantage that many off-the-shelf algorithms perform surprisingly well on a variety of tasks. In addition, a large selection of machine learning algorithms are available and are technically easy to use (Mullainathan and Spiess 2017), making them attractive for practitioners.\n\nBesides forecasting, the second core task of FP&A is to provide recommendations for the design of financial plans and for potential corrective actions when deviations from plans occur. In statistical terminology, this requires causal inference techniques, which are fundamentally different from forecasting. Consider the trivial example of hotel occupancy rates and room prices (Athey 2018). High room prices coincide with high occupancy rates. Thus, price variations are strongly predictive of hotel occupancy. If the goal is to make a forecast, we do not need to be concerned with understanding why occupancy was high. However, if we want to recommend an action to increase the occupancy rate (an intervention) or imagine in retrospect what the occupancy rate would have been if the room rates had been different [“counterfactual” (Pearl and Mackenzie 2018) or “potential outcome” (Rubin 2005)], FP&A requires a causal understanding of the business dynamics. To conclude with this example, a plan consisting of a room price increase will not lead to higher occupancy. Most likely, prices have increased in the past in reaction to high demand, which was stimulated by other factors (e.g., the holiday season). While this trivial example seems obvious, it illustrates a major pitfall: many companies struggle in practice to identify truly causal measures for the effectiveness of their promotional activities. Blake et al. (2015) discuss this phenomenon in the context of large-scale field experiments conducted at the e-commerce platform eBay.\n\nFor interventional and counterfactual analysis, data-driven approaches need to produce reliable estimates for the parameters that govern the relationship between input and output variables. Machine learning algorithms are typically not built for this purpose. Historically, the machine learning community has pursued the goal of maximizing predictive performance as opposed to understanding model parameters (Taddy 2019); however, using a tool built for forecasting and assuming that it also possesses the properties required for causal inference in economic applications can be misleading (Mullainathan and Spiess 2017). Maximizing the predictive power of a model to use it for interventional analysis represents a major trap. Indeed, it may even be necessary to sacrifice predictive accuracy to arrive at a correct understanding of the relationships that are relevant for making decisions about interventions (Athey 2018). The current lack of understanding of cause–effect connections is even cited as a fundamental obstacle for machine learning by some authors (Pearl 2019). Nevertheless, many inference procedures include prediction tasks as an important step (Mullainathan and Spiess 2017). Machine learning is especially suited for this step in high-dimensional settings (Belloni et al. 2014b). The double machine learning framework (Chernozhukov et al. 2017), which we will apply in Sect. 5, allows us to take advantage of the predictive performance of machine learning algorithms when seeking solutions for causal problems.\n\n4 Literature review\n\nWe conducted a search of the literature across Google, Google Scholar and finance journals on the use of machine learning in FP&A. The use of quantitative methods in the broad field of finance has been studied intensively for close to 40 years (Ozbayoglu et al. 2020), in part because of the general availability of data in this field, the existence of many areas of implementation and the substantial economic impact of financial decisions. Our search yielded surprisingly few recent publications on the use of machine learning explicitly in FP&A and related fields. The key thrust of machine learning in finance is directed towards various applications ultimately linked to forecasting and trading financial instruments such as stocks, bonds, currencies and derivatives. Credit scoring and fraud detection are other major areas. Examples of recent surveys include Ozbayoglu et al. (2020) and Henrique et al. (2019).\n\nWe see two possible reasons for the apparent scarcity of publications on machine learning in FP&A. First, time-series forecasting has been thoroughly covered and researched for many years (De Gooijer et al. 2006). A large variety of tools for this purpose have been developed, both from an academic and theoretical perspective, as well as from the perspective of practitioners, including easy to use off-the-shelf software (Küsters et al. 2006). From a practical FP&A perspective, these tools, together with the domain knowledge of the experts working in the FP&A function, allow practitioners to arrive at results that—by and large—serve sufficiently well to meet the objective of developing financial plans. Especially, practitioners may therefore perceive machine learning as a “so-so” technology (Acemoglu and Restrepo 2018), which is not (yet) quite worth their (full) attention. Thus, the intrinsic urge to look for new tools, including machine learning, in FP&A is still less pronounced than it is, for instance, in stock-market forecasting, where even a relatively small improvement in forecasting accuracy can yield significant economic payoff. We believe that this will change with the further deployment of digitalization and the consequent increase in data availability as described above. Besides improving the precision of financial forecasts, automated forecasts driven by machine learning can also lead to a substantial reduction in costs and to increased flexibility given that the traditional process is quite labor- and time-intensive.\n\nSecond, we hypothesize the following reason for the limited number of publications on machine learning in FP&A. The initial development of artificial intelligence and machine learning methods was driven mostly by academia. Because these methods are highly relevant for industrial applications, companies (in particular in the tech field) have shown strong interest in applying and developing them further. Indeed, some of the large tech companies host their own dedicated research teams. However, the limited availability of skilled professionals represents a hurdle to fast diffusion in all corporate functions of a company. Therefore, the application of machine learning for FP&A is still rare in the finance function, even though a host of machine learning publications by the industry has already appeared in other functional areas.\nFootnote\n2 Management consultancies have also discovered the benefit of machine learning for finance and FP&A. However, their publications remain general and directional in nature (see, for instance, Balakrishnan et al. 2020; Roos et al. 2020; Tucker et al. 2017; Chandra et al. 2018).\n\nOne company that has made public its use of machine learning in FP&A in scientific papers is Microsoft Corporation. In the past several years, Microsoft appears to have followed an innovative approach with machine learning in FP&A as witnessed by three publications from its employees. One paper (Gajewar et al. 2016) compares the performance of random forests to that of traditional time-series methods such as autoregressive integrated moving average (ARIMA), error trend and seasonality (ETS, a variant of exponential smoothing) and seasonal-trend decomposition using loess (STL, another variant of smoothing) for forecasting quarterly revenues by major geographic region and at the global level up to 1 year into the future. Based on their exploratory analysis, the random forest model with a restricted number of features outperformed the traditional time-series methods and the forecasts generated by the domain experts in the Microsoft FP&A department.\n\nA second paper (Barker et al. 2018) describes a machine learning-based solution that forecasts revenue on a quarterly basis, including individual forecasts for 30 products in three different business segments. Specifically, the machine learning forecast used an elastic net, a random forest, a K-nearest-neighbor and a support vector machine. The winner model was then selected via back-testing. The forecasts generated in this way proved to be more accurate than the traditional forecasts generated by FP&A in approximately 70% of the cases. The paper cites the ability to incorporate external information (e.g., temperature as a driver for electricity demand) in regression frameworks as an advantage of these over pure (standard) time-series models. While classical time-series are good at capturing trends and seasonality, they often struggle to incorporate external data. In particular, they generally lack a regularization mechanism, leading to low out-of-sample accuracy for new forecasts, especially in high-dimensional settings. Many machine learning methods include by design mechanisms to avoid overfitting (e.g., regularization for ridge, lasso and elastic net).\n\nBarker et al. (2018) also highlight some requirements that arise from the intent to use the results of machine learning forecasts in a practical manner in a corporate setting. Traditionally, FP&A works with a point estimate, coupled with an estimation of the risks and opportunities around this mid-point. Risks and opportunities typically consist of a list of items or events that will materially impact the business results if they do not turn out as assumed in the mid-point forecast (Conine and McDonald 2017). Judgmental probability estimates provided by subject matter experts are often attached to these items, together with a quantification of the expected impact under the different scenarios.\nFootnote\n3 For forecasts generated by traditional statistical or machine learning models, prediction intervals are therefore an important element for FP&A practitioners to quantify the risk in the forecast. However, prediction intervals are not typically part of machine learning models. The solution proposed by Barker et al. (2018) consists of creating intervals from out-of-sample error distributions obtained during back-testing. Other practical requirements in a corporate environment are the need for a mostly automated solution allowing for fast forecast generation as well as the need to ensure high security standards for data storage, processing and access. Financial data such as sales and profits are highly sensitive, and companies are reluctant to release them into public cloud environments. Barker et al. (2018) explain the details of their workflow automation and security controls, which revolve around the Microsoft Azure cloud-computing platform.\n\nThe third publication (Koenecke and Gajewar 2020) evaluated deep neural networks traditionally used in natural language processing (encoder–decoder LSTMs) and computer vision (dilated convolutional neural networks) to forecast company revenues. The approach incorporated transfer and curriculum learning. For the products and time period under study in this publication, deep neural networks improved predictive accuracy compared to the company’s internal baseline, which combined traditional statistical and machine learning methods other than deep neural networks.\n\nIn another example of applied machine learning in the area of FP&A, Daimler Mobility used an undisclosed library of machine learning algorithms to generate a monthly forecast set, spanning the next 18 months and updated monthly (Unger and Rodt 2019). In this respect, the approach followed the concept of a rolling forecast. The forecasted set of values comprised key financial performance indicators that were representative of Daimler Mobility’s car rental, leasing, financing and fleet management business. According to Unger and Rodt (2019), one of the key advantages of this approach compared to the traditional way of forecasting and budgeting is the speed with which updated forecasts are available, allowing faster adoption of corrective action.\n\nThese papers all discuss modern machine learning methods for financial forecasting. In the next section, we will show that these approaches cannot be applied directly to inference problems and how the double machine learning framework overcomes this problem. A first example will illustrate the use of machine learning techniques in FP&A for forecasting. A second example will serve to illustrate the use of double machine learning for planning (inference). Finally, we will explore whether having additional data improves the results for both the forecasting and planning tasks.\n\n5 Simulation example\n\nIn this section, we provide the results of a small simulation study. The design of the simulation reflects the setting, types of data and questions that the FP&A department in a large, multinational company could face. We will start with an example in which FP&A is predominantly interested in the accuracy of sales forecasting. We will then carry this example forward into a question related to planning. In this second example, FP&A is interested in assessing the effectiveness of promotional activities in generating sales; in other words, the question of interest relates to causal inference and the answer to this question can inform decisions about resource planning. Finally, we will investigate how the results change if the FP&A department obtains additional data points for their tasks.\n\n5.1 Forecasting\n\nAssume for our stylized simulation the following setting:\nFootnote\n4 for a given month n, the FP&A department would like to forecast the sales \\(y_{n}\\) of a specific product or service. FP&A has collected monthly data over 5 years (\\(N=60\\)) for sales as well as a set of \\(P=40\\) factors or features that FP&A believes could be predictive of these sales. We represent these factors as \\(x_{p,n}\\) and the corresponding sales with \\(y_{n}\\).\nFootnote\n5 In practice, there can be a wide range of factors depending on the product or service. Examples include weather conditions and various macroeconomic indicators, but also specific customer shipment patterns or the current competitive market situation. Note that the size of the feature set can easily reach 40 plausible predictors once an initial, smaller feature set is increased due to the inclusion of transformed and newly created features. This step, called feature engineering, can include the creation of lagged variables (e.g., when the effect of the economic situation affects sales several months later) or interaction effects (e.g., when a particular weather situation coincides with a peak shipment date, nullifying or exacerbating the effect of the peak shipment date). A further example is the transformation of categorical variables into several binary values via the so-called one-hot encoding (e.g., when classifying the competitive market situation as “highly competitive”, “moderately competitive”, “not competitive” and the like).\n\nIn addition to developing a set of 40 features, FP&A measures the promotional activity carried out by the company for the product under investigation during the reference timeframe. We denote this promotional activity as \\(d_{n}\\). For the purpose of this illustrative simulation, we work with the assumption that the promotional activity can be measured using a single variable. In other words, we do not enter into promotional mix considerations with interaction effects among the different promotional tools. In practice, this single variable could be a summary measure such as the amount of money spent on promotion and advertising; another possibility for a summary measure could be the number of customer calls or minutes of customer interaction. For forecasting and planning activities performed by FP&A at aggregate levels, such as the regional, divisional or group levels, an approach like this, based on a summary measure, is sometimes applied. Extending the analysis to include several marketing variables is possible without any major changes.\n\nGiven the nature and intent of promotional activity, it appears natural for FP&A to include \\(d_{n}\\) in the list of likely predictors for the sales forecasting model. Furthermore, estimating the effect of the promotional activity on sales represents an important question for FP&A, which we will address in the second part of this section, dedicated to planning.\n\nTo evaluate the accuracy of the sales forecasts, we will follow an out-of-sample evaluation approach. Only the first four years (48 data points) are used to build and train the forecasting models. FP&A then compares the forecasts generated by the models to the actual values from the last year in the available dataset (12 data points). Note that these 12 data points have been intentionally excluded from the model creation phase. While more sophisticated training and evaluation strategies exist (e.g., rolling evaluation windows), the described approach is sufficient for the purpose of this simulation study, because the out-of-sample forecasting performance is evaluated separately for each simulation.\n\nFor our simulation, we generate the data as \\(n=1, \\ldots , N\\) independent and identically distributed (i.i.d.) draws from the following model:\n\nand\n\nwith \\(x\\sim \\mathcal {N}(0,\\Sigma )\\) where \\(\\Sigma\\) is a \\(p \\times p\\) matrix with \\(\\Sigma _{k,j}=c^{|j-k |}\\), \\(\\varepsilon \\sim \\mathcal {N}(0,2)\\) and \\(\\nu \\sim \\mathcal {N}(0,2)\\).\n\nThe second equation captures confounding, i.e., variables that are simultaneously correlated with the outcome variable and the variable of interest. By setting \\(\\alpha = 0\\), we assume that the promotional activity undertaken by the company has no effect on sales, i.e., that the promotion efforts are, in reality, a waste of resources. With \\(c = 0.3\\), we include some moderate correlation\nFootnote\n6 between features, which can be expected if several features from the same general background (e.g., macroeconomic factors) are included in the model.\n\nWe set \\(\\beta = 0\\), except for \\(\\beta _{39}\\) and \\(\\beta _{40}\\), both of which we set equal to 1. Thus, out of the 40 features included in the analysis, only two are actually related to sales. Similarly, we set \\(\\gamma = 0\\), except for \\(\\gamma _{39}\\) and \\(\\gamma _{40}\\), both of which we also set equal to 1. The two features related to sales also determine the amount of promotional activity \\(d_{n}\\).\nFootnote\n7\\(\\varepsilon\\) and \\(\\nu\\) are random error terms (so-called noise). We report results based on 1000 simulation replications.\n\nIt is important to remind ourselves that the FP&A department naturally does not know any details about this data generation process. Only an oracle would know that, in reality, solely 2 of the 40 plausible predictors are linked to sales and that the coefficient in the data-generating process is 0 for the other 38 features. This situation characterizes sparse models. In such models, only a small number of many potential predictors and/or control variables are actually relevant (Belloni et al. 2014a). Identifying them leads to a correct model specification and is the main challenge.\nFootnote\n8 Additionally, FP&A does not know that the promotional activity \\(d_{n}\\) is correlated with the two features that have non-zero coefficients with respect to sales and that the promotional activity has no influence on sales (\\(\\alpha\\), the parameter of interest, is zero). We will come back to this point when we discuss inference.\n\nWe now provide results for two forecasting approaches. Both have in common that they rely—in this case correctly—on the typical assumption of a linear relationship between the output variable Y (sales) and the full set of regressors X, which includes 40 presumably predictive features and one variable reflecting promotional activity\n\nThe first approach is a traditional linear regression based on the ordinary least squares (OLS) method. Formally, OLS optimizes the parameters in such a way as to minimize the mean squared error (MSE)\n\nwhere \\(x'_i\\hat{\\beta }\\) corresponds to the predicted sales value.\n\nThe second approach, post-lasso, is a classic machine learning technique. To estimate the coefficients, lasso uses a regularization strategy that is suited to high-dimensional problems in which the number of predictors exceeds or approaches the number of observations, as is the case in our simulation. In the first step, the lasso regression is performed. In the second (i.e., post-lasso) step, the method fits OLS on the coefficients selected in the first step. Formally, lasso optimizes the parameters to minimize MSE subject to a penalty for using parameters\n\nwhere \\(x'_i\\hat{\\beta }\\) corresponds again to the predicted sales value.\n\nThe key difference between the lasso and OLS is that lasso minimizes a penalized MSE, in which the penalty amount corresponds to the absolute amount of each parameter included in the model, scaled by the tuning- or hyperparameter \\(\\lambda\\)\n\nA detailed discussion of the theory behind regularization approaches would go beyond the scope of this article. Readers are referred, among many possible sources, to Hastie et al. (2009), Bühlmann and van de Geer (2011) and Taddy (2019). Taddy (2019) sees regularization as “the key to modern statistics” by virtue of its ability to prevent overfitting in high-dimensional settings. Instead, we will recall a few characteristics of the lasso that are particularly relevant to our FP&A example and the corresponding simulation.\n\nThe full name of the lasso (“least absolute shrinkage and selection operator”) indicates two important characteristics. First, as we can see in the formula for \\(\\mathrm{Penalty}_\\mathrm{Lasso}\\), the absolute size of the coefficients included in the model represents a cost in the minimization of the MSE. Lasso will therefore shrink the coefficients towards zero. This makes the prediction system more stable and avoids overfitting. Second, the lasso-specific penalty in the form of the absolute value of the coefficients has the property that some parameters will be exactly equal to zero. In other words, the lasso will fully exclude some variables from the model and therefore perform automatic variable selection.\n\nAs indicated above, the lasso can handle situations in which the number of predictors approaches or even exceeds the number of observations. In our case, the number of predictors (including the measure of promotional activity) is 41 and the number of observations is 48. Although OLS can still be calculated, we will see that its out-of-sample predictive accuracy becomes extremely unreliable. If we were to chose a simulation scenario with 48 or more predictors, OLS could no longer be computed. A second challenge for OLS in settings with many predictors is the increased risk of correlation among the predictors. If predictors are highly correlated among themselves, or if, in an extreme case, there is an exact linear relationship between two predictors (multicollinearity), OLS estimates become unstable. For instance, macroeconomic variables tend to be strongly correlated.\n\nAn important ingredient in the lasso is the size of the penalty, which depends on the tuning parameter \\(\\lambda\\). \\(\\lambda\\) is not determined by the lasso itself, but needs to be selected. Intuitively, \\(\\lambda\\) plays a role in filtering the relevant variables. Several strategies to select \\(\\lambda\\) have been proposed in the literature and are used by practitioners. The most common are cross-validation strategies and information criteria such as Akaike’s or Bayes’ information criterion. Our simulation study uses the data-dependent penalty level proposed by Belloni and Chernozhukov (2013). We refer interested readers to this source for details.\n\nCompared to the standard lasso approach, which induces bias due to the shrinkage of coefficients, post-lasso has the advantage of a smaller bias, even if the model selected in the first step by lasso fails to include some of the true predictors. It also converges at a faster rate towards the true parameter values if the model selected by lasso correctly includes all true predictors (in addition to some irrelevant predictors). If lasso selects exactly (only) the true predictors, the post-lasso coefficient estimators are equal to the ones produced by an oracle that is aware of the underlying data-generating process (Belloni et al. 2012, 2014a).\n\nTable 1 summarizes the results of 1000 simulation runs for the forecasting task, comparing OLS to post-lasso.\n\nWe report the forecast accuracy in terms of average root-mean-squared error (RMSE) over all simulation runs both on the in-sample and the out-of-sample data set. As outlined above, the in-sample data set consists of 48 data points, which are used to build and train the models. The out-of-sample data set consists of 12 data points, which are intentionally not used in the model construction (“hold-out sample”), allowing the model to be evaluated on new, previously unseen data. The strong focus on forecasting performance on previously unseen data is a hallmark of the machine learning approach.\n\nOn the in-sample data, OLS produces a higher predictive accuracy than post-lasso, with an RMSE of 0.738 which is nearly one-third that of the post-lasso RMSE of 1.991. However, the real interest of the FP&A department here is not to model past sales data. Rather, the predictive performance on new data is what matters to FP&A; this is why, the out-of-sample data have been set aside. Here, the OLS RMSE increases substantially to 5.321, more than twice as high as the post-lasso RMSE of 2.162.\n\nWe can draw two main conclusions from the simulation. First, the RMSE of standard OLS increases significantly between in-sample and out-of-sample data. With nearly as many features (regressors) as observations in the model, the resulting overfitting is immediately exposed when OLS is evaluated using previously unseen data. Second, the post-lasso RMSE is relatively stable between the in-sample and out-of-sample data. The in-sample performance is thus already indicative of the true predictive power when post-lasso is used on unseen data. Lasso achieves this through the regularization strategy described above, which leads to a very selective inclusion of features and thus parsimonious models. For reference, of the 40 available features in the simulation, post-lasso retains an average of only 1.2 as relevant and shrinks the coefficients of all the others to exactly zero. As a reminder, our simulation includes only two truly relevant features. The out-of-sample RMSE for post-lasso is thus slightly hihger than the perfect RMSE score of 2.0 (equal to the standard error that was selected for the noise parameter \\(\\varepsilon\\)), which would be achieved by an oracle.\n\nFigure 1 shows the distribution of the out-of-sample RMSE for the post-lasso forecast over the 1000 simulation runs. The distribution of the errors follows approximately a normal distribution (overlaid as a red line). From a practical perspective, the risk of generating a highly incorrect lasso forecast is therefore limited. Furthermore, the right tail of the lasso errors ends before the mean of the OLS error. This provides additional reassurance when relying on lasso.\n\nDistribution of the out-of-sample RMSE for the post-lasso forecast (bars), compared to the normal distribution (red line) (colour figure online)\n\n5.2 Planning\n\nWe will now discuss the use of machine learning in financial planning. To come back to our example, the task for the FP&A department consists of evaluating the effectiveness of promotional activity in generating sales; in statistical parlance, the task relates to statistical inference of the effect of a treatment or intervention (i.e., the promotional activity) on an outcome (i.e., sales). This estimate forms the basis for planning and optimizing marketing activities. In our simulation examples, evaluating the effectiveness of promotion equates to estimating the parameter \\(\\alpha\\). As the parameter of interest, \\(\\alpha\\) corresponds to the effect of the promotional activity on sales, also called the “lift” in business applications. Let us remind ourselves that in our simulation, only two features are relevant for the sales forecast and that these two features also determine the amount of promotional activity. Thus, we are dealing with confounders, because these two features are correlated with both the treatment and the outcome. Moreover, we have set \\(\\alpha\\) to zero, which effectively means that the promotional activity does not have an impact on sales.\n\nIn a business environment, this setting could correspond to an ice cream vendor at the beach who spends money on promotion whenever the weather is warm and sunny on the weekends. He ascribes the increased ice cream sales, or at least a part of them, to his promotional efforts, whereas in reality, it is the favorable weather on the weekend that makes people come to the beach and enjoy his ice cream. Similar to the forecasting exercise, the FP&A department is obviously not aware of the data-generating process governing the simulation and needs to find a way to estimate \\(\\alpha\\).\n\nOne approach to estimating the effect of promotion could be to use the parameter estimate for \\(\\alpha\\) from the lasso model employed in sales forecasting. However, lasso shrinks parameter estimates because of the penalty loading used in the regularization process and therefore does not generate unbiased estimates of the parameter values, even though it allocates the least possible penalty amount to large signals while retaining the stable behavior of a convex penalty (Taddy 2019). Additionally, lasso estimates predictors sparingly insofar as it sets many parameter estimates to exactly zero. In many cases, the factor measuring promotional activity “may not make it” into the second step of the post-lasso procedure. It is therefore not meaningful to infer from the forecasting model the effectiveness of the promotional activities. We have previously highlighted the warning by Mullainathan and Spiess (2017) and Athey (2018) that using a tool built for forecasting and assuming that its parameters possess the properties required for inference can be misleading.\n\nWith the above in mind, one could decide to pursue a hybrid solution with the following approach. Because the lasso has identified the most relevant features for prediction, we carry these forward into the inference model. Additionally, we include in the model the variable of interest (the intervention), which in our example is the variable that represents promotional activity. In a sense, we force this variable of interest into the model. We then estimate the parameter values for all of these features using OLS, which allows us to perform inference on the parameter estimates. In particular, we are able to interpret our parameter of interest \\(\\alpha\\) in this model. In our example, \\(\\alpha\\) will tell us the effectiveness of the promotional activities. Intuitively, a model that is constructed in this way can be understood as attempting to estimate the effect of promotional activity, while controlling for other factors with proven high predictive power from the forecasting model. For the ice cream vendor at the beach, this corresponds to controlling for the effect of the favorable weather during the weekend and thus deriving an isolated estimate of the effect of promotional activity on sales. This approach can be represented as\n\nwith \\(p*\\) corresponding to the subset of all p features for which \\(\\hat{\\beta }_\\mathrm{Lasso}\\) is non-zero.\n\nWe will see from the simulation results that this approach, which we will call “naive”, grossly fails to discover the true value of the parameter of interest, when modern machine learning methods are used in high-dimensional settings; still, it is widely used by practitioners and applied researchers. In our model, the promotional activity measure is correlated with the features that concomitantly and directly influence sales. In the presence of such confounders, the naive approach will fail.\n\nIn short, the naive approach will suffer from omitted variable bias. This is because machine learning methods capture the features correlated with the outcome variable and deliver good predictive performance but often miss variables that are correlated weakly with the outcome but correlated more strongly with the intervention variable. Missing these variables does not harm predictive performance but biases the estimation of the intervention effect, leading to invalid post-selection inference.\n\nFor an approach to be valid, it must overcome this problem of imperfect model selection and related omitted variable bias. Double or debiased machine learning, as proposed by Chernozhukov et al. (2017), is one way to do so. The fundamental idea\nFootnote\n9 is to reduce, for the estimation of the parameter of interest (i.e., the intervention variable), the sensitivity with respect to errors in selecting and estimating the nuisance parameters (i.e., the other predictors in the model). Technically, this can be achieved by regressing residuals on residuals. The first set of residuals is generated by regressing the outcome variable on the control features, notably using regularizing machine learning methods such as (post-)lasso, random forests, boosted trees or other methods suited for high-dimensional settings. The second set of residuals is generated by regressing the treatment variable on the control features, again using modern machine learning methods. This auxiliary step helps to control for the confounders that might lead to omitted variable bias. Finally, the first set of residuals is regressed on the second set of residuals. The parameter value obtained in this residuals-on-residuals regression represents the effect of the treatment variable on the outcome. This procedure is known as Frisch–Waugh–Lovell partialling out. In our simulation study, machine learning methods are used for partialling out. This approach allows for valid inference compared to the naive approach.\n\nTranslated into our stylized simulation, the first regression relates sales to the 40 presumably predictive features; the differences between the predictions \\(\\hat{y}_{n}\\) from this first regression and the actual outcomes (sales) y constitute the first set of residuals \\(r^{1}_{n}\\)\n\nThe second regression relates the promotional activity score d to the 40 presumably predictive features; the differences between the predictions from this second regression \\(\\hat{d}_{n}\\) and the actual outcomes (promotional activity) \\({d}_{n}\\) constitute the second set of residuals \\(r^{2}_{n}\\)\n\nConcretely, we will use a post-lasso approach in the regressions to derive both sets of residuals, but in principle, any other machine learning method could be used, such as random forests or support vector machines. Finally, we regress the residuals from the first regression onto the residuals from the second regression to obtain an estimate for the parameter of interest, \\(\\alpha\\), which represents the impact of promotional activities on sales\n\nThis approach works well in practice, because the residuals-on-residuals approach makes the estimation of the treatment effect less sensitive to errors in the model specification. Athey (2018) provides an intuitive explanation: “[...] in high dimensions, mistakes in estimating nuisance parameters are likely, but working with residualized variables makes the estimation of the average treatment effect orthogonal to errors in estimating nuisance parameters.” This is why the family of approaches that use this principle is also referred to as orthogonal machine learning (Taddy 2019). Interested readers are referred to the literature for an in-depth theoretical discussion, including underlying assumptions and formal proofs, which is beyond the scope of this paper. Key sources include Belloni et al. (2014a) and Chernozhukov et al. (2015, 2018). To implement our simulation, we use the partialling-out approach as defined by Chernozhukov et al. (2016) and report the corresponding results under this label.\n\nTable 2 summarizes the results of the two approaches (i.e., “naive” and “partialling out”) from 1000 simulation runs. We report the mean estimate for \\(\\alpha\\), the standard deviation of the estimate and the corresponding t-statistic and p value for a two-sided test of whether the mean is different from zero. The rejection rate represents the proportion of individual simulation runs in which the ingoing assumption of \\(\\alpha\\)=0 has been rejected based on the t test (at the customary 5% significance level). In other words, these are the instances in which the model incorrectly suggests an effect (positive or negative) of promotional activity on sales.\n\nThe simulation results provide several insights. First, and this is the main point we seek to make, the naive approach grossly fails to discover the true value of \\(\\alpha\\), because it suffers from significant bias. Put simply in the context of our simulation, this bias represents systematic over-estimation of \\(\\alpha\\) and thus over-estimation of the effectiveness of promotion. On average, the naive approach estimates a value for \\(\\alpha\\) of 0.1604, compared to a true value of zero. The partialling-out approach also yields an average positive value for \\(\\alpha\\) of 0.0081, but is much closer to the true value of zero. Relatively speaking, the bias of the naive approach is roughly 20 times higher than that of the partialling-out approach.\n\nA second point is that the standard deviation of the estimates for \\(\\alpha\\) are similar for both approaches. Figures 2 (naive approach) and 3 (partialling-out approach) show the distribution of the estimates for \\(\\alpha\\) from the 1000 simulation runs compared to a normal distribution curve. Visual inspection suggests that the shapes of both distributions are well approximated by a normal distribution. Of course, the center of the distribution for the naive approach is clearly shifted to the right of zero. This reinforces the point made above that bias is induced by the naive approach.\n\nDistribution of estimator for \\(\\alpha\\) from the naive approach\n\nDistribution of estimator for \\(\\alpha\\) from the partialling-out approach\n\nTable 2 also reports the t-statistic and corresponding p value for a two-sided test of whether the mean estimate of \\(\\alpha\\) is zero. Under the naive approach, this hypothesis would be rejected with high confidence (t-statistic of 30), reinforcing the incorrect belief that the promotional efforts positively affect sales. Under the partialling-out approach, the hypothesis of no effect from promotional efforts would not be rejected at the customary 5% threshold level (t-statistic of 1.93). In practice, the FP&A department would of course not benefit from this kind of insight as they would not have access to repeated estimates for \\(\\alpha\\). With the advantage of being able to run multiple simulations, we can use this information to support the point of significant bias in the naive approach. Nevertheless, the rejection rate, reported in the last line of Table 2, provides a good indication of how often FP&A would make an incorrect decision. For each individual run in the simulation, this metric records whether FP&A would (incorrectly) reject the assumption that \\(\\alpha\\) is zero at the typical 5% significance level. Under the naive approach, this would happen 46% of the time. Put differently, a bit less than half of the time, FP&A would incorrectly assume that promotional activity does have an effect on sales. With partialling-out, this error drops to slightly below 5%.\nFootnote\n10\n\nIn summary, by relying on the naive approach, the FP&A department (or the ice cream vendor) would substantially overestimate the causal effect of the promotional activity on sales. Consequently, this activity would probably be maintained or even increased for this product or service, even though in reality, it does not increase sales. Put differently, the company would draw up plans that allocate resources wastefully on this particular product or market. The impact from falling into this trap could multiply even further across the organization if the results of such an analysis were used as a benchmark for similar products, services or geographic markets. This might happen, for example, if data are not readily available for a particular product (for example, one that is being newly launched) and the decision is made to extrapolate from existing (and potentially wrong) information. Such a situation is even more likely when the existing information appears plausible and suitable\nFootnote\n11 and, in addition, is perceived as objective, unbiased (in the sense of free from human/cognitive bias) or even scientific, because it was generated using data-driven methods.\n\n5.3 The value of data\n\nIn 2017, “The Economist” (Economist, 2017) asserted in the title of its May 6 edition that data are now the world’s most valuable resource. Questions about the value of data as a resource and production factor have generated great interest in academia and policy institutes. One consideration within this vast topic is a (hypothesized) positive feedback loop: more data lead to more data-driven insights, allowing a company to serve its customers better, to attract more customers and, in turn, to collect even more data. Nevertheless, there seems to be a broad consensus that data are generally governed by decreasing returns to scale, like any other production factor (Varian 2018; Bajari et al. 2019).\n\nIn this paper, we will limit ourselves to a short discussion of how the number of observations affects the accuracy achieved by the forecasting and inference methods used by the FP&A department within the frame of our simulation. For empirical results, we refer interested readers to Bajari et al. (2019), which contains a study of the performance of Amazon’s retail forecasting system. The study finds performance gains in the time dimension (i.e., from longer data history), but not in the product dimension (i.e., panel data forecasts do not improve with more products within a category). An interesting finding is the overall improvement of forecasts over time (controlling for the length of data history and the number of products), suggesting positive effects from improved technology (e.g., new machine learning models, better hardware or adaptation of organizational practices).\n\nIn our simulation, the hypothetical FP&A department uses a training set of 48 observations, 40 predictive features and one variable of interest for inference (i.e., the measure of promotional activity). In many real-life applications relevant to FP&A departments, the number of observations available for analysis is typically limited. More observations may simply not exist; for instance, new products generate sales data starting only from their launch date. Even if data do exist, collecting, accessing and, if necessary, curating them come at a cost; for instance, companies may limit the amount of directly accessible data history due to system constraints, or data generated prior to the introduction of new software may be inaccessible, in full or in part.\n\nLet us now explore simulation results assuming that the FP&A department has invested in expanding the training set of observations to 60, 72 or 96. The number of features (i.e., 40), the variable of interest (promotional activity measure) and the overall simulation set-up remain unchanged.\nFootnote\n12 We again run 1000 simulations. What is the return on accuracy of expanding the observation set?\nFootnote\n13\n\nTable 3 reports the forecasting results based on 60, 72 and 96 observations compared to the previous simulation based on 48 observations. For OLS, the in-sample accuracy drops, as witnessed by the increase in RMSE to 1.507 (for 96 observations) from the initial RMSE of 0.738 with 48 observations. However, the out-of-sample accuracy increases: the corresponding RMSE drops to 2.561 (for 96 observations) from the previous RMSE of 5.321 based on 48 observations. In fact, the additional observations reduce the extent of overfitting seen in the initial setting. With 40 features and (only) 48 observations, OLS was actually close to the point of failing. This point would have been reached if the number of features had been equal to or exceeded the number of observations. Intuitively, OLS moves further away from this point by expanding the set of observations (and keeping the number of features constant).\n\nFor post-lasso, the results based on 60, 72 and 96 observations are quite similar to those obtained with 48 observations. Neither the in-sample nor the out-of-sample RMSE change notably. As expected and unlike OLS, post-lasso already deals well with the initial situation in which the number of features is close to the number of observations and benefits only marginally from the increase in observations. Put differently, post-lasso does not require investing in the generation or acquisition of additional data. Our finding is consistent with standard stochastic theory.\nFootnote\n14\n\nIn summary, while having more data is generally beneficial, expanding the observation set for forecasting in our simulation study creates a tangible advantage only for OLS. If the FP&A department employs post-lasso, which is the preferable method in this setting, the gain in precision from expanding the observation set is very small and, for many practical applications, would not warrant the effort.\n\nWe will now look at inference, which entails estimating the (causal) effect of promotional activities on sales. Table 4 reports the inference results for \\(\\alpha\\) based on 60, 72 and 96 training observations compared to the previous simulation based on 48 observations. Recall that the true value of \\(\\alpha\\) is zero. For OLS, as the number of observations increases, the mean estimate for \\(\\alpha\\) decreases to 0.0617 (for 96 observations) from the previous estimate of 0.1604 with 48 observations. However, based on a standard t test, this value is still significantly different from zero (t-statistic of 13.257). In comparison, for the partialling-out approach, the mean estimate for \\(\\alpha\\) declines from 0.0081 with 48 observations to 0.0042 for 96 observations, with a minimum of \\(-\\) 0.0008 in the simulation run based on 72 observations. In all three additional scenarios, it is not statistically different from zero (t-statistic of 1.381, \\(-\\) 0.217 and 1.400, respectively).\n\nThe expanded set of observations reduces the bias of the naive approach. Intuitively, the risk of imperfect model selection described above becomes smaller. Still, the naive approach exhibits significant bias compared to the true value of \\(\\alpha\\). For the partialling-out approach, the additional observations lead to a mean estimate for \\(\\alpha\\) that comes even closer to the true value. Depending on the required precision of the estimate, the FP&A department could benefit from the additional set of observations in its analysis. Again, our finding is consistent with standard theory on convergence rates (see, for instance, Bühlmann and van de Geer 2011 or Belloni and Chernozhukov 2013). Whereas post-lasso converges for forecasting towards the true parameter value at a relatively slower rate of \\(n^{-1/4}\\), the double machine learning estimator of the treatment effect converges at the faster rate of \\(n^{-1/2}\\) (i.e., the same rate as OLS).\n\n6 Conclusion\n\nDigitalization, especially when it couples large amounts of data with appropriate tools for analysis, represents an important opportunity for the financial planning and analysis function. In this article, we have provided an introductory overview of machine learning in this context. By reviewing several relevant theoretical aspects of machine learning and discussing the results of a simulation study, we have demonstrated how machine learning may prove useful for FP&A practitioners. We have paid special attention to explain the distinction between forecasting and planning tasks, the first of which involves prediction and the latter of which involves causal inference. We see the confusion of these two concepts as a major pitfall that practitioners should strive to avoid. Specific approaches to causal machine learning have begun to gain traction, as awareness has increased that the naive application of machine learning can fail in applications that go beyond prediction. This applies to all modern machine learning methods in a high-dimensional setting.\n\nOur article has several limitations. It was impossible to cover the vast number of machine learning techniques that exist. Depending on the causal question at hand, a range of econometric approaches (e.g., instrumental variables, synthetic controls or regression discontinuity designs) coupled with machine learning methods may be suitable. We intentionally used a simple data generation process in our simulation; additional elements such as trends or seasonal components or a real-life example could complement our simulation study. Despite these limitations, we believe that our article can be a valuable source of insights into the ways in which FP&A can benefit from machine learning. With it, we hope to contribute to the adoption of machine learning in this area and help practitioners avoid common mistakes.\n\nNotes\n\nIn some companies, the (short-term) plans formally expressed in budgets are prepared by controllers (management accountants) within the accounting department (Garrison et al. 2006), while the strategy department formulates the directional (long-term) plans.\n\nFor instance, www.bosch.com/de/forschung/know-how/publikationen (accessed Feb 23, 2021) contains a collection by Robert Bosch GmbH.\n\nOther methods with a very similar intent exist. Examples are the quantification of a best and worst case in addition to the normal or base case, or sensitivity analysis with varying degrees of sophistication.\n\nWe have intentionally kept the simulation example simple. For instance, we have not added any time-series-specific effects such as a trend component or serially correlated error terms. This allows us to focus on the key elements. For instance, the \\(N=60\\) data points could represent observations in different countries or sub-markets, which would warrant a cross-sectional approach to the analysis. The conclusions presented in the simulation example will remain largely unchanged.\n\nThis is a case of supervised learning, because we have observations for both the input (\\(x_{p,n}\\)) and the output (\\(y_{n}\\)).\n\nThe value of c represents the correlation between immediate neighbor features (e.g., feature \\(x_{p}\\) and feature \\(x_{p+1}\\)). Due to the way \\(\\Sigma\\) is constructed, the correlation decays quickly as the distance between features increases (e.g., feature \\(x_{p}\\) and \\(x_{p+3}\\) have only a correlation of \\(c^{3}\\), which is 0.027 for \\(c=0.3\\)).\n\nA simple example can help clarify the intuition behind this setting. Ice cream sales on the beach are probably positively related to weather conditions (feature 1) and the day of the week (feature 2). At the same time, the ice cream salesperson may decide to run some promotional activity when weather conditions are favorable on a weekend. Thus, the same features have an influence both on sales and on promotional activity. We will revert to this illustrative example in the section on planning.\n\nBy construction, our simulation example is exactly sparse, with parameter values for all non-relevant features exactly equal to zero. For practical applications, a more realistic assumption is approximate sparsity, meaning that all or many features can have non-zero parameter values. Nevertheless, only a limited number of features are needed to approximate the true relationship with sufficient accuracy. We refer interested readers to Belloni et al. (2010). Our simulation could easily be extended to such a setting. Results would remain largely unchanged.\n\nDouble machine learning also uses cross-fitting, an efficient way of data splitting. Interested readers are referred to Chernozhukov et al. (2017).\n\nNote that one would expect an error rate here of around 5% from a correct model, because the 5% significance level corresponds to a 5% probability of rejecting the null hypothesis when it is, in reality, true.\n\nBlake et al. (2015) highlight in their paper that “[...] the incentives faced by advertising firms, publishers, analytics consulting firms, and even marketing executives within companies, are all aligned with increasing advertising budgets.”\n\nWe intentionally do not allow the number of features to grow with the sample size (see for instance Belloni et al. 2010) to isolate the effect of the additional observations clearly. In practice, a significant extension of the number of observations may require including additional control features.\n\nThe natural way to think about the expansion is to assume that the department “digs out” additional historical observations. However, from a theoretical standpoint, the department could also wait 1, 2 or 4 years, respectively, and gather the additional data points over time. In this case, the change in forecasting accuracy could be mistaken for a technology or learning effect by an outside observer.\n\nSee, for instance, Bühlmann and van de Geer (2011) or Belloni and Chernozhukov (2013). Post-lasso converges towards the true parameter value at a rate of \\(n^{-1/4}\\), which is slower than the OLS rate of \\(n^{-1/2}\\). The value of additional data is thus generally smaller for post-lasso than for OLS.\n\nReferences\n\nAcemoglu, D., & Restrepo, P. (2018). Artificial intelligence, automation, and work. The economics of artificial intelligence: an agenda (pp. 197–236). University of Chicago Press. https://doi.org/10.7208/chicago/9780226613475.001.0001.\n\nChapter\n  Google Scholar\n\nAthey, S. (2018). The impact of machine learning on economics. The economics of artificial intelligence: an agenda (pp. 507–547). University of Chicago Press.\n\nGoogle Scholar\n\nBajari, P., Chernozhukov, V., Hortaçsu, A., & Suzuki, J. (2019). The impact of big data on firm performance: an empirical investigation. AEA Papers and Proceedings, 109, 33–37. https://doi.org/10.1257/pandp.20191000.\n\nArticle\n  Google Scholar\n\nBalakrishnan, T., Chui, M., Hall, B., & Henke, N. (2020). Global survey: the state of AI in 2020. McKinsey & Company. https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/global-survey-the-state-of-ai-in-2020. Accessed 6 Dec 2020.\n\nBarker, J., Gajewar, A., Golyaev, K., Bansal, G., & Conners, M. (2018). Secure and automated enterprise revenue forecasting. In AAAI, pp. 7657–7664.\n\nBecker, S. D., Mahlendorf, M. D., Schäffer, U., & Thaten, M. (2016). Budgeting in times of economic crisis. Contemporary Accounting Research, 33, 1489–1517. https://doi.org/10.1111/1911-3846.12222.\n\nArticle\n  Google Scholar\n\nBelloni, A., Chen, D., Chernozhukov, V., & Hansen, C. (2012). Sparse models and methods for optimal instruments with an application to eminent domain. Econometrica, 80, 2369–2429. https://doi.org/10.3982/ECTA9626.\n\nArticle\n  Google Scholar\n\nBelloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. Bernoulli, 19, 521–547. https://doi.org/10.3150/11-BEJ410.\n\nArticle\n  Google Scholar\n\nBelloni, A., Chernozhukov, V., & Hansen, C. (2010). Inference for high-dimensional sparse econometric models. In Advances in Economics and Econometrics. 10th World Congress of Econometric Society, Aug 2010 III, pp. 245–295. ArXiv, 2011.\n\nBelloni, A., Chernozhukov, V., & Hansen, C. (2014a). High-dimensional methods and inference on structural and treatment effects. Journal of Economic Perspectives, 28, 29–50. https://doi.org/10.1257/jep.28.2.29.\n\nArticle\n  Google Scholar\n\nBelloni, A., Chernozukov, V., & Hansen, C. (2014b). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81, 608–650.\n\nArticle\n  Google Scholar\n\nBishop, C. M. (2006). Pattern recognition and machine learning. Information science and statistics. Springer (Softcover published in 2016).\n\nGoogle Scholar\n\nBlake, T., Nosko, C., & Tadelis, S. (2015). Consumer heterogeneity and paid search effectiveness: a large-scale field experiment. Econometrica, 83, 155–174. https://doi.org/10.3982/ECTA12423.\n\nArticle\n  Google Scholar\n\nBrealey, R. A., Myers, S. C., & Franklin, A. (2020). Principles of corporate finance (13th ed.). McGraw-Hill Education.\n\nGoogle Scholar\n\nBühlmann, P., & van de Geer, S. (2011). Statistics for high-dimensional data: methods, theory and applications. Springer series in statistics. Springer.\n\nBook\n  Google Scholar\n\nChandra, K., Plaschke, F., & Seth, I. (2018). Memo to the CFO: get in front of digital finance - or get left back. McKinsey & Company. https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/memo-to-the-cfo-get-in-front-of-digital-finance-or-get-left-back. Accessed 10 Dec 2020.\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., & Newey, W. (2017). Double/debiased/neyman machine learning of treatment effects. American Economic Review, 107, 261–65. https://doi.org/10.1257/aer.p20171038.\n\nArticle\n  Google Scholar\n\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21, C1–C68. https://doi.org/10.1111/ectj.12097.\n\nArticle\n  Google Scholar\n\nChernozhukov, V., Hansen, C., & Spindler, M. (2015). Valid post-selection and post-regularization inference: an elementary, general approach. Annual Review of Economics, 7, 649–688. https://doi.org/10.1146/annurev-economics-012315-015826.\n\nArticle\n  Google Scholar\n\nChernozhukov, V., Hansen, C., & Spindler, M. (2016). High-dimensional metrics in R. arXiv:1603.01700v2.\n\nConine, T. C., & McDonald, M. (2017). The application of variance analysis in FP&A organizations: survey evidence and recommendations for enhancement. Journal of Accounting and Finance, 17, 54–70.\n\nGoogle Scholar\n\nDe Gooijer, J. G., & Hyndman, R. J. (2006). 25 years of time series forecasting. International Journal of Forecasting, 22, 443–473. https://doi.org/10.1016/j.ijforecast.2006.01.001 (twenty five years of forecasting).\n\nArticle\n  Google Scholar\n\nEconomist (2017). The world’s most valuable resource is no longer oil, but data. https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data. Accessed 6 Dec 2020.\n\nFischer, E. O. (2009). Finanzwirtschaft für Anfänger. Lehr- und Handbücher zur entscheidungsorientierten Betriebswirtschaft. Oldenbourg.\n\nGoogle Scholar\n\nGajewar, A., & Bansal, G. (2016). Revenue forecasting for enterprise products. arXiv:1701.06624.\n\nGandomi, A., & Haider, M. (2015). Beyond the hype: big data concepts, methods, and analytics. International Journal of Information Management, 35, 137–144. https://doi.org/10.1016/j.ijinfomgt.2014.10.007.\n\nArticle\n  Google Scholar\n\nGarrison, R. H., Noreen, E. W., & Brewer, P. C. (2006). Managerial accounting. McGraw-Hill/Irwin.\n\nGoogle Scholar\n\nGray, G. L., & Alles, M. (2015). Data fracking strategy: why management accountants need it. Management Accounting Quarterly, 16, 22–33.\n\nGoogle Scholar\n\nHansen, S. C. (2011). A theoretical analysis of the impact of adopting rolling budgets, activity-based budgeting and beyond budgeting. European Accounting Review, 20, 289–319. https://doi.org/10.1080/09638180.2010.496260.\n\nArticle\n  Google Scholar\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction (2nd ed.). Springer-Verlag.\n\nBook\n  Google Scholar\n\nHenrique, B. M., Sobreiro, V. A., & Kimura, H. (2019). Literature review: machine learning techniques applied to financial market prediction. Expert Systems with Applications, 124, 226–251. https://doi.org/10.1016/j.eswa.2019.01.012.\n\nArticle\n  Google Scholar\n\nKoenecke, A., & Gajewar, A. (2020). Curriculum learning in deep neural networks for financial forecasting. In V. Bitetta, I. Bordino, A. Ferretti, F. Gullo, S. Pascolutti, & G. Ponti (Eds.), Mining data for financial applications (pp. 16–31). Springer International Publishing.\n\nChapter\n  Google Scholar\n\nKüsters, U., McCullough, B. D., & Bell, M. (2006). Forecasting software: past, present and future. International Journal of Forecasting, 22, 599–615. https://doi.org/10.1016/j.ijforecast.2006.03.004 (twenty five years of forecasting).\n\nArticle\n  Google Scholar\n\nLaney, D. (2001). 3-D data management: controlling data volume, velocity and variety. Application Delivery Strategies by META Group Inc., Gartner. https://blogs.gartner.com/doug-laney/files/2012/01/ad949-3DData-Management-Controlling-Data-Volume-Velocity-andVariety.pdf. Accessed 30 July 2020.\n\nMöller, K., Schäffer, U., & Verbeeten, F. (2020). Digitalization in management accounting and control: an editorial. Journal of Management Control, 31, 1–8. https://doi.org/10.1007/s00187-020-00300-5.\n\nArticle\n  Google Scholar\n\nMullainathan, S., & Spiess, J. (2017). Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31, 87–106. https://doi.org/10.1257/jep.31.2.87.\n\nArticle\n  Google Scholar\n\nOesterreich, T. D., Teuteberg, F., Bensberg, F., & Buscher, G. (2019). The controlling profession in the digital age: understanding the impact of digitisation on the controller’s job roles, skills and competences. International Journal of Accounting Information Systems. https://doi.org/10.1016/j.accinf.2019.100.\n\nArticle\n  Google Scholar\n\nOzbayoglu, A. M., Gudelek, M. U., & Sezer, O. B. (2020). Deep learning for financial applications: a survey. Applied Soft Computing, 93, 106384. https://doi.org/10.1016/j.asoc.2020.106384.\n\nArticle\n  Google Scholar\n\nPearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62, 54–60. https://doi.org/10.1145/3241036.\n\nArticle\n  Google Scholar\n\nPearl, J., & Mackenzie, D. (2018). The book of why: the new science of cause and effect (1st ed.). Basic Books Inc.\n\nGoogle Scholar\n\nRoos, A., Tucker, J., Rodt, M., Stange, S., Ego, P., Boudadi, A., & Sheth, H. (2020). Lessons from best-in-class CFOs. Boston Consulting Group. https://www.bcg.com/publications/2020/lessons-best-in-class-cfos. Accessed 29 July 2020.\n\nRoss, S. A., Westerfield, R. W., & Jordan, B. D. (2019). Fundamentals of corporate finance (12th ed.). McGraw-Hill Education.\n\nGoogle Scholar\n\nRubin, D. B. (2005). Causal inference using potential outcomes. Journal of the American Statistical Association, 100, 322–331. https://doi.org/10.1198/016214504000001880.\n\nArticle\n  Google Scholar\n\nStrauß, E., & Zecher, C. (2013). Management control systems: a review. Journal of Management Control, 23, 233–268. https://doi.org/10.1007/s00187-012-0158-7.\n\nArticle\n  Google Scholar\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: an introduction. Adaptive computation and machine learning series. MIT Press.\n\nGoogle Scholar\n\nTaddy, M. (2019). Business data science: combining machine learning and economics to optimize, automate, and accelerate business decisions. McGraw-Hill Education.\n\nGoogle Scholar\n\nTucker, J., Foldesy, J., Roos, A., & Rodt, M. (2017). How digital CFOs are transforming finance. Boston Consulting Group. https://www.bcg.com/publications/2017/function-excellence-how-digital-cfo-transforming-finance. Accessed 10 Dec 2020.\n\nUnger, G., & Rodt, M. (2019). The art of forward-looking steering: the power of algorithmic forecasting. Boston Consulting Group. https://www.bcg.com/publications/2019/power-of-algorithmic-forecasting. Accessed 30 Nov 2020.\n\nVarian, H. (2018). Artificial intelligence, economics, and industrial organization. The economics of artificial intelligence: an agenda (pp. 399–419). University of Chicago Press. https://doi.org/10.7208/chicago/9780226613475.001.0001.\n\nChapter\n  Google Scholar\n\nZhu, X., & Goldberg, A. B. (2009). Introduction to semi-supervised learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 3, 1–130. https://doi.org/10.2200/S00196ED1V01Y200906AIM006.\n\nArticle\n  Google Scholar\n\nDownload references\n\nAcknowledgements\n\nWe thank the editor and two anonymous referees for their very helpful comments and suggestions.\n\nFunding\n\nOpen Access funding enabled and organized by Projekt DEAL.\n\nAuthor information\n\nAuthors and Affiliations\n\nNovartis International AG, Novartis Campus, 4002, Basel, Switzerland\n\nHelmut Wasserbacher\n\nHamburg Business School, University of Hamburg, Moorweidenstr. 18, 20148, Hamburg, Germany\n\nMartin Spindler\n\nCorresponding author\n\nCorrespondence to Martin Spindler.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nThe views and opinions expressed in this document are those of the first author, and do not necessarily reflect the official policy or position of Novartis or any of its officers.\n\nRights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nWasserbacher, H., Spindler, M. Machine learning for financial forecasting, planning and analysis: recent developments and pitfalls. Digit Finance 4, 63–88 (2022). https://doi.org/10.1007/s42521-021-00046-2\n\nDownload citation\n\nReceived\n27 May 2021\n\nAccepted\n17 November 2021\n\nPublished\n16 December 2021\n\nIssue Date\nMarch 2022\n\nDOI\nhttps://doi.org/10.1007/s42521-021-00046-2\n\nShare this article\n\nAnyone you share the following link with will be able to read this content:\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\nKeywords\n\nJEL Classification\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nUsing machine learning methods to predict financial performance: Does ... : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nUsing machine learning methods to predict financial performance: Does disclosure tone matter?\n\n1639 Accesses\n\n24 Citations\n\n1 Altmetric\n\nExplore all metrics\n\nAbstract\n\nWe use three supervised machine learning methods, namely linear discriminant analysis, quadratic discriminant analysis, and random forest, to predict corporate financial performance. We use a sample of 63 listed banks from eight emerging markets, covering 10 years from 2008 to 2017, using earning per share as a measure of performance. We use the design science research (DSR) framework to examine whether the textual contents of annual reports in previous years contain value-relevant information to predict future performance; thus, these contents can improve the accuracy and quality of predictive models. We combine two groups of variables in the proposed models. The first group is the sentiment analysis of disclosure tone in annual report narratives using the Loughran and McDonald dictionary (J Finance 66:35–65, 2011), while the second group is the quantitative properties of banks which consist of five variables, namely size, financial leverage, age, market-to-book ratio, and risk. Our analysis suggests that the random forest method provides the best predictive model. We also provide evidence on the accuracy and performance of predictive models that can be increased by incorporating disclosure tone variables as non-financial variables with financial variables. Interestingly, we find that the uncertainty variable is the most important disclosure tone variable. Finally, we find that size is the most important variable related to banks’ quantitative characteristics. Our study suggests that the analysis of tone through corporate narrative disclosures can be used as a complementary or diagnostic approach rather than an alternative in making decisions by different stakeholders such as analysts, investors, and auditors.\n\nThis is a preview of subscription content, log in via an institution to check access.\n\nAccess this article\n\nSubscribe and save\n\nBuy Now\n\nPrice includes VAT (Lebanon)\n\nInstant access to the full article PDF.\n\nInstitutional subscriptions\n\nSimilar content being viewed by others\n\nData Analysis for Predicting Stock Prices Using Financial Indicators Based on Business Reports\n\nBenchmarking Machine Learning Algorithms to Predict Profitability Directional Changes\n\nPerformance of Indebted Companies Using a Machine Learning Approach\n\nReferences\n\nAl-Khatib, H.B., and A. Al-Horani. 2012. Predicting financial distress of public companies listed in Amman stock exchange. European Scientific Journal 8(15): 1–18.\n\nGoogle Scholar\n\nAltman, D.G., and J.M. Bland. 1994a. Diagnostic tests. 1: Sensitivity and specificity. British Medical Journal 308: 1552.\n\nArticle\n  Google Scholar\n\nAltman, D.G., and J.M. Bland. 1994b. Statistics notes: Diagnostic tests 2—Predictive values. British Medical Journal 309: 102.\n\nArticle\n  Google Scholar\n\nAltman, E.I., M. Iwanicz-Drozdowska, E.K. Laitinen, and A. Suvas. 2017. Financial distress prediction in an international context: A review and empirical analysis of Altman’s z-score model. Journal of International Financial Management & Accounting 28: 131–171.\n\nArticle\n  Google Scholar\n\nAltman, E.I., G. Sabato, and N. Wilson. 2010. The value of non-financial information in small and medium-sized enterprise risk management. Journal of Credit Risk 2: 95–127.\n\nArticle\n  Google Scholar\n\nAly, D., S. El-Halaby, and K. Hussainey. 2018. Tone disclosure and financial performance: Evidence from Egypt. Accounting Research Journal 31(1): 63–74.\n\nArticle\n  Google Scholar\n\nAppiah, K.O., and J. Abor. 2009. Predicting corporate failure: Some empirical evidence from the UK. Benchmarking: an International Journal 16: 432–444.\n\nArticle\n  Google Scholar\n\nAppiah, K.O., A. Chizema, and J. Joseph Arthur. 2015. Predicting corporate failure: A systematic literature review of methodological issues. International Journal of Law and Management 57: 461–485.\n\nArticle\n  Google Scholar\n\nArslan-Ayaydin, Ö., K. Boudt, and J. Thewissen. 2016. Managers set the tone: Equity incentives and the tone of earnings press releases. Journal of Banking & Finance 72: S132–S147.\n\nArticle\n  Google Scholar\n\nAssociation for Investment Management and Research (AIMR). 2000. Corporate disclosure survey: A report to AIMR. St. Louis: Fleishman-Hillard Research.\n\nGoogle Scholar\n\nAziz, M.A., and H.A. Dar. 2006. Predicting corporate bankruptcy: Where do we stand? Corporate Governance 6: 18–33.\n\nArticle\n  Google Scholar\n\nBaginski, S.P., E. Demers, A. Kausar, and Y.J. Yu. 2018. Linguistic tone and the small trader. Accounting, Organizations and Society 68–69: 21–37.\n\nArticle\n  Google Scholar\n\nBalakrishnan, R., X.Y. Qiu, and P. Srinivasan. 2010. On the predictive ability of narrative disclosures in annual reports. European Journal of Operational Research 202: 789–801.\n\nArticle\n  Google Scholar\n\nBalcaen, S., and H. Ooghe. 2006. 35 years of studies on business failure: An overview of the classic statistical methodologies and their related problems. The British Accounting Review 38: 63–93.\n\nArticle\n  Google Scholar\n\nBeattie, V.A., and M.J. Jones. 2000. Changing graph use in corporate annual reports: A time-series analysis. Contemporary Accounting Research 17(2): 213–226.\n\nArticle\n  Google Scholar\n\nBreiman, L. 2001a. Random forests. Machine Learning 45: 5–32.\n\nArticle\n  Google Scholar\n\nBreiman, L. 2001b. Statistical modeling: The two cultures. Statistical Science 16: 199–215.\n\nArticle\n  Google Scholar\n\nBreiman, L. 2017. Classification and regression trees. London: Routledge.\n\nBook\n  Google Scholar\n\nCampbell, J.L., H. Chen, D.S. Dhaliwal, H.M. Lu, and L.B. Steele. 2014. The information content of mandatory risk factor disclosures in corporate filings. Review of Accounting Studies 19: 396–455.\n\nArticle\n  Google Scholar\n\nChen, K., T. Chen, and J. Yen. 2009. Predicting future earnings change using numeric and textual information in financial reports. In Proceedings of intelligence and security informatics, Pacific Asia Workshop, PAISI 2009, Bangkok, Thailand, 54–63.\n\nChou, C., C.J. Chang, C. Chin, and W. Chiang. 2018. Measuring the consistency of quantitative and qualitative information in financial reports: A design science approach. Journal of Emerging Technologies in Accounting 15: 93–109.\n\nArticle\n  Google Scholar\n\nClatworthy, M., and M. Jones. 2003. Financial reporting of good news and bad news: Evidence from accounting narratives. Accounting and Business Research 33(3): 171–185.\n\nArticle\n  Google Scholar\n\nClatworthy, M., and M. Jones. 2006. Differential patterns of textual characteristics and company performance in the chairman’s statement. Accounting, Auditing & Accountability Journal 19(4): 493–511.\n\nArticle\n  Google Scholar\n\nConnelly, B.L., S.T. Certo, R.D. Ireland, and C.R. Reutzel. 2011. Signaling theory: A review and assessment. Journal of Management 37(1): 39–67.\n\nArticle\n  Google Scholar\n\nCooper, W., L. Seiford, and K. Tone. 2000. Data envelopment analysis: A comprehensive text with models, applications, references and DEA solver software. Boston/Dordrecht/London: Kluwer Academic Publishers.\n\nBook\n  Google Scholar\n\nDavis, A., J. Piger, and L. Sedor. 2006. Beyond the numbers: An analysis of optimistic and pessimistic language in earnings press releases. Working paper, Washington University at St. Louis, Federal Reserve Bank of St. Louis and the University of Notre Dame.\n\nDavis, A.K., and I. Tama-Sweet. 2012. Managers’ use of language across alternative disclosure outlets: Earnings press releases versus MD&A. Contemporary Accounting Research 29(3): 804–837.\n\nArticle\n  Google Scholar\n\nde Graaff, R. 2017. Sentiment analysis of annual reports as a financial performance indicator. Master of Science in Business Information Systems, Eindhoven University of Technology.\n\nDias, W., and R. Matias-Fonseca. 2010. The language of annual reports as an indicator of the organizations’ financial situation. International Review of Business Research Papers 6: 206–215.\n\nGoogle Scholar\n\nDonner, A., and N. Klar. 1996. The statistical analysis of kappa statistics in multiple samples. Journal of Clinical Epidemiology 49: 1053–1058.\n\nArticle\n  Google Scholar\n\nDuda, R.O., P.E. Hart, and D.G. Stork. 2012. Pattern classification. New York: Wiley.\n\nGoogle Scholar\n\nElshandidy, T., and P.J. Shrives. 2016. Environmental incentives for and usefulness of textual risk reporting: Evidence from Germany. The International Journal of Accounting 51: 464–486.\n\nArticle\n  Google Scholar\n\nFalschlunger, L.M., C. Eisl, H. Losbichler, and A.M. Greil. 2015. Impression management in annual reports of the largest European companies: A longitudinal study on graphical representations. Journal of Applied Accounting Research 16(3): 383–399.\n\nArticle\n  Google Scholar\n\nFeldman, R., S. Govindaraj, J. Livnat, and B. Segal. 2010. Management’s tone change, post earnings announcement drift and accruals. Review of Accounting Studies 15: 915–953.\n\nArticle\n  Google Scholar\n\nGarcia, D. 2013. Sentiment during recessions. Journal of Finance 68: 1267–1300.\n\nArticle\n  Google Scholar\n\nGenuer, R., J.-M. Poggi, and C. Tuleau-Malot. 2010. Variable selection using random forests. Pattern Recognition Letters 31: 2225–2236.\n\nArticle\n  Google Scholar\n\nGregor, S., and A.R. Hevner. 2013. Positioning and presenting design science research for maximum impact. MIS Quarterly 37: 337–355.\n\nArticle\n  Google Scholar\n\nHackfort, D., R. Schinke, and B. Strauss. 2019. Dictionary of sport psychology. London: Academic Press.\n\nGoogle Scholar\n\nHahn, R., and R. Lülfs. 2014. Legitimizing negative aspects in GRI-oriented sustainability reporting: A qualitative analysis of corporate disclosure strategies. Journal of Business Ethics 123(3): 401–420.\n\nArticle\n  Google Scholar\n\nHastie, T., R. Tibshirani, and J. Friedman. 2009. The elements of statistical learning: Data mining, inference, and prediction. Springer Series in Statistics. New York: Springer.\n\nBook\n  Google Scholar\n\nHeinle, M.S., and K.C. Smith. 2017. A theory of risk disclosure. Review of Accounting Studies 22: 1459–1491.\n\nArticle\n  Google Scholar\n\nHenry, E. 2006. Market reaction to verbal components of earnings press releases: Event study using a predictive algorithm. Journal of Emerging Technologies in Accounting 3: 1–19.\n\nArticle\n  Google Scholar\n\nHenry, E. 2008. Are investors influenced by how earnings press releases are written? Journal of Business Communication 45: 363–407.\n\nArticle\n  Google Scholar\n\nHevner, A., and S. Chatterjee. 2010. Design science research in information systems. In Design research in information systems, ed. A. Hevner and S. Chatterjee, 9–22. Boston, MA: Springer.\n\nChapter\n  Google Scholar\n\nHildebrandt, H.W., and R.D. Snyder. 1981. The Pollyanna hypothesis in business writing: Initial results, suggestions for research. Journal of Business Communication 18: 5–15.\n\nArticle\n  Google Scholar\n\nHo, T.K. 1995. Random decision forests. In Proceedings of the 3rd international conference on document analysis and recognition, Montreal, QC, 14–16 August 1995, 278–282.\n\nHo, T.K. 1998. The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence 20(8): 832–844.\n\nArticle\n  Google Scholar\n\nHo, C.T., and D.S. Zhu. 2004. Performance measurement of Taiwan’s commercial banks. International Journal of Productivity and Performance Management 53(5/6): 425–434.\n\nArticle\n  Google Scholar\n\nHope, O., D. Hu, and H. Lu. 2016. The benefits of specific risk-factor disclosures. Rotman School of Management Working Paper No. 2457045; Singapore Management University School of Accountancy Research Paper No. 2015-35.\n\nHorváth, I. 2007. Comparison of three methodological approaches of design research. the 16th International Conference on Engineering Design. Proceedings of ICED 2007. Paris, France, pp 1–11.\n\nHothorn, T., F. Leisch, A. Zeileis, and K. Hornik. 2005. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics 14: 675–699.\n\nArticle\n  Google Scholar\n\nHuang, X., S.H. Teoh, and Y. Zhang. 2014. Tone management. The Accounting Review 89(3): 1083–1113.\n\nArticle\n  Google Scholar\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An introduction to statistical learning. New York: Springer.\n\nBook\n  Google Scholar\n\nJensen, H., and W. Meckling. 1976. Theory of the firm: Managerial behaviour, agency costs and ownership structure. Journal of Financial Economics 16: 305–360.\n\nArticle\n  Google Scholar\n\nKearney, C., and S. Liu. 2014. Textual sentiment in finance: A survey of methods and models. International Review of Financial Analysis 33: 171–185.\n\nArticle\n  Google Scholar\n\nKeusch, T., L.H.H. Bollen, and H.F.D. Hassink. 2012. Self-serving bias in annual report narratives: An empirical analysis of the impact of economic crises. European Accounting Review 21(3): 623–648.\n\nGoogle Scholar\n\nKloptchenko, A., T. Eklund, B. Back, J. Karlsson, H. Vanharanta, and A. Visa. 2002. Combining data and text mining techniques for analysing financial reports. In Proceedings of eighth Americas conference on information systems.\n\nKravet, T., and V. Muslu. 2013. Textual risk disclosures and investors’ risk perceptions. Review of Accounting Studies 18: 1088–1122.\n\nArticle\n  Google Scholar\n\nKuhn, M. 2008. Building predictive models in R using the caret package. Journal of Statistical Software 28: 1–26.\n\nArticle\n  Google Scholar\n\nKursa, M.B., A. Jankowski, and W.R. Rudnicki. 2010. Boruta: A system for feature selection. Fundamenta Informaticae 101: 271–285.\n\nArticle\n  Google Scholar\n\nKursa, M., and W. Rudnicki. 2010. Feature selection with the Boruta package. Journal of Statistical Software 36(11): 1–13.\n\nArticle\n  Google Scholar\n\nLeary, M.R. 2001. Impression management, psychology. In International encyclopedia of the social and behavioral sciences, ed. J. Wright, 7245–7248. Amsterdam: Elsevier.\n\nChapter\n  Google Scholar\n\nLeary, M.R., and R.M. Kowalski. 1990. Impression management: A literature review and two component model. Psychological Bulletin 107(1): 34–47.\n\nArticle\n  Google Scholar\n\nLee, A., J.T. Lin, R. Kao, and K. Chen. 2010. An effective clustering approach to stock market prediction. In PACIS 2010 proceedings, 54.\n\nLehavy, R., F. Li, and K. Merkley. 2011. The effect of annual report readability on analyst following and the properties of their earnings forecasts. The Accounting Review 86: 1087–1115.\n\nArticle\n  Google Scholar\n\nLev, B. 1989. On the usefulness of earnings: Lessons and directions from two decades of empirical research. Journal of Accounting Research 27(Supplement): 153–192.\n\nArticle\n  Google Scholar\n\nLi, F. 2006a. Annual report readability, current earnings and earnings persistence. Working paper, University of Michigan, Ann Arbor.\n\nLi, F. 2006b. Do stock market investors understand the risk sentiments of corporate annual reports? Working paper, University of Michigan, Ann Arbor.\n\nLi, F. 2008. Annual report readability, current earnings, and earnings persistence. Journal of Accounting and Economics 45: 221–247.\n\nArticle\n  Google Scholar\n\nLi, F. 2010. The information content of forward-looking statements in corporate filings: A naïve Bayesian machine learning approach. Journal of Accounting Research 48: 1049–1102.\n\nArticle\n  Google Scholar\n\nLiu, B., and J.J. McConnell. 2013. The role of the media in corporate governance: Do the media influence managers’ capital allocation decisions? Journal of Financial Economics 110: 1–17.\n\nArticle\n  Google Scholar\n\nLoughran, T., and B. McDonald. 2011. When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. Journal of Finance 66: 35–65.\n\nArticle\n  Google Scholar\n\nLu, C., and T. Chen. 2009. A study of applying data mining approach to the information disclosure for Taiwan’s stock market investors. Expert Systems with Applications 36: 3536–3542.\n\nArticle\n  Google Scholar\n\nLukason, O., and E.K. Laitinen. 2019. Firm failure processes and components of failure risk: An analysis of European bankrupt firms. Journal of Business Research 98: 380–390.\n\nArticle\n  Google Scholar\n\nMagnusson, C., A. Arppe, T. Eklund, B. Barbro, H. Vanharanta, and A. Visa. 2005. The language of quarterly reports as an indicator of change in the company’s financial status. Information & Management 42: 561–574.\n\nGoogle Scholar\n\nMcLachlan, G. 2004. Discriminant analysis and statistical pattern recognition, vol. 544. New York: Wiley.\n\nGoogle Scholar\n\nMerkl-Davies, D.M., and N.M. Brennan. 2007. Discretionary disclosure strategies in corporate narratives: Incremental information or impression management? Journal of Accounting Literature 27: 116–196.\n\nGoogle Scholar\n\nMerkl-Davies, D.M., and N.M. Brennan. 2011. A conceptual framework of impression management: New insights from psychology, sociology and critical perspectives. Accounting and Business Research 41(5): 415–437.\n\nArticle\n  Google Scholar\n\nOnder, E., and A.T. Altintas. 2017. Financial performance evaluation of Turkish construction companies in Istanbul Stock Exchange (BIST). International Journal of Academic Research in Accounting, Finance and Management Sciences 7: 108–113.\n\nArticle\n  Google Scholar\n\nQiu, X.Y. 2007. On building predictive models with company annual reports (Ph.D. thesis, University of Iowa, Iowa City).\n\nQiu, X.Y., P. Srinivasan, and Y. Hu. 2014. Supervised learning models to predict firm performance with annual reports: An empirical study. Journal of the American Society for Information Science and Technology 65: 400–413.\n\nGoogle Scholar\n\nQiu, X.Y., P. Srinivasan, and N. Street. 2006. Exploring the forecasting potential of company annual reports. In 69th Annual meeting of the American Society for Information Science and Technology (ASIST), Austin, 3–8 November 2006.\n\nRahman, S. 2012. Impression management motivations, strategies and disclosure credibility of corporate narratives. Journal of Management Research 4(3): 1.\n\nArticle\n  Google Scholar\n\nRessas, M.S., and K. Hussainey. 2014. Does financial crisis affect financial reporting of good news and bad news? International Journal of Accounting, Auditing and Performance Evaluation 10(4): 410–429.\n\nArticle\n  Google Scholar\n\nRogers, K., and J. Grant. 1997. Content analysis of information cited in reports of sell-side financial analysts. Journal of Financial Statement Analysis 3: 17–30.\n\nGoogle Scholar\n\nSchleicher, T., and M. Walker. 2010. Bias in the tone of forward-looking narratives. Accounting and Business Research 40(4): 371–390.\n\nArticle\n  Google Scholar\n\nSharma, S. 1995. Applied multivariate techniques. New York: Wiley.\n\nGoogle Scholar\n\nSiqueira, L.F., R.F.A. Júnior, A.A. de Araújo, C.L. Morais, and K.M. Lima. 2017. LDA versus QDA for FT-MIR prostate cancer tissue classification. Chemometrics and Intelligent Laboratory Systems 162: 123–129.\n\nArticle\n  Google Scholar\n\nSmith, M., and R.J. Taffler. 2000. The chairman’s statement: A content analysis of discretionary narrative disclosures. Accounting Auditing & Accountability Journal 13(5): 624–647.\n\nArticle\n  Google Scholar\n\nTedeschi, J.T., and M. Riess. 1981. Identities, the phenomenal self, and laboratory research. In Impression management theory and social psychological research, ed. J.T. Tedeschi, 3–22. New York: Academic Press.\n\nChapter\n  Google Scholar\n\nTharwat, A. 2016. Linear versus quadratic discriminant analysis classifier: A tutorial. International Journal of Applied Pattern Recognition 3: 145–180.\n\nArticle\n  Google Scholar\n\nYu, Y., A. Barros, C. Tsai, and K. Liao. 2014. A comparison of ratios and data envelopment analysis: Efficiency assessment of Taiwan public listed companies. International Journal of Academic Research in Accounting, Finance and Management Sciences 4(1): 212–219.\n\nArticle\n  Google Scholar\n\nZhang, W., Q. Cao, and M. Schniederjans. 2004. Neural network earnings per share forecasting models: A comparative analysis of alternative methods. Decision Sciences 5: 205–237.\n\nArticle\n  Google Scholar\n\nDownload references\n\nAcknowledgements\n\nThe authors are grateful to the Editor of the International Journal of Disclosure and Governance for their help and support. The authors also would like to thank the anonymous referees for their constructive comments. This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\n\nAuthor information\n\nAuthors and Affiliations\n\nCollege of Business Administration, Accounting Department, University of Bahrain, Zallaq, Kingdom of Bahrain\n\nGehan A. Mousa\n\nCollege of Business Administration, Management and Marketing Department, University of Bahrain, Zallaq, Kingdom of Bahrain\n\nElsayed A. H. Elamir\n\nPortsmouth Business School, Faculty of Business and Law, University of Portsmouth, Portsmouth, UK\n\nKhaled Hussainey\n\nCorresponding author\n\nCorrespondence to Gehan A. Mousa.\n\nEthics declarations\n\nConflict of interest\n\nThe authors do not have any conflicts of interest to declare.\n\nAdditional information\n\nPublisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nRights and permissions\n\nReprints and permissions\n\nAbout this article\n\nCite this article\n\nMousa, G.A., Elamir, E.A.H. & Hussainey, K. Using machine learning methods to predict financial performance: Does disclosure tone matter?. Int J Discl Gov 19, 93–112 (2022). https://doi.org/10.1057/s41310-021-00129-x\n\nDownload citation\n\nReceived\n24 November 2020\n\nAccepted\n27 July 2021\n\nPublished\n05 September 2021\n\nIssue Date\nMarch 2022\n\nDOI\nhttps://doi.org/10.1057/s41310-021-00129-x\n\nKeywords\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nA performance comparison of machine learning models for stock market ... : \nAn official website of the United States government\n\nA performance comparison of machine learning models for stock market prediction with novel investment strategy\n\nAbstract\n\nStock market forecasting is one of the most challenging problems in today’s financial markets. According to the efficient market hypothesis, it is almost impossible to predict the stock market with 100% accuracy. However, Machine Learning (ML) methods can improve stock market predictions to some extent. In this paper, a novel strategy is proposed to improve the prediction efficiency of ML models for financial markets. Nine ML models are used to predict the direction of the stock market. First, these models are trained and validated using the traditional methodology on a historic data captured over a 1-day time frame. Then, the models are trained using the proposed methodology. Following the traditional methodology, Logistic Regression achieved the highest accuracy of 85.51% followed by XG Boost and Random Forest. With the proposed strategy, the Random Forest model achieved the highest accuracy of 91.27% followed by XG Boost, ADA Boost and ANN. In the later part of the paper, it is shown that only classification report is not sufficient to validate the performance of ML model for stock market prediction. A simulation model of the financial market is used in order to evaluate the risk, maximum draw down and returns associate with each ML model. The overall results demonstrated that the proposed strategy not only improves the stock market returns but also reduces the risks associated with each ML model.\n\nIntroduction\n\nStock markets being one of the essential pillars of the economy have been extensively studied and researched [1]. Forecasting the stock price is an essential objective in the stock market since the higher expected return to the investors can be guaranteed with better prediction [2]. The price and uncertainty in the stock market is predicted by exploiting the patterns found in the past data [3]. The nature of the stock market has always been vague for investors because predicting the performance of a stock market is very challenging. Various factors like the political disturbance, natural catastrophes, international events and much more must be considered in predicting the stock market [4]. The challenge is so huge that even a small improvement in stock market prediction can lead to huge returns.\n\nThe stock market can only move in one of the two directions: upwards (when stock prices rise) or downwards (when stock prices fall) [5]. Generally, there are four ways to analyze the stock market direction [6]. The most basic type of analysis is the fundamental analysis, which is the way of analyzing the stock market by looking at the company’s economic conditions, reports and future projects [7]. The second and most common technique is technical analysis [8]. In this method, the direction of the stock market is anticipated by looking at the stock market price charts and comparing it with its previous prices [9]. The third and most advanced technique is the Machine learning (ML) based analysis that analyzes the market with less human interaction [10]. ML models find the patterns inside historical data based on which they try to forecast the stock market prices for the future. The fourth technique, called sentimental-based analysis, analyzes the stock market prices by the sentiments of other individuals like activity on social media or financial news websites [11].\n\nThe difficulty of the stock market prediction drew the attention of numerous researchers worldwide. A number of papers have been presented that could predict the stock prices based on ML models. These models include Artificial Neural Network (ANN) [12], Decision Tree (DT) [13], Support Vector Machine (SVM) [14], K-Nearest Neighbors (KNN) [15], Random Forest (RF) [16] and Long Short-Term Memory networks (LSTM) [17]. The proposed systems either used a single ML model optimized for specific stocks [18–20], or multiple ML models in order to analyze their performance on different stocks [21–24]. Many advanced techniques like hybrid models were also employed in order to improve prediction accuracy [25–27].\n\nDifferent ML models like RF and stochastic gradient boosting were used to predict the prices of Gold and Silver with an accuracy of more than 85% [18]. A novel model based on SVM and Genetic Algorithm, called Genetic Algorithm Support Vector Machine (GASVM), was proposed to forecast the direction of Ghana Stock Exchange [19]. The proposed model achieved an accuracy of 93.7% for a 10-day stock price movement outperforming other traditional ML models. The Artificial Neural Network Garch (ANNG) model was used to forecast the uncertainty in oil prices [20]. In this model, first, the GARCH model is used to predict the oil price. This prediction is then used as input to ANN for improvement in the overall commodity price forecast by 30%.\n\nDifferent ML models perform differently on the same historical data. Their performance depends on the type of data and the duration for which the past data is available. In many recent papers, multiple ML models were used on the same financial time series data to predict the future price of the stock to see the performance of each ML model [21–24]. Comparative analysis of nine ML and two Deep Learning (DL) models was performed on Tehran stock market [21]. The main purpose of this analysis was to compare the accuracy of different models on continuous and binary datasets. The binary dataset was found to increase the accuracy of models. In [22], four ML models (ANN, SVM, Subsequent Artificial Neural Network (SANN) and LSTM) were used to predict the Bitcoin prices using different time frames. The results show that SANN was able to predict the Bitcoin prices with an accuracy of 65%, whereas LSTM showed an accuracy of 53% only. In another comparative study [23], four ML models (Multi-Layer Perceptron (MLP), SVM and RF) were used to forecast the prices for different crypto-currencies like Bitcoin, Ethereum, Ripple and Litecoin using their historical prices. MLP outperformed all other models with an accuracy ranging from 64 to 72%. Similar study was performed in [24] showing the performance comparison of different ML models on the same data.\n\nIn some recent studies, hybrid models (a combination of different ML models) are used to forecast stock prices. A hybrid model designed with the SVM and sentimental-based technique was proposed for Shanghai Stock Exchange prediction [25]. This hybrid model was able to achieve the accuracy of 89.93%. A system consisting of k-mean clustering and ensemble learning technique was developed to predict the Chinese stock market [26]. The hybrid prediction model obtained the best forecasting accuracy of the stock price on Chinese stock market. Another hybrid framework was developed in [27] for the Indian Stock Market, this model was developed using SVM with different kernel functions and KNN to predict profit or loss. The proposed system was used to predict the future of stock value. Although the accuracy of the hybrid systems is much higher but they are too complex to be implemented in real-life. Furthermore, a comparative analysis of the prior and proposed study has been shown in Table 1.\n\nTable 1. Comparative analysis of previous and proposed study.\n\nIn almost all the proposed ML-based systems, a primary limitation has been observed in the empirical results. The performance of the ML models were only gauged by their classification ability. Although, it is one of the important parameters being used for the evaluation of the ML model, but it is insufficient to determine the performance of the ML model for stock market prediction. The classification metrics do not take into the account some important factors like returns, maximum draw down, risk-to-reward ratio, transactional cost and the risks associated with each ML model. These factors must be considered in the evaluation of ML models for stock market predictions.\n\nResearch cContributions\n\nThe following are the major contributions of paper:\n\nA performance comparison of nine ML models trained using the traditional methodology for stock market prediction using both performance metrics and financial system simulations.\n\nProposing a novel strategy to train the ML models for financial markets that perform much better than the traditional methodologies.\n\nProposing a novel financial system simulation that provides financial performance metrics like returns, maximum drawdown and risk-to-reward ratio for each ML model.\n\nPaper organization\n\nThe rest of the paper is organized as follows: The next section explains the proposed methodology used in training nine ML models for stock market prediction. Section III analyses the outcomes of simulation models in detail. This section consists of ML models simulation as well as Financial models simulations. The conclusions and future directions are discussed in Sections IV and V respectively.\n\nMethodology\n\nIn this paper, a software approach is used to apply different ML algorithms to predict the direction of the stock market for Tesla Inc. [28]. This prediction system is implemented in Python using frameworks like Scikit-learn [29], Pandas [30], NumPy [31], Alpaca broker [32] and Plotly [33].\n\nThe flowchart of the methodology is illustrated in Fig 1. The first step is to import the stock market data from Alpaca broker and preprocess it using various techniques. The imported stock market data has some information that is not needed in the proposed system. This unwanted data, like trade counts and volume-weighted average price, is removed in the preprocessing stage. Preprocessing also involves handling missing stock prices and cleaning data from unnecessary noise. Missing values can be estimated using interpolation techniques or just by taking the mean value of the point before and after the missing point.\n\nFig 1. Flow chart of the proposed prediction system.\n\nTraditionally, the stock price at the end of the day (EOD) is used in ML-based systems. The variation in the stock price is usually the most in the first hour after the market is open. So, stock price within this hour is more effective than the EOD stock price. The direction of the market is set by the business done in this hour. So, in this paper, the stock price after 15 minutes, when the stock market is open, will also be extracted. The results from the stock price at EOD will be compared with the results from the proposed 15 minutes strategy.\n\nOnce the stock price data has been extracted, the subsequent stage involves computing various input features from the technical indicators and statistical formulas. Nine input features, listed in Table 2, are selected for the prediction purposes. These calculated input features are subjected to overfitting tests. These tests are essential because overfit data can cause reduction in the accuracy of the ML models [34].\n\nTable 2. Selected input feature variables for ML models.\n\nRSI = Relative Strength Index, SMA = Simple Moving Average, ADX = Average Directional Movement Index\n\nFollowing the overfitting tests, the input data is divided into training and testing data. The data is then normalized using Min-Max normalization technique to prevent the biasing phenomenon. Normalization is performed using the following Eq (1):\n\nThe input features and output variables are provided to the ML models in order to detect the patterns within the training data. Various ML models have been employed in this study. Table 3 shows the selected nine ML models to predict the direction of the stock market in this paper. The optimal parameters for each ML models are selected through GridSearchCV [35]. A scikit-learn function that helps in selecting best performing parameter for a particular model. After choosing the optimal parameters, the ML models are trained and tested.\n\nTable 3. Selected ML models for stock market prediction.\n\nIn the next step, the outcome of the trained ML models is assessed using some performance metrics. There are a number of classification metrics that can be used to evaluate the performance of an ML algorithm [45]. Usually, three most powerful measures are chosen to classify these models with respect to their performance. The measures are accuracy, F1 score and Receiver Operator Characteristic and Area Under the Curve (ROC_AUC) [46]. The equations for Accuracy and F1_score are shown below:\n\nFor evaluation purposes the accuracy, ROC_AUC and F1_score are useful measures, however, they are not sufficient for all problems. Recall and precision are two additional well-known metrics for classification problems [47, 48]. The expression for Recall and Precision are also shown in below:\n\nAdditionally, a confusion matrix is used to summarize the performance of each ML model. It provides detailed insight into ML predictions by indicating False Positives (FP), True Positives (TP), False Negatives (FN) and True Negatives (TN) [49]. False Positives show that the model prediction is true while the real sample is false; True Positives show that the model prediction and the real sample both are true; False Negatives represent that the model prediction is false while the real sample is true; True Negatives show that the model prediction and real sample both are false.\n\nIn the next step, a novel financial model is developed and simulated to analyze the performance of the trained ML models. The financial performance metrics like Sharpe ratio, maximum drawdown, cumulative return and annual return [50] are used to analyze the performance of the trained ML models.\n\nThe Sharpe ratio is the measure of risk-free return while the maximum drawdown is the greatest decline in the value of the portfolio [51]. The equations for Sharpe ratio and maximum drawdown are shown in below:\n\nwhere Rp = Return of portfolio, Rf = Risk free rate, σ = Std of portfolio excess return, P = Peak value before largest drop, and L = Lowest value before new high.\n\nAnnual return is the return gained during the period of one year while the cumulative return is the total return on the invested capital within any specific time frame. The expressions for annual return and cumulative return are shown in Eqs (8) and (9).\n\nwhere, E = Ending value, I = Initial value and n = Number of years.\n\nExperimental results\n\nDataset description and project specifications\n\nTesla Inc. is a major American automobile company producing technologically advanced electric vehicles. The company has recently obtained a lot of attention due to its stock prices. A drastic increase in revenue in the year 2021 made Tesla stocks very appealing for capitalists and investors around the world as shown in Table 4 [52].\n\nTable 4 shows the annual growth of Tesla from 2016 to 2021. There has been an increase of almost 70.67% in the year 2021. By taking into account the stock volatility in the previous years and its recent growth, Tesla Inc. is an ideal candidate for this study.\n\nThe stock prices for Tesla Inc. from 2016 to 2021 are considered for experimental evaluations in this paper. Furthermore, the data is split into training data and test datasets. Table 5 shows the ranges of our datasets. The stock market data for Tesla Inc., downloaded from Alpaca broker, from 2016 to 2021 is shown in Fig 2. Additionally, the project specifications can be found in Table 6.\n\nMachine learning models simulation\n\nFirst, the optimal parameters settings for the nine ML models are selected through GridSearchCV. The selected optimal parametric settings for each model are shown in Table 7.\n\nThe simulations for stock market prediction are performed using Python on a Jupiter notebook. ML models were evaluated using Tesla Inc. stock prices for a 1-day time frame and 15-min time interval strategy. These models were first trained on the data from Jan 01, 2016 to Nov 15, 2020. The trained models were then validated on the test data from Nov 16, 2020 to Dec 31, 2021 as shown in Table 5.\n\nTables 8–10 show the classification report for nine different ML models. Tables 8 and 9 show the performance metrics for different ML models for a 1-day time frame and 15-min time interval strategy. These tables list the accuracy, F1 score, ROC AUC, precision and recall in percentage for all of the ML models. Table 10 shows the confusion matrix for the ML models. It lists the number of correct and wrong predictions made by each ML model.\n\nTable 8 shows the performance metrics of nine ML models optimized for a 1-day time frame. As shown in the table, the Logistic Regression achieved the highest accuracy of 85.51% while the Naive Bayes model is found to be the least accurate model with an accuracy of 73.49%. Other classification metrics in Table 8 show a similar tendency with Logistic Regression having the best performance followed by XG Boost and Random Forest.\n\nThe confusion matrix in Table 10 shows a similar trend. For Logistic Regression, the True Positives are 132 and the False Positives are 26 for the ‘Move Up’ class. The True Negatives are 110 and the False Negatives are 15 for the ‘Move Down’ class.\n\nBased on the discussion above, it can be seen that the performance of Logistic Regression model is better than the rest of the models for 1-day time frame. Even though its accuracy among the nine ML models is only 85.51%.\n\nThe graphical illustration of the predictions made by the Logistic Regression model for a 1-day time frame can be seen in Fig 3. It can be seen that the trained Logistic Regression model is able to make more profits than losses. However, it is interesting to note that sometimes the predictions made by the LR model are wrong in the consecutive trades that results in more drawdown. For example, during the period 180 to 230 days, there are a total of 6 trades executed, out of which 4 are losses and 2 are profitable trades.\n\nIn this paper, a novel 15-min time interval strategy has been proposed. In this strategy, the initial 15-min time interval is filtered out from 1-day time frame. Then the filtered 15-min time frame is used to train and validate the ML models in order to make prediction for the time frame of 1-day.\n\nTable 9 shows the performance metrics of the ML models optimized for a 15-min time interval strategy. As shown in Table, the Random Forest achieved the highest accuracy of 91.27% followed by XG Boost and ADA Boost model. The KNN model is found to be the least accurate model with an accuracy of 80.53%. Other classification metrics in Table 9 show a similar tendency with the Random Forest having the best performance model.\n\nThe confusion matrix in Table 10 shows a similar trend. For Random Forest, the True Positives are 130 and the False Positives are 15 for the ‘Move Up’ class. The True Negatives are 142 and the False Negatives are 11 for the ‘Move Down’ class. When the results in Tables 8 and 9 are compared, it can be observed that by employing the proposed methodology, the performance of all the ML models has been greatly improved.\n\nThe graphical illustration of the predictions made by the Random Forest model is shown in Fig 4, it shows the loss and profit in trades. It can also be observed that by using our proposed strategy, the number of consecutive losses has also been reduced. As shown in Fig 4(b), there are only 2 consecutive losses, which occurred during the period of 150 to 200. Factually, the proposed methodology has not only improved the performance metrics of the ML models but it also reduced the number of consecutive losses.\n\nFinancial models simulation\n\nIn this section, a novel financial simulation model is built that is able to make investment based on the decision of the ML model. Each ML model is evaluated using financial parameters to validate their performance and suitability for real-time stock market trading. The performance of ML models is gauged using cumulative return, annual return, maximum drawdown, Sharpe ratio and capital in hand at the end of the investment period.\n\nInitially, a USD 10k is invested. A commission fee of 0.1% (Alpaca standard commission fee) is set for each buy or sell trade. Based on the prediction by the ML model, a decision regarding buying, holding or shorting a share is taken. A single share is bought or sold on each trade to validate the performance of ML models.\n\nFigs 5 and 6 show the portfolio performance of ML models on Tesla Inc. stocks for a 1-day time frame and 15-min time interval strategy. These figures show how initial capital is used to buy and sell shares based on the decision made by the ML models. Each box in the figure represents one full year from Jan 01 till Dec 31. The portfolio of each ML model is compared to a benchmark that serves as a reference for all models. This benchmark is obtained using the positive gains of stock prices.\n\nThe simulated outcomes of the ML models to forecast the stock price of Tesla Inc. for a 1-day period are displayed in Table 11. In the previous section, it was shown that Logistic Regression had the highest accuracy as compared to the other ML models. Therefore, it is expected that this ML model will generate highest revenue. However, the outcome of the financial simulations shows different results. It can be seen in Table 11 that the Random Forest is the best ML model with an ending capital of USD 28,966. It has a cumulative return of 189.66%, and an annual return of 19.48%, with the highest Sharpe ratio of 0.68. The Random Forest did poorly at first but after the 2019 financial market crisis, it outperformed all other ML models. The maximum drawdown of the Random Forest model is -37.21% which happened during 2019 financial crisis as shown in Fig 7. This is the lowest drawdown by any ML model.\n\nThe reason for better revenue generation by the Random Forest model is the quality of each True Positive and True Negative outcome. Even though the accuracy of the model is inferior to the Logistic Regression, each of its correct prediction resulted in more profit. The annual growth of Tesla Inc. from 2020 to 2021 is more than 70% as shown in Table 4. Any correct prediction during this time will result in greater revenue generation. Random Forest model outperformed all other models during this time as shown in Fig 5. Among the ML models, the Naive Bayes model shows the worst performance. Fig 5 shows that the Naive Bayes model is negative most of the time during the simulation. It is the only model with a negative cumulative return of -19.16% and worst Sharpe ratio of 0.1.\n\nThe portfolio performance of the ML models using the proposed approach of a 15-min time interval strategy is shown in Fig 6. This figure shows that the performance of some of the models has improved significantly when compared with a 1-day time frame. It can also be noticed that the models maintained their stability throughout the financial crisis of 2019, which indicates a significant improvement in the real-time performance of the models.\n\nTable 12 displays the outcome of the financial model simulation of ML models trained and validated on Tesla Inc. stocks for a 15-min time interval strategy. As expected, it can be seen that the Random Forest is the best performing model with an ending capital of USD 25,300. It records a cumulative return of 153% and annual return of 16.80% with the highest Sharpe ratio of 0.79. The maximum drawdown by the Random Forest model is—35.09% as shown in Fig 8, but it still able to generate the highest ending capital.\n\nThe above discussion shows that KNN is the worst performing model on the proposed strategy. Although, Random Forest is the best model in terms of portfolio returns but ANN is the most rewarding model with a Sharpe ratio of 0.91 on the proposed 15-min time interval strategy.\n\nConclusion\n\nIn this paper, nine ML models are used to predict the direction of the Tesla Inc. stock prices. The performance of this stock is first assessed for a 1-day time frame followed by a proposed 15-min time interval strategy. Following the traditional methodology, the Logistic Regression achieved the highest accuracy of 85.51% while Naive Bayes model is found to be the least accurate model with an accuracy of 73.49%. The proposed strategy significantly improved the classification performance of the ML models. With this strategy, the Random Forest model achieved the highest accuracy of 91.93% followed by XG Boost and ADA Boost. Conversely, the KNN model is found to be the least accurate model with an accuracy of 80.53%.\n\nIn this paper, it was shown that only classification metrics are not enough to justify the performance of ML models in the stock market. These metrics do not consider important factors like risk, maximum draw down and returns associate with each ML model. A simulation model of the financial market is used to simulate the trained ML models so that their performance is gauged with actual investment strategies. The evaluated results revealed that although some models are performing well in terms of portfolio returns on a traditional methodology but models on the proposed 15-min time frame strategy are significantly better in terms of risk to reward ratio and maximum drawdown. The evaluated result shows that Random Forest outperformed other models in terms of returns in both 1-day and 15-min time interval strategy.\n\nSome other interesting observations are revealed by the comparison of the classification and financial results. The Logistic Regression model has the highest accuracy for a 1-day time frame data. So, it was expected that this ML model will generate the highest revenue. However, the outcome of the financial simulations showed different results. Similarly, the accuracy of the Random Forest model for a 15-min time interval strategy was much higher than the accuracy of the Random Forest model for a 1-day time frame. But instead of generating higher revenue on 15-min time frame strategy, it generated higher revenue on 1-day time frame. The above discussion revealed that however, the accuracy of the ML models is an important factor but the quality of each true positive outcome and true negative outcome is an equally important factor in the performance evaluation of the ML models for stock market prediction.\n\nThe overall results show that the proposed strategy has not only improved classification metrics but it also enhanced the stock market returns, risks and risk to reward ratio of each ML model. Additionally, the results also revealed that how important it is to consider both classification as well as financial analysis to evaluate the performance of the ML model on stock market.\n\nSupporting information\n\nThe data and script has been uploaded to GitHub. It can be accessed using the following link: https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques/.\n\n(IPYNB)\n\nData Availability\n\nData for this study is publicly available from the GitHub repository (https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques).\n\nFunding Statement\n\nThe authors received no specific funding for this work.\n\nReferences\n\nAssociated Data\n\nThis section collects any data citations, data availability statements, or supplementary materials included in this article.\n\nSupplementary Materials\n\nThe data and script has been uploaded to GitHub. It can be accessed using the following link: https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques/.\n\n(IPYNB)\n\nData Availability Statement\n\nData for this study is publicly available from the GitHub repository (https://github.com/AzazHassankhan/Machine-Learning-based-Trading-Techniques).\n\nArticles from PLOS ONE are provided here courtesy of PLOS\n\nConnect with NLM\n\nNational Library of Medicine\n8600 Rockville Pike\nBethesda, MD 20894\n",
        "Machine learning models employed in Market Trend Analysis": "Leveraging Machine Learning and Deep Learning Models for ... - Springer : \nYour privacy, your choice\n\nWe use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\n\nBy accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\n\nSee our privacy policy for more information on the use of your personal data.\n\nManage preferences for further information and to change your choices.\n\nLeveraging Machine Learning and Deep Learning Models for Enhanced Stock Price Prediction: A State-of-the-Art Analysis\n\nPart of the book series: Lecture Notes in Networks and Systems ((LNNS,volume 1259))\n\nIncluded in the following conference series:\n\n167 Accesses\n\nAbstract\n\nThe prediction of stock prices has recently gained considerable attention as a complex and challenging issue within the realms of economics and finance. Stock prices are affected by various factors, such as the business environment, stock market operations, inflation, and unexpected events. Since the stock market is volatile and nonlinear, finding the most effective model to forecast stock prices is one of the most challenging problems. Researchers have increasingly explored various Machine Learning (ML) and Deep Learning (DL) models to address this issue due to their capacity to handle time series data and nonlinear patterns. These models often outperform traditional approaches in predicting stock prices with high accuracy and lower root mean square error (RMSE). This paper reviews various works that have utilized ML approaches for stock price prediction, covering research published between 2017 and 2023. This literature review discusses various techniques, their performance, limitations, and future work. We assess the latest techniques in many studies, including ML and DL models. The findings of this review conclude that Neural Networks (NNs) are the most commonly used approaches in predicting stock prices due to their effectiveness in detecting complex patterns in financial data.\n\nThis is a preview of subscription content, log in via an institution to check access.\n\nAccess this chapter\n\nSubscribe and save\n\nBuy Now\n\nTax calculation will be finalised at checkout\n\nPurchases are for personal use only\n\nInstitutional subscriptions\n\nReferences\n\nZhong, X., Enke, D.: Forecasting daily stock market return using dimensionality reduction. Expert Syst. Appl. 67, 126–139 (2017)\n\nArticle\n  Google Scholar\n\nBhattacharjee, I., Bhattacharja, P.: Stock price prediction: a comparative study between traditional statistical approach and machine learning approach. In: 2019 4th International Conference on Electrical Information and Communication Technology (EICT), pp. 1–6 (2019)\n\nGoogle Scholar\n\nNikou, M., Mansourfar, G., Bagherzadeh, J.: Stock price prediction using DEEP learning algorithm and its comparison with machine learning algorithms. Intell. Syst. Acc., Finance Manag. 26, 164–174 (2019)\n\nArticle\n  Google Scholar\n\nHenrique, B.M., Sobreiro, V.A., Kimura, H.: Literature review: machine learning techniques applied to financial market prediction. Expert Syst. Appl. 124, 226–251 (2019)\n\nArticle\n  Google Scholar\n\nGandhmal, D.P., Kumar, K.: Systematic analysis and review of stock market prediction techniques. Comput. Sci. Rev. 34, 100190 (2019)\n\nArticle\n  MathSciNet\n  Google Scholar\n\nShah, J., Vaidya, D., Shah, M.: A comprehensive review on multiple hybrid deep learning approaches for stock prediction. Intell. Syst. Appl. 16, 200111 (2022)\n\nMATH\n  Google Scholar\n\nSoni, P., Tewari, Y., Krishnan, D.: Machine Learning approaches in stock price prediction: a systematic review. J. Phys.: Conf. Ser. 2161, 012065 (2022)\n\nGoogle Scholar\n\nMohan, S., Mullapudi, S., Sammeta, S., Vijayvergia, P., Anastasiu, D.C.: Stock price prediction using news sentiment analysis. In: 2019 IEEE Fifth International Conference on Big Data Computing Service and Applications (BigDataService), pp. 205–208 (2019)\n\nGoogle Scholar\n\nJin, Z., Yang, Y., Liu, Y.: Stock closing price prediction based on sentiment analysis and LSTM. Neural Comput. Appl. 32, 9713–9729 (2020)\n\nArticle\n  MATH\n  Google Scholar\n\nGupta, R., Chen, M.: Sentiment analysis for stock price prediction. In: 2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pp. 213–218 (2020)\n\nGoogle Scholar\n\nValencia, F., Gómez-Espinosa, A., Valdés-Aguirre, B.: Price movement prediction of cryptocurrencies using sentiment analysis and machine learning. Entropy 21, 589 (2019)\n\nArticle\n  Google Scholar\n\nHafid, A., Hafid, A.S., Makrakis, D.: Bitcoin price prediction using machine learning and technical indicators. In: International Symposium on Distributed Computing and Artificial Intelligence, pp. 275–284 (2023)\n\nGoogle Scholar\n\nParray, I.R., Khurana, S.S., Kumar, M., Altalbe, A.A.: Time series data analysis of stock price movement using machine learning techniques. Soft. Comput. 24(21), 16509–16517 (2020). https://doi.org/10.1007/s00500-020-04957-x\n\nArticle\n  Google Scholar\n\nVijh, M., Chandola, D., Tikkiwal, V.A., Kumar, A.: Stock closing price prediction using machine learning techniques. Procedia Comput. Sci. 167, 599–606 (2020)\n\nArticle\n  Google Scholar\n\nKumar, I., Dogra, K., Utreja, C., Yadav, P.: A comparative study of supervised machine learning algorithms for stock market trend prediction. In: 2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT), pp. 1003–1007 (2018)\n\nGoogle Scholar\n\nBalaji, A.J., Ram, D.H., Nair, B.B.: Applicability of deep learning models for stock price forecasting an empirical study on BANKEX data. Procedia Comput. Sci. 143, 947–953 (2018)\n\nArticle\n  MATH\n  Google Scholar\n\nDing, G., Qin, L.: Study on the prediction of stock price based on the associated network model of LSTM. Int. J. Mach. Learn. Cybern. 11, 1307–1317 (2020)\n\nArticle\n  MATH\n  Google Scholar\n\nMukherjee, S., Sadhukhan, B., Sarkar, N., Roy, D., De, S.: Stock market prediction using deep learning algorithms. CAAI Trans. Intell. Technol. 8, 82–94 (2023)\n\nArticle\n  MATH\n  Google Scholar\n\nMoghar, A., Hamiche, M.: Stock market prediction using LSTM recurrent neural network. Procedia Comput. Sci. 170, 1168–1173 (2020)\n\nArticle\n  MATH\n  Google Scholar\n\nSunny, M.A.I., Maswood, M.M.S., Alharbi, A.G.: Deep learning-based stock price prediction using LSTM and bi-directional LSTM model. In: 2020 2nd Novel Intelligent and Leading Emerging Sciences Conference (NILES), pp. 87–92 (2020)\n\nGoogle Scholar\n\nYadav, A., Jha, C., Sharan, A.: Optimizing LSTM for time series prediction in Indian stock market. Procedia Comput. Sci. 167, 2091–2100 (2020)\n\nArticle\n  MATH\n  Google Scholar\n\nAsiful, M., Hossain, R.K., THulasiram, R., Bruce, N.D., Wang, Y.: Hybrid deep learning model for stock price prediction in IEEE Symposium Series on Computational Intelligence, SSCI, Bangalore, India (2018)\n\nGoogle Scholar\n\nGhosh, B.P., et al.: Deep learning in stock market forecasting: comparative analysis of neural network architectures across NSE and NYSE. J. Comput. Sci. Technol. Stud. 6, 68–75 (2024)\n\nArticle\n  MATH\n  Google Scholar\n\nReddy, N.N., Naresh, E., BP, V.K.: Predicting stock price using sentimental analysis through twitter data in 2020 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT), pp. 1–5 (2020)\n\nGoogle Scholar\n\nHashish, I.A., Forni, F., Andreotti, G., Facchinetti, T., Darjani, S.: A hybrid model for bitcoin prices prediction using hidden Markov models and optimized LSTM networks. In: 2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA), pp. 721–728 (2019)\n\nGoogle Scholar\n\nPolamuri, S.R., Srinivas, K., Mohan, A.K.: Multi-model generative adversarial network hybrid prediction algorithm (MMGAN-HPA) for stock market prices prediction. J. King Saud Univ. Comput. Inf. Sci. 34, 7433–7444 (2022)\n\nGoogle Scholar\n\nDi Persio, L., Honchar, O., et al.: Artificial neural networks architectures for stock price prediction: comparisons and applications. Int. J. Circ., Syst. Sig. Process. 10, 403–413 (2016)\n\nMATH\n  Google Scholar\n\nLu, W., Li, J., Li, Y., Sun, A., Wang, J.: A CNN-LSTM-based model to forecast stock prices. Complexity 2020, 1–10 (2020)\n\nMATH\n  Google Scholar\n\nXiao, C., Xia, W., Jiang, J.: Stock price forecast based on combined model of ARI-MA-LS-SVM. Neural Comput. Appl. 32(10), 5379–5388 (2020). https://doi.org/10.1007/s00521-019-04698-5\n\nArticle\n  MATH\n  Google Scholar\n\nMintarya, L.N., Halim, J.N., Angie, C., Achmad, S., Kurniawan, A.: Machine learning approaches in stock market prediction: a systematic literature review. Procedia Comput. Sci. 216, 96–102 (2023)\n\nArticle\n  Google Scholar\n\nDownload references\n\nAuthor information\n\nAuthors and Affiliations\n\nLISA, Ecole Nationale des Sciences Appliquées (ENSA), 72, Fez, Morocco\n\nSafae Belamfedel Alaoui & Mhamed Sayyouri\n\nESISA Analytica, Ecole Supérieure d’Ingénierie En Sciences Appliquées (ESISA), 505, Fez, Morocco\n\nAbdelatif Hafid\n\nComputer and Information Science Department, Fordham University, New York, USA\n\nMohamed Rahouti\n\nCorresponding author\n\nCorrespondence to Safae Belamfedel Alaoui .\n\nEditor information\n\nEditors and Affiliations\n\nVellore Institute of Technology University, Vellore, Tamil Nadu, India\n\nRavikumar Chinthaginjala\n\nKielce University of Technology, Kielce, Poland\n\nPawel Sitek\n\nDepartment of Computer Science, Imam Abdulrahman Bin Faisal University (KSA), Dammam, Saudi Arabia\n\nNasro Min-Allah\n\nOsaka Institute of Technology, Osaka, Japan\n\nKenji Matsui\n\nUniversity Rey Juan Carlos, Madrid, Spain\n\nSascha Ossowski\n\nBiotechnology, Intelligent Systems and Educational Technology (BISITE) Research Group, University of Salamanca, Salamanca, Spain\n\nSara Rodríguez\n\nRights and permissions\n\nReprints and permissions\n\nCopyright information\n\n© 2025 The Author(s), under exclusive license to Springer Nature Switzerland AG\n\nAbout this paper\n\nCite this paper\n\nBelamfedel Alaoui, S., Hafid, A., Sayyouri, M., Rahouti, M. (2025). Leveraging Machine Learning and Deep Learning Models for Enhanced Stock Price Prediction: A State-of-the-Art Analysis. In: Chinthaginjala, R., Sitek, P., Min-Allah, N., Matsui, K., Ossowski, S., Rodríguez, S. (eds) Distributed Computing and Artificial Intelligence, 21st International Conference. DCAI 2024. Lecture Notes in Networks and Systems, vol 1259. Springer, Cham. https://doi.org/10.1007/978-3-031-82073-1_6\n\nDownload citation\n\nDOI\nhttps://doi.org/10.1007/978-3-031-82073-1_6\n\nPublished\n18 February 2025\n\nPublisher Name\nSpringer, Cham\n\nPrint ISBN\n978-3-031-82072-4\n\nOnline ISBN\n978-3-031-82073-1\n\neBook Packages\nIntelligent Technologies and Robotics\nIntelligent Technologies and Robotics (R0)\n\nPublish with us\n\nPolicies and ethics\n\nDiscover content\n\nPublish with us\n\nProducts and services\n\nOur brands\n\n178.135.19.63\n\nNot affiliated\n\n© 2025 Springer Nature\n\nA Comparative Analysis of Machine Learning and Deep Learning Techniques ... : \nA Comparative Analysis of Machine Learning and Deep Learning Techniques for Accurate Market Price Forecasting\n\nAbstract\n\nShare and Cite\n\nShobayo, O.; Adeyemi-Longe, S.; Popoola, O.; Okoyeigbo, O. A Comparative Analysis of Machine Learning and Deep Learning Techniques for Accurate Market Price Forecasting. Analytics 2025, 4, 5. https://doi.org/10.3390/analytics4010005\n\nShobayo O, Adeyemi-Longe S, Popoola O, Okoyeigbo O. A Comparative Analysis of Machine Learning and Deep Learning Techniques for Accurate Market Price Forecasting. Analytics. 2025; 4(1):5. https://doi.org/10.3390/analytics4010005\n\nShobayo, Olamilekan, Sidikat Adeyemi-Longe, Olusogo Popoola, and Obinna Okoyeigbo. 2025. \"A Comparative Analysis of Machine Learning and Deep Learning Techniques for Accurate Market Price Forecasting\" Analytics 4, no. 1: 5. https://doi.org/10.3390/analytics4010005\n\nShobayo, O., Adeyemi-Longe, S., Popoola, O., & Okoyeigbo, O. (2025). A Comparative Analysis of Machine Learning and Deep Learning Techniques for Accurate Market Price Forecasting. Analytics, 4(1), 5. https://doi.org/10.3390/analytics4010005\n\nArticle Metrics\n\nCitations\n\nArticle Access Statistics\n\nFurther Information\n\nGuidelines\n\nMDPI Initiatives\n\nFollow MDPI\n\nSubscribe to receive issue release notifications and newsletters from MDPI journals\n\nEL-MTSA: Stock Prediction Model Based on Ensemble Learning and ... - MDPI : \nEL-MTSA: Stock Prediction Model Based on Ensemble Learning and Multimodal Time Series Analysis\n\nAbstract\n\n1. Introduction\n\n2. Related Work\n\n3. Model Construction\n\n4. Experimental Results and Discussion\n\n5. Conclusions\n\nAuthor Contributions\n\nFunding\n\nInstitutional Review Board Statement\n\nInformed Consent Statement\n\nData Availability Statement\n\nConflicts of Interest\n\nReferences\n\nShare and Cite\n\nKong, J.; Zhao, X.; He, W.; Yang, X.; Jin, X. EL-MTSA: Stock Prediction Model Based on Ensemble Learning and Multimodal Time Series Analysis. Appl. Sci. 2025, 15, 4669. https://doi.org/10.3390/app15094669\n\nKong J, Zhao X, He W, Yang X, Jin X. EL-MTSA: Stock Prediction Model Based on Ensemble Learning and Multimodal Time Series Analysis. Applied Sciences. 2025; 15(9):4669. https://doi.org/10.3390/app15094669\n\nKong, Jianlei, Xueqi Zhao, Wenjuan He, Xiaobo Yang, and Xuebo Jin. 2025. \"EL-MTSA: Stock Prediction Model Based on Ensemble Learning and Multimodal Time Series Analysis\" Applied Sciences 15, no. 9: 4669. https://doi.org/10.3390/app15094669\n\nKong, J., Zhao, X., He, W., Yang, X., & Jin, X. (2025). EL-MTSA: Stock Prediction Model Based on Ensemble Learning and Multimodal Time Series Analysis. Applied Sciences, 15(9), 4669. https://doi.org/10.3390/app15094669\n\nArticle Metrics\n\nCitations\n\nArticle Access Statistics\n\nFurther Information\n\nGuidelines\n\nMDPI Initiatives\n\nFollow MDPI\n\nSubscribe to receive issue release notifications and newsletters from MDPI journals\n"
    },
    "AnalyzedArticles": {
        "Machine learning models employed in Credit Risk Assessment": {
            "Article_Summary": "The articles discuss the application of machine learning models in credit risk assessment, focusing on how AI and ML techniques can improve predictive accuracy by leveraging both structured and unstructured data sources. The research highlights the potential of advanced models to capture complex borrower behaviors, especially for individuals with limited credit histories, while also addressing challenges of model transparency, bias, and regulatory compliance.",
            "ML_Models": "Support Vector Machines (SVM), Multilayer Perceptron (MLP), Ensemble Learning Models"
        },
        "Machine learning models employed in Customer Segmentation and Targeting": {
            "Article_Summary": "The articles discuss customer segmentation using machine learning, focusing on how ML algorithms like K-means clustering can help businesses divide customers into meaningful groups based on various characteristics such as demographics, behavior, and purchasing patterns. The goal is to enable more personalized marketing strategies, improve customer targeting, and enhance overall business performance.",
            "ML_Models": "Random Forest Clustering, Gaussian Mixture Models, Spectral Clustering"
        },
        "Machine learning models employed in Financial Performance Prediction": {
            "Article_Summary": "The paper presents a comprehensive study on using machine learning models for stock market prediction, focusing on Tesla Inc. stock prices. The research compares nine different machine learning models using two strategies: a traditional 1-day time frame and a novel 15-minute time interval approach. The study emphasizes that classification metrics alone are insufficient to evaluate model performance and introduces a financial simulation model to assess returns, risks, and drawdown.",
            "ML_Models": "Random Forest, Artificial Neural Networks (ANN), Logistic Regression"
        },
        "Machine learning models employed in Market Trend Analysis": {
            "Article_Summary": "The articles focus on leveraging advanced machine learning and deep learning techniques for stock price prediction, highlighting the complexity of forecasting stock prices due to market volatility and nonlinear patterns. The research emphasizes the effectiveness of neural networks in detecting complex financial data patterns and explores various ML and DL approaches to improve prediction accuracy.",
            "ML_Models": "LSTM (Long Short-Term Memory), CNN-LSTM Hybrid Model, Generative Adversarial Networks (GAN)"
        }
    },
    "Relationship": {
        "Machine learning models employed in Credit Risk Assessment": [
            "The ML models (SVM, MLP, Ensemble Learning) are highly relevant for credit risk assessment using both datasets. The banking table provides customer demographic and behavioral data, while the data table contains detailed financial health indicators. Together, they provide comprehensive inputs for predicting default risk, bankruptcy probability, and creditworthiness."
        ],
        "Machine learning models employed in Customer Segmentation and Targeting": [
            "Customer Segmentation and Targeting models (Random Forest Clustering, Gaussian Mixture Models, Spectral Clustering) can leverage the banking dataset to group customers based on demographic attributes and behavioral patterns. These models can identify distinct customer segments with similar characteristics, allowing for personalized marketing strategies and improved campaign effectiveness."
        ],
        "Machine learning models employed in Financial Performance Prediction": [
            "The financial performance prediction models (Random Forest, ANN, Logistic Regression) align perfectly with the 'data' table which contains extensive financial metrics and ratios. These models can analyze patterns in financial indicators to predict outcomes like bankruptcy risk, profitability trends, and overall financial health. The banking table provides complementary demographic and economic context that could enhance prediction accuracy."
        ],
        "Machine learning models employed in Market Trend Analysis": [
            "Market Trend Analysis models like Time Series Analysis, ARIMA, Prophet, and RNNs can leverage the banking dataset's economic indicators and temporal data to forecast market movements. The data table contains numerous financial metrics that can be used to identify patterns and predict future market behavior. The proposed LSTM, CNN-LSTM Hybrid, and GAN models are particularly well-suited for capturing complex temporal dependencies in financial data."
        ]
    },
    "Needs": {
        "Machine learning models employed in Credit Risk Assessment": [
            "For credit risk assessment, we need numerical and categorical features to train classification models. The banking table provides categorical features (job, education) and binary targets (default), while the data table offers financial ratios as numerical features with 'Bankrupt?' as a binary target. These will be used in SVM for decision boundaries between risky/non-risky clients, MLP for capturing non-linear relationships in financial data, and Ensemble Learning to combine multiple models for improved prediction accuracy. Data preprocessing will include normalization for numerical features and encoding for categorical variables."
        ],
        "Machine learning models employed in Customer Segmentation and Targeting": [
            "For customer segmentation, we need both categorical features (job, marital, education, housing, loan, poutcome) and numerical features (age, campaign, previous, duration). These will be used to train unsupervised clustering models. Categorical variables will require encoding (one-hot or label encoding). The target variable 'y' can be used to evaluate how well segments correlate with conversion rates. The models will create distinct customer groups based on similar characteristics, requiring normalized numerical data and properly encoded categorical data for optimal clustering performance."
        ],
        "Machine learning models employed in Financial Performance Prediction": [
            "For financial performance prediction, we need numerical features from the data table as inputs, with 'Bankrupt?' likely serving as the target variable for classification. The Random Forest and Logistic Regression models would perform binary classification to predict bankruptcy risk, while ANN could handle both classification and regression tasks for predicting various financial outcomes. Banking table features provide economic context that could be encoded (categorical variables) or used directly (numerical variables) to enhance prediction accuracy by incorporating external economic factors."
        ],
        "Machine learning models employed in Market Trend Analysis": [
            "For market trend analysis, we need time-series data with economic indicators and financial metrics. The banking table provides temporal markers and macroeconomic indicators, while the data table offers company-specific financial metrics. These columns will be used to train regression models that predict future values and classification models that identify trend directions. LSTM networks require sequential data formatted as time steps, CNN-LSTM hybrids need data structured for both spatial and temporal pattern recognition, and GANs require sufficient historical data to generate realistic market scenarios. The target variables would typically be future price movements or trend classifications (upward/downward/sideways)."
        ]
    },
    "ModelsPerTopic": {
        "Machine learning models employed in Credit Risk Assessment": "Support Vector Machines (SVM), Multilayer Perceptron (MLP), Ensemble Learning Models",
        "Machine learning models employed in Customer Segmentation and Targeting": "Random Forest Clustering, Gaussian Mixture Models, Spectral Clustering",
        "Machine learning models employed in Financial Performance Prediction": "Random Forest, Artificial Neural Networks (ANN), Logistic Regression",
        "Machine learning models employed in Market Trend Analysis": "LSTM (Long Short-Term Memory), CNN-LSTM Hybrid Model, Generative Adversarial Networks (GAN)"
    },
    "ML_Models1": [
        "Logistic Regression, Random Forest, Gradient Boosting, Neural Networks",
        "K-means Clustering, Hierarchical Clustering, DBSCAN, Decision Trees",
        "XGBoost, LSTM Networks, Support Vector Machines, Ensemble Methods",
        "Time Series Analysis, ARIMA, Prophet, Recurrent Neural Networks"
    ],
    "GPT_Columns": {
        "Machine learning models employed in Credit Risk Assessment": [
            [
                {
                    "banking": [
                        "age",
                        "job",
                        "education",
                        "default",
                        "housing",
                        "loan",
                        "duration",
                        "campaign"
                    ]
                },
                {
                    "data": [
                        "Bankrupt?",
                        "Debt ratio %",
                        "Cash flow rate",
                        "Total debt/Total net worth",
                        "Net Income to Total Assets",
                        "Operating Profit Rate",
                        "Interest Coverage Ratio (Interest expense to EBIT)",
                        "Liability to Equity"
                    ]
                }
            ]
        ],
        "Machine learning models employed in Customer Segmentation and Targeting": [
            [
                {
                    "banking": [
                        "age",
                        "job",
                        "marital",
                        "education",
                        "housing",
                        "loan",
                        "campaign",
                        "previous",
                        "poutcome",
                        "duration",
                        "y"
                    ]
                }
            ]
        ],
        "Machine learning models employed in Financial Performance Prediction": [
            [
                {
                    "data": [
                        "Bankrupt?",
                        "ROA(C) before interest and depreciation before interest",
                        "Operating Gross Margin",
                        "Cash flow rate",
                        "Total debt/Total net worth",
                        "Net Income to Total Assets",
                        "Liability to Equity"
                    ]
                },
                {
                    "banking": [
                        "age",
                        "job",
                        "education",
                        "default",
                        "housing",
                        "loan",
                        "euribor3m"
                    ]
                }
            ]
        ],
        "Machine learning models employed in Market Trend Analysis": [
            [
                {
                    "banking": [
                        "emp_var_rate",
                        "cons_price_idx",
                        "cons_conf_idx",
                        "euribor3m",
                        "nr_employed",
                        "month",
                        "y"
                    ]
                },
                {
                    "data": [
                        "Total Asset Growth Rate",
                        "Net Value Growth Rate",
                        "Total Asset Return Growth Rate Ratio",
                        "Cash Flow to Total Assets",
                        "Net Income to Total Assets",
                        "Total assets to GNP price",
                        "Equity to Liability"
                    ]
                }
            ]
        ]
    },
    "AdjustedColumns": {}
}