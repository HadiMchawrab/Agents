{
    "content1": "About us\n\nCorporate governance\n\nAccessibility\nA plan that shares how the Bank strives to be accessible to everyone\n\nReconciliation Action Plan\nA plan to guide the Bank along its Reconciliation journey\n\nThe Bank and you\n\nEducational resources\n\nBank of Canada Museum\nExpand your understanding of spending and saving, discover your place in the economy and explore a world-class currency collection.\n\nWhat is a central bank?\nA central bank works to promote a country’s economic stability and its citizens’ financial well-being.\n\nCareers\n\nCareers\nTake a central role at the Bank of Canada with our current opportunities and scholarships.\n\nAbout us\n\nCorporate governance\n\nAccessibility\nA plan that shares how the Bank strives to be accessible to everyone\n\nReconciliation Action Plan\nA plan to guide the Bank along its Reconciliation journey\n\nThe Bank and you\n\nEducational resources\n\nBank of Canada Museum\nExpand your understanding of spending and saving, discover your place in the economy and explore a world-class currency collection.\n\nWhat is a central bank?\nA central bank works to promote a country’s economic stability and its citizens’ financial well-being.\n\nCareers\n\nCareers\nTake a central role at the Bank of Canada with our current opportunities and scholarships.\n\nCore functions\n\nFeatured\n\nRenewing Canada’s monetary policy framework\nEvery five years, the Bank of Canada and the Government of Canada review and renew the agreement on Canada’s monetary policy framework.\n\nRetail payments supervision\nWe supervise payment service providers under theRetail Payment Activities Act. We make sure providers meet risk management requirements and keep your funds safe.\n\nMarkets\n\nCommittees\n\nGovernment securities auctions\n\nCanada Mortgage Bonds\nView the latest data on the Government of Canada's purchases and holdings of Canadian Mortgage Bonds.\n\nMarket notices\n\nBank notes\n\nResearch and reports\nAccess our research papers, public consultations and surveys related to bank notes.\n\nOur next $20 bank note\nWe’ve started the design process for the new $20 bank note featuring His Majesty King Charles III. See more about the new note and our design process.\n\nPublications\n\nEconomic survey results\n\nFinancial System Hub\n\nMonetary Policy Report—April 2025\nThe Canadian economy ended 2024 in a strong position. However, the trade conflict and tariffs are expected to slow growth and add to price pressures. The outlook is very uncertain because of the unpredictability of US trade policy and the magnitude of its impact on the Canadian economy.\n\nResearch\n\nPeople\n\nAwards\n\nCollaboration\n\nLatest research\nSee more\n\nPress\n\nInfo\n\nNews\n\nPress Conference: Monetary Policy Report – April 2025\nRelease of the Monetary Policy Report– Press conference by Governor Tiff Macklem and Senior Deputy Governor Carolyn Rogers (10:30 (ET) approx.).\n\nStatistics\n\nTools and resources\n\nStaff economic projections\nThese forecasts are provided to Governing Council in preparation for monetary policy decisions. They are released once a year with a five-year lag.\n\nMachine learning for economics research: when, what and how\n\nIntroduction\nThe size and complexity of economic data are increasing rapidly as the economy becomes more and more digital. This presents both opportunities and challenges for analysts who want to process and interpret these data to gain insights into economic phenomena. Machine learning (ML) has emerged as a powerful tool for analyzing large and complex datasets across disciplines and could mitigate some of the challenges that the digitalization of the economy poses to economics research and analysis. ML models can effectively process large volumes of diverse data, which could allow more complex economic models to be built. As a result, the use of ML in economics research is expanding.\nA review of articles published in 10 leading economics journals1shows that the number of articles about ML has increased significantly in the last few years (Chart 1). This trend is expected to continue as researchers explore new ways to apply ML techniques to a range of economic problems. Nevertheless, the suitability and applicability of these tools is not widely understood among economists and data scientists. To bridge this gap, we review selected papers by authors who use ML tools and that have been published in prominent economics journals. The goal of this review is to help economists interested in leveraging ML tools for their research and analysis as well as data scientists seeking to apply their skills to economic applications.2\n\nChart 1: Published research papers that use machine learning, 2018–22\nChart 1: Published research papers that use machine learning, 2018–22\nNote: The data include articles from the following 10 journals:American Economic Review, Econometrica, Journal of Economic Perspectives, Journal of Monetary Economics, Journal of Political Economy, Journal of Econometrics, Quarterly Journal of Economics, Review of Economic Studies, American Economic Journal: Macroeconomics, American Economic Journal: Microeconomics. The relevant papers are identified using the following search terms: machine learning, ensemble learning, deep learning, statistical learning, reinforcement learning and natural language processing.\nWe aim to showcase the benefits of using ML in economics research and policy analysis and offer suggestions on where, when and how to effectively apply ML models. We take a suggestive approach, rather than an explanatory one. In particular, we focus on supervised ML methods commonly used for prediction problems, such as regressions and classifications. The article is organized into three main sections, each focusing on a key question:\nFinally, we briefly discuss the limitations of ML.\nThe key lessons of the review are summarized as follows:\nDespite the potential benefits of ML, our review identifies a few challenges that must be overcome for ML to be used effectively in economics research. For instance, ML models require large amounts of data and ample computational resources. This can limit some researchers because it can be difficult to obtain high-quality data. Data also may be incomplete or biased. Additionally, ML models are prone to overfitting and can be challenging to interpret, which limits their utility. Moreover, most ML models do not have standard errors, and other statistical properties have not yet been well-defined. This can make it difficult to draw conclusions from the results. Therefore, we recommend caution when using ML models.\nDespite these limitations, we find that researchers successfully use ML alongside traditional econometric tools to advance understanding of economic systems. By combining the strengths of both fields, researchers can improve the accuracy and reliability of economic analyses to better inform policy decisions.\n\nUse of machine learning in economics\nThe literature suggests that ML models could add value to economic research and analysis by:\nFor instance, Varian (2014) suggests that big data, due to its sheer size and complexity, may require the more powerful manipulation tools that ML can offer. Also, ML can help with variable selections when researchers have more potential predictors (or features) than appropriate. Finally, ML techniques are handy in dealing with big data because they can capture more flexible relationships between the data than nonlinear models, potentially offering new insights.\nMullainathan and Spiess (2017) argue that the success of ML is largely due to its ability to discover complex structures in data that are not specified in advance. As well, they suggest that applying ML to economics requires finding relevant tasks where the focus is on improving the accuracy of predictions or uncovering patterns from complex datasets that can be generalized. Also, Athey and Imbens (2019) argue that ML methods have been particularly successful in big data settings that have information on a large number of features, many pieces of information on each feature, or both. They suggest that researchers using ML tools for economics research and analysis should clearly articulate their goals and explain whether certain properties of ML algorithms are important.\n\nProcessing non-traditional data\nNon-traditional datasets, such as images, text, audio and video, can be difficult to process using traditional econometric models. In those cases, ML models can extract valuable information that can be incorporated into traditional economic models.\nFor instance, Hansen, McMahon and Prat (2018) use an ML algorithm for probabilistic topic modelling to assess how transparency—a key design feature of central banks—affects the deliberations of monetary policy-makers. Similarly, Larsen, Thorsrud and Zhulanova (2021) use a large collection of news articles and ML algorithms to investigate how the media help households form inflation expectations, while Angelico et al. (2022) use data from the X platform (formerly known as Twitter) and an ML model to measure inflation expectations. Likewise, Henderson, Storeygard and Weil (2012) use satellite images to measure growth in gross domestic product (GDP) at the sub-national and supranational regions, while Naik, Raskar and Hidalgo (2016) use a computer vision algorithm that measures the correlation between population density and household income with the perceived safety of streetscapes. Gorodnichenko, Pham and Talavera (2023) use deep learning to detect emotions embedded in US Federal Reserve press conferences and examine how those detected emotions influence financial markets. Likewise, Alexopoulos et al. (2022) use machine learning models to extract soft information from congressional testimonies. They analyze textual, audio and video data from Federal Reserve chairs to assess how they affect financial markets.\n\nCapturing strong non-linearity\nML could be useful if the data and application contain strong non-linearity, which is hard to capture with traditional approaches. For instance, Kleinberg et al. (2018) evaluate whether ML models can help improve US judges’ decisions on granting bail. Although the outcome is binary, granting bail demands that judges process complex data to make prudent decisions.\nSimilarly, Maliar, Maliar and Winant (2021) use ML to solve “dynamic economic models by casting them into non-linear regression equations.” Here, ML is used to deal with multicolinearity and to perform the model reduction. Mullainathan and Obermeyer (2022) use ML to test how effective physicians are at diagnosing heart attacks. The authors utilize a large and complex dataset similar to one that physicians had available at the time of diagnosis to uncover potential sources of error in medical decisions.\n\nProcessing traditional data\nML could be useful for processing traditional datasets that are large and complex and have many variables. In such cases, the ML models can help:\nFor instance, Bianchi, Ludvigson and Ma (2022) combine a data-rich environment with an ML model to provide new estimates of time-varying expectational errors embedded in survey responses. They conclude that ML can be used to correct errors in human judgment and improve predictive accuracy.\nSimilarly, Bandiera et al. (2020) use high-frequency, high-dimensional diary data and an ML algorithm to measure the behaviour of chief executive officers. Farbmacher, Löw and Spindler (2022) use ML to detect fraudulent insurance claims using unstructured data comprising inputs of varying lengths and variables with many categories. They argue that ML alleviates the challenges that traditional methods encounter when working with these types of data. Also, Dobbie et al. (2021) suggest that the accuracy of ML-based, data-driven decisions for consumer lending could be free from bias and more accurate than examiner-based decisions.\nML is probably not useful for cases where the data are not very complex. This could be related to shape, size, collinearity or non-linearity. In these cases, traditional econometric models would likely be sufficient. However, if the data become more complex, such as when dealing with big data, the value added by ML models could be higher after a certain threshold, as shown by the vertical dotted line inFigure 1.\n\nFigure 1: Relative merits of machine learning and traditional econometric methods\nFigure 1: Relative merits of machine learning and traditional econometric methods\nNote: The plot is adapted from Dell and Harding (2023) and Harding and Hersh (2018).\n\nCommonly preferred machine learning models\nDifferent ML models are better suited for different types of applications and data characteristics. In this section, we discuss which models are most effective for a given type of economic application.\n\nDeep learning\nNatural language processing, which primarily relies on analyzing textual data, has many applications in economics.4For instance, it could be used for topic modelling or sentiment analysis. The model of choice for topic modelling to quantify text is LDA, proposed by Blei, Ng and Jordan (2003). LDA is an ML algorithm for probabilistic topic modelling. It breaks down documents in terms of the fraction of time spent on a variety of topics (Hansen, McMahon and Prat 2018; Larsen, Thorsrud and Zhulanova 2021).\nThe use of deep-learning models for NLP is evolving rapidly, and various large language models could be used to process textual data. However, transformer models are more efficient at extracting valuable information from textual data (Dell and Harding 2023; Gorodnichenko, Pham and Talavera 2023; Vaswani et al. 2017). Moreover, almost all general-purpose large language models, including GPT-3 and chatGPT, are trained using generative pre-trained transformers (Brown et al. 2020).\nAn interesting application of computer vision models in economics is the use of a broad set of satellite images or remote sensing data for analysis. For instance, Xie et al. (2016) use satellite high-﻿resolution images and machine learning to develop accurate predictors of household income, wealth and poverty rates in five African countries. Similarly, Henderson, Storeygard and Weil (2012) use satellite data to measure GDP growth at the sub-national and supranational level.\nDonaldson and Storeygard (2016) document the opportunities and challenges in using satellite data and ML in economics. The authors conclude that models with such data have the potential to perform economic analysis at lower geographic levels and higher time frequencies than commonly available models. Various deep-learning models can extract useful information from images, including transformers (Chen et al. 2020). But the ConvNext model (Liu, Mao et al. 2022) is becoming more successful at efficiently processing image datasets (Dell and Harding 2023).\n\nEnsemble learning\nEnsemble learning models could be useful if the data are small, include many features and contain collinearity or non-linearity, which are hard to model. For instance, Mullainathan and Spiess (2017) compare the performance of different ML models in predicting house prices. They show how non-﻿parametric ML algorithms, such as random forests, can perform significantly better than ordinary least squares, even with moderate sample sizes and a limited number of covariates.\nSimilarly, Mullainathan and Obermeyer (2022) use ensemble learning models that combine gradient-﻿boosted trees and LASSO (least absolute shrinkage and selection operator), to study physicians’ decision making to uncover potential sources of errors. Also, Athey, Tibshirani and Wager (2019) propose using generalized random forests, a method for nonparametric statistical estimation. This method can be used for three statistical tasks:\nMoreover, the reviewed literature shows that ensemble learning models are popular in macroeconomic prediction (Richardson et al. 2020; Yoon 2021; Chapman and Desai 2021; Goulet Coulombe et al. 2022; Bluwstein et al. 2023).\n\nCausal machine learning\nCausal ML is helpful for inferring causalities in large and complex datasets. For instance, Wager and Athey (2018) extend a random forest by developing a nonparametric causal forest for estimating heterogeneous treatment effects. The authors demonstrate that any type of random forest, including classification and regression forests, can provide valid statistical inferences. In experimenting with these models, they find causal forests to be substantially more powerful than classical methods—especially in the presence of irrelevant covariates.\nSimilarly, Athey and Imbens (2016) use ML to estimate heterogeneity in causal effects in experimental and observational studies. Their approach is tailored for applications where many attributes of a unit relative to the number of units observed could be observed and where the functional form of the relationship between treatment effects and the attributes is unknown. This approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without sparsity assumptions. Davis and Heller (2017) demonstrate the applicability of these methods for predicting treatment heterogeneity using applications for summer jobs.\n\nMachine learning for economic applications\nFor large machine learning models applied to non-traditional data, Dell and Harding (2023) recommend using pre-trained models and transfer learning techniques. Approaches for processing non-traditional data that are based on deep learning are state-of-the-art. However, researchers face many difficulties when using them for economic applications. For instance, large amounts of data and ample computational resources are often necessary to train these models, both of which are scarce resources for economists. Moreover, these models are notoriously convoluted, and many economics researchers who would benefit from using these methods lack the technical background to implement them from scratch (Shen et al. 2021). Therefore, transfer learning, which involves adapting large models pre-trained on specific applications, could be adapted for a similar economic application.\nOff-the-shelf ensemble learning models are useful when dealing with panel data that have a strong collinearity or nonlinearity, but these models should be adapted to suit the task. For instance, Athey, Tibshirani and Wager (2019) adapt the popular random forests algorithm to perform non-parametric regression or estimate average treatment effects. Recall earlier the example of Farbmacher, Löw and Spindler (2020) where the authors adapt a standard neural network to detect fraudulent insurance claims. The authors also demonstrate that the adapted model—an explainable attention network—performs better than an off-the-shelf-model. Likewise, Goulet Coulombe (2020) adapts a canonical ML tool to create a macroeconomic random forest. The author shows that the adapted model, to some extent, is interpretable and provides improved forecasting performance compared with off-﻿the-shelf ML algorithms and traditional econometric approaches.\nThere are many other examples where the model or the procedure to train the model is adapted to improve performance. For instance, Athey and Imbens (2016) adopt the standard cross-validation approach to construct valid confidence intervals for treatment effects, even with many covariates relative to the sample size. Similarly, Chapman and Desai (2021) propose a variation of cross-﻿validation approaches to improve the predictive performance of macroeconomic nowcasting models—especially during economic crises.\nIn their online course, Dell and Harding (2023) offer the following practical recommendations for using ML in economic applications:\n\nOther emerging applications\nOther types of ML approaches, such as unsupervised learning and reinforcement learning, have not made a notable impact in economics research. However, some initial applications of these approaches can be found in the literature. For instance, Gu, Kelly and Xiu (2021) use an unsupervised dimension reduction model, called an auto-encoder neural network, for asset pricing. Similarly, Triepels, Daniels and Heijmans (2017) use an auto-encoder-based unsupervised model to detect anomalies in high-value payments systems. And Decarolis and Rovigatti (2021) use data on nearly 40 million Google keyword auctions and unsupervised ML algorithms to cluster keywords into thematic groups serving as relevant markets.\nReinforcement learning (RL) models could be used to model complex strategic decisions arising in many economic applications. For instance, Castro et al. (2020) use RL to estimate optimal decision rules of banks interacting in high-value payments systems. Similarly, Chen et al. (2021) use deep RL to solve dynamic stochastic general equilibrium models for adaptive learning at the interaction of monetary and fiscal policy, while Hinterlang and Tänzer (2021) use RL to optimize monetary policy decisions. Likewise, Zheng et al. (2022) use an RL approach to learn dynamic tax policies.\n\nLimitations\nThe key limitations of ML for economics research and analysis are outlined below:\nThe literature is evolving to address these challenges, but some of these challenges could take longer to mitigate than others. For instance, limited data exist in many economic applications, which restricts the applicability of large ML models. This could be potentially mitigated in certain applications as the economy becomes more digital, allowing researchers to gather more data at a much higher frequency than traditional economics datasets.\nResearchers are also making progress in overcoming the challenges of interpretability and explainability of models. For instance, one approach recently developed to address this issues is to use Shapley-value-based methodologies, such as those developed in Lundberg and Lee (2017) and Buckmann, Joseph and Robertson (2021). These methods are useful for macroeconomic prediction models, as shown in Buckmann, Joseph and Robertson (2021); Chapman and Desai (2021); Liu, Li et al. (2022) and Bluwstein et al. (2023). However, although such methods are based on game theory, they do not provide any optimal statistical criterion, and asymptotics for many of those approaches are not yet available. To overcome these issues, Babii, Ghysels and Striaukas (2022) and Babii et al. (2022) develop asymptotics in the context of linear regularized regressions and propose an ML-based sampling of mixed data. However, much progress needs to be made to use such asymptotic analysis for popular nonlinear ML approaches.\n\nConclusion\nWe highlight that ML is increasingly used for economics research and policy analysis, particularly for analyzing non-traditional data, capturing non-linearity and improving the accuracy of predictions. Importantly, ML can complement traditional econometric tools by identifying complex relationships and patterns in data that can then be incorporated into econometric models. As the digital economy and economic data continue to grow in complexity, ML remains a valuable tool for economic analysis. However, a few limitations need to be addressed to improve the utility of ML models, and the literature is progressing toward mitigating those challenges.\nLastly,Figure 2presents the word clouds generated from the titles and abstracts of the articles in our dataset. These word clouds illustrate the frequency of certain terms, with larger font sizes indicating more frequent usage. For example, the terms “machine” and “learning” are prominently featured in both titles and abstracts, highlighting their relevance in those articles. They are followed by words such as “data,” “effect” and “decision.”\n\nFigure 2: Word clouds generated from titles and abstracts in the dataset\nFigure 2: Word clouds generated from titles and abstracts in the dataset\na. Titles\nb. Abstracts\nNote: Our dataset of articles about machine learning, collected from prominent economics journals, is visualized through word clouds of their titles and abstracts. These word clouds display term frequency, with larger fonts indicating higher usage. For instance, “machine” and “learning” appear prominently in titles and abstracts, underlining their significance.\n\nAppendix A: Latent Dirichlet allocation\nLatent Dirichlet allocation is a probabilistic ML model commonly used for topic modelling. It assumes that each text document in a corpus contains a mixture of various topics that reside within a latent layer and that each word in a document is associated with one of these topics. The model infers those topics based on the distribution of words across the entire corpus. The output of the model is a set of topic probabilities for each document and a set of word probabilities for each topic. It has many practical applications, including text classification, information retrieval and social network data analysis. Refer to Blei et al. (2003) for more details on the LDA model and its formulation.\n\nAppendix B: Transformers\nTransformers are a deep-learning model architecture commonly used in text and image processing tasks. The key feature of transformers is a self-attention mechanism to process sequential input data, such as words in a sentence. This self-attention allows the model to identify the most relevant parts of the input sequence for each output. Vaswani et al. (2017) provide more details of the model.\nGenerally, the architecture of a transformer comprises an encoder and a decoder consisting of multiple self-attention layers and feed-forward neural networks. The encoder processes the input sequence, such as a sentence in one language, and produces a sequence of context vectors. The decoder uses these context vectors to generate a sequence of outputs, such as translating a sentence into another language. The key benefit of transformer models is their ability to handle long-range dependencies in input sequences, making them particularly effective for NLP tasks that require understanding the context of words or phrases within a longer sentence or paragraph.\n\nAppendix C: ConvNext\nConvNext is a convolutional neural network (CNN) model inspired by the architecture of transformers. It is a deep-learning model commonly used for processing image and video data for various tasks like object detection, image classification and facial recognition. Liu, Mao et al. (2022) shed light on the model and its formulation.\nGenerally, CNN models have multiple layers sandwiched between input and output layers, including convolutional, pooling and fully connected layers. In the convolutional layers, the model performs a set of mathematical operations, called a convolution, on the input image to extract high-level features such as edges, corners and textures. The pooling layers downsample the convolutional layers’ output, such as by reducing the feature maps’ size for subsequent layers. Finally, the fully connected layers classify the image based on the extracted features. One of the key benefits of CNNs is their ability to learn spatial hierarchies of features. This means that the model can identify complex patterns and objects in images by first learning to recognize simpler features and gradually building up to more complicated ones.\n\nAppendix D: Ensemble learning\nEnsemble learning is a popular ML technique that combines multiple individual models, called base models or weak learners, to improve the accuracy and robustness of an overall prediction. Each base model is trained on randomly sampled subsets of the data, and their predictions are then combined to produce a final prediction. Ensemble learning using decision trees is more popular than other models. By combining multiple decision trees trained on different subsets of the data and using different parameters, ensemble learning can capture a broader range of patterns and complex relationships in the data to produce more accurate and robust predictions.\nBagging and boosting are two commonly used ensemble learning approaches. Bagging involves creating multiple decision trees, each trained on a randomly sampled subset of the training data. The final prediction is made by combining the predictions of all the individual decision trees, such as by taking the average or majority vote of the individual tree predictions. A random forest (Breiman 2001) is a popular example of this approach. In contrast, boosting involves training decision trees sequentially, with each new tree attempting to correct the errors of the previous tree by using the modified version of the original training data. The final prediction is made by combining the predictions of all the individual decision trees, with greater weight given to the predictions of the more accurate trees. A popular example is gradient boosting (Natekin and Knoll 2013). The advanced version of boosting methods such as XGBoost (Chen et al. 2015) and LightGBM (Ke et al. 2017) are the most popular. For more, see Sagi and Rokach (2018), which details the types, applications and limitations of ensemble learning.\n\nAppendix E: Transfer learning\nTransfer learning is an ML technique that involves leveraging knowledge gained from training a model on one task to improve the performance of a model on a different but related task. By using a pre-trained model as a starting point for a new task, the model can leverage the knowledge and patterns learned from the previous task to improve its performance. This can lead to faster convergence, better generalization and improved accuracy, especially in situations where the new task has limited data available for training.\nTransfer learning is particularly useful in deep learning. It is effective for large models with millions of parameters that require significant amounts of data for training. Transfer learning has been successfully applied in a wide range of tasks for processing nontraditional data. See Xie et al. (2016) for more details on the types, applications and limitations of transfer learning.\n\nEndnotes\n\nReferences\nAlexopoulos, M., X. Han, O. Kryvtsov and X. Zhang. 2022. “More Than Words: Fed Chairs’ Communication During Congressional Testimonies.” Bank of Canada Staff Working Paper No. 2022-20.\nAngelico, C., J. Marcucci, M. Miccoli and F. Quarta. 2022. “Can We Measure Inflation Expectations Using Twitter?”Journal of Econometrics228 (2): 259–277.\nAthey, S. and G. W. Imbens. 2016. “Recursive Partitioning for Heterogeneous Causal Effects.”Proceedings of the National Academy of Sciences113 (27): 7353–7360.\nAthey, S. and G. W. Imbens. 2019. “Machine Learning Methods That Economists Should Know About.”Annual Review of Economics11: 685–725.\nAthey, S., J. Tibshirani and S. Wager. 2019. “Generalized Random Forests.”Annals of Statistics47 (2): 1148–1178.\nBabii, A., R. T. Ball, E. Ghysels and J. Striaukas. 2022. “Machine Learning Panel Data Regressions with Heavy-Tailed Dependent Data: Theory and Application.”Journal of Econometrics, corrected proof.https://doi.org/10.1016/j.jeconom.2022.07.001\nBabii, A., E. Ghysels and J. Striaukas. 2022. “Machine Learning Time Series Regressions With an Application to Nowcasting.”Journal of Business & Economic Statistics40 (3): 1094–1106.\nBandiera, O., A. Prat, S. Hansen and R. Sadun. 2020. “CEO Behavior and Firm Performance.”Journal of Political Economy128 (4): 1325–1369.\nBianchi, F., S. C. Ludvigson and S. Ma. 2022. “Belief Distortions and Macroeconomic Fluctuations.”American Economic Review112 (7): 2269–2315.\nBlei, D. M., A. Y. Ng and M. I. Jordan. 2003. “Latent Dirichlet Allocation.”Journal of Machine Learning Research3: 993–1022.\nBluwstein, K., M. Buckmann, A. Joseph, S. Kapadia and O. Simsek. 2023. “Credit Growth, the Yield Curve and Financial Crisis Prediction: Evidence from a Machine Learning Approach.”Journal of International Economics, pre-proof: 103773.\nBreiman, L. 2001. “Random Forests.”Machine Learning45: 5–32.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever and D. Amodei. 2020. “Language Models are Few-Shot Learners.”Advances in Neural Information Processing Systems33: 1877–1901.\nBuckmann, M., A. Joseph and H. Robertson. 2021. “Opening the Black Box: Machine Learning Interpretability and Inference Tools With an Application to Economic Forecasting.” InData Science for Economics and Finance, edited by S. Consoli, D. Reforgiato Recupero and M. Saisana, 43–63. Cham, Switzerland: Springer Nature Switzerland AG.\nCastro, P. S., A. Desai, H. Du, R. Garratt and F. Rivadeneyra. 2021. “Estimating Policy Functions in Payment Systems Using Reinforcement Learning.” Bank of Canada Staff Working Paper No. 2021-﻿7.\nChapman, J. T. and A. Desai. 2022. “Macroeconomic Predictions Using Payments Data and Machine Learning.” Bank of Canada Staff Working Paper No. 2022-10.\nChen, M., A. Joseph, M. Kumhof, X. Pan, R. Shi and X. Zhou. 2021. “Deep Reinforcement Learning in a Monetary Model.” arXiv preprint, arXiv:2104.09368v1.\nChen, M., A. Radford, R. Child, J. Wu, H. Jun, D. Luan and I. Sutskever. 2020. “Generative Pretraining from Pixels.” InProceedings of the 37thInternational Conference on Machine Learning119: 1691–1703.\nChen, T., T. He, M. Benesty, V. Khotilovich, Y. Tang, H. Cho, K. Chen, R. Mitchell, I. Cano, T. Zhou, M. Li, J. Xie, M. Lin, Y. Geng and Y. Li. 2015. “Xgboost: Extreme Gradient Boosting.”R package version 0.4-21 (4): 1–4.\nDavis, J. M. V. and S. B. Heller. 2017. “Using Causal Forests to Predict Treatment Heterogeneity: An Application to Summer Jobs.”American Economic Review107 (5): 546–550.\nDecarolis, F. and G. Rovigatti. 2021. “From Mad Men to Maths Men: Concentration and Buyer Power in Online Advertising.”American Economic Review111 (10): 3299–3327.\nDell, M. and M. Harding. 2023. “Machine Learning and Big Data.” Presentation at the American Economic Association 2023 Continuing Education Program, January 8–10, New Orleans, La.https://www.aeaweb.org/conference/cont-ed/2023-webcasts\nDobbie, W., A. Liberman, D. Paravisini and V. Pathania. 2021. “Measuring Bias in Consumer Lending.”Review of Economic Studies88 (6): 2799–2832.\nDonaldson, D. and A. Storeygard. 2016. “The View from Above: Applications of Satellite Data in Economics.”Journal of Economic Perspectives30 (4): 171–198.\nFarbmacher, H., L. Löw and M. Spindler. 2022. “An Explainable Attention Network for Fraud Detection in Claims Management.”Journal of Econometrics228 (2); 244–258.\nGentzkow, M., B. Kelly and M. Taddy. 2019. “Text as Data.”Journal of Economic Literature57 (3): 535–574.\nGorodnichenko, Y., T. Pham and O. Talavera. 2023. “The Voice of Monetary Policy.”American Economic Review113 (2): 548–584.\nGoulet Coulombe, P. 2020. “The Macroeconomy as a Random Forest.” Available at SSRN:https://ssrn.com/abstract=3633110.\nGoulet Coulombe, P., M. Leroux, D. Stevanovic and S. Surprenant. 2022. “How Is Machine Learning Useful for Macroeconomic Forecasting?”Journal of Applied Econometrics37 (5): 920–964.\nGu, S., B. Kelly and D. Xiu. 2021. “Autoencoder Asset Pricing Models.”Journal of Econometrics222 (1): 429–450.\nHansen, S., M. McMahon and A. Prat. 2018. “Transparency and Deliberation Within the FOMC: A Computational Linguistics Approach.”Quarterly Journal of Economics133 (2): 801–870.\nHarding, M. and J. Hersh. 2018. “Big Data in Economics.”IZA World of Labor451. doi:10.15185/izawol.451\nHenderson, J. V., A. Storeygard and D. N. Weil. 2012. “Measuring Economic Growth from Outer Space.”American Economic Review102 (2): 994–1028.\nHinterlang, N. and A. Tänzer. 2021. “Optimal Monetary Policy Using Reinforcement Learning.” Deutsche Bundesbank Discussion Paper No. 51/2021.\nKe, G., Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and T.-Y. Liu. 2017. “Lightgbm: A Highly Efficient Gradient Boosting Decision Tree.”Advances in Neural Information Processing Systems30.\nKleinberg, J., H. Lakkaraju, J. Leskovec, J. Ludwig and S. Mullainathan. 2018. “Human Decisions and Machine Predictions.”Quarterly Journal of Economics133 (1): 237–293.\nLarsen, V. H., L. A. Thorsrud and J. Zhulanova. 2021. “News-driven Inflation Expectations and Information Rigidities.”Journal of Monetary Economics117: 507–520.\nLiu, J., C. Li, P. Ouyang, J. Liu and C. Wu. 2022. “Interpreting the Prediction Results of the Tree-based Gradient Boosting Models for Financial Distress Prediction with an Explainable Machine Learning Approach.”Journal of Forecasting42 (5): 1037–1291.\nLiu, Z., H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell and S. Xie. 2022. “A ConvNet for the 2020s.” InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJune: 11966–11976.\nLundberg, S. M. and S.-I. Lee. 2017. “A Unified Approach to Interpreting Model Predictions.”Advances in Neural Information Processing Systems30.\nMaliar, L., S. Maliar and P. Winant. 2021. “Deep Learning for Solving Dynamic Economic Models.”Journal of Monetary Economics122: 76–101.\nMullainathan, S. and Z. Obermeyer. 2022. “Diagnosing Physician Error: A Machine Learning Approach to Low-value Health Care.”Quarterly Journal of Economics137 (2): 679–727.\nMullainathan, S. and J. Spiess. 2017. “Machine Learning: An Applied Econometric Approach.”Journal of Economic Perspectives31 (2): 87–106.\nNaik, N., R. Raskar and C. A. Hidalgo. 2016. “Cities Are Physical Too: Using Computer Vision To Measure the Quality and Impact of Urban Appearance.”American Economic Review106 (5): 128–132.\nNatekin, A. and A. Knoll. 2013. “Gradient Boosting Machines, a Tutorial.”Frontiers in Neurorobotics7.\nRichardson, A., T. van Florenstein Mulder and T. Vehbi. 2021. “Nowcasting GDP Using Machine-Learning Algorithms: A Real-Time Assessment.”International Journal of Forecasting37 (2): 941–948.\nSagi, O. and L. Rokach. 2018. “Ensemble Learning: A Survey.”Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery8 (4): e1249.\nShen, Z., R. Zhang, M. Dell, B. C. G. Lee, J. Carlson and W. Li. 2021. “Layoutparser: A Unified Toolkit for Deep Learning Based Document Image Analysis.” InDocument Analysis and Recognition — ICDAR 2021, edited by J. Lladós, D. Lopresti and S. Uchida, 131–146. Proceedings of the 16thInternational Conference, Lausanne, Switzerland, September 5–10.\nTriepels, R., H. Daniels and R. Heijmans. 2017. “Anomaly Detection in Real-Time Gross Settlement Systems.” InProceedings of the 19thInternational Conference on Enterprise Information Systems – Volume 1: ICEIS, 433–441. Porto, Portugal, April 26–29.\nVarian, H. R. 2014. “Big Data: New Tricks for Econometrics.”Journal of Economic Perspectives28 (2): 3–28.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. 2017. “Attention Is All You Need.”Advances in Neural Information Processing Systems30: 5999–6009.\nWager, S. and S. Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.”Journal of the American Statistical Association113 (523): 1228–1242.\nXie, M., N. Jean, M. Burke, D. Lobell and S. Ermon. 2016. “Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping.” InAAAI’16: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 3929–3935. Phoenix, Arizona, February 12–17.\nYoon, J. 2021. “Forecasting of Real GDP Growth Using Machine Learning Models: Gradient Boosting and Random Forest Approach.”Computational Economics57 (1): 247–265.\nZheng, S., A. Trott, S. Srinivasa, D. C. Parkes and R. Socher. 2022. “The AI Economist: Taxation Policy Design Via Two-Level Deep Multiagent Reinforcement Learning.”Science Advances8 (18).\n\nAcknowledgements\nThe opinions here are solely those of the authors and do not necessarily reflect those of the Bank of Canada. We thank Jacob Sharples for his assistance on the project. We also thank Andreas Joseph, Dave Campbell, Jonathan Chiu, Narayan Bulusu and Stenio Fernandes for their suggestions and comments on the article.\n\nDisclaimer\nBank of Canada staff analytical notes are short articles that focus on topical issues relevant to the current economic and financial context, produced independently from the Bank’s Governing Council. This work may support or challenge prevailing policy orthodoxy. Therefore, the views expressed in this note are solely those of the authors and may differ from official Bank of Canada views. No responsibility for them should be attributed to the Bank.\nDOI: https://doi.org/10.34989/san-2023-16\n\nOn this pageTable of contents\n\nAbout\n\nAffiliate sites\n\nLegal\n\nConnect with us\nWe usecookiesto help us keep improving this website.",
    "content2": "How Machine learning Models Impacting Economic Predictions? A Review\n\nAbstract and Figures\nDiscover the world's research\n\nCitations (0)\n\nReferences (11)\n\nRecommended publications\n\nMachine Learning Models in Predicting Mortgage Prices\n\nEarly Detection of Heart Valve Dysfunction in Adolescents Using Computational Models\n\nEvaluating the Impact of Data Quality on AI Predictions in Cancer Studies\n\nApplying sensitivity analysis to missing data in classifiers",
    "content3": "Our monetary policy strategy, the tools we use and the impact they have\nInsights into our work on financial stability and payments and market infrastructures\nAccess to all ECB statistics and background information\nAll you need to know about our common currency\nIn-depth studies and expert analyses covering diverse topics and fields\n\nUsing machine learning and big data to analyse the business cycle\n\nUsing machine learning and big data to analyse the business cycle\nSearch in this publication\n\nUsing machine learning and big data to analyse the business cycle\nPrepared by Dominik Hirschbühl, Luca Onorante and Lorena Saiz\nPublished as part of theECB Economic Bulletin, Issue 5/2021.\n\n1 Introduction\nPolicymakers take decisions in real time based on incomplete information about current economic conditions.Central banks and economic analysts largely rely on official statistics together with soft data and surveys, to assess the state of the economy. Although a wide range of high-quality conventional data is available, the datasets are released with lags ranging from a few days or weeks to several months after the reference period. For these reasons, central banks have been looking at ways to exploit timelier data and employ more sophisticated methods to enhance accuracy when forecasting metrics that are relevant for policymaking.\nOver recent years, policy institutions have started to explore new sources of data and alternative statistical methods for the real-time assessment of economic activity.Since the financial crisis, they have stepped up their efforts to systematically use micro and survey data to better gauge changes in aggregate consumption, investment and the labour market. In parallel, technological advances have allowed users to start examining unconventional sources such as text data and images from newspaper articles, social media and the internet together with numerical data from payments. Also now available are alternative statistical methods such as regression trees, neural networks and support-vector machines that may help the potential insights that can be gained from these data sources to be fully exploited.\nThe coronavirus (COVID-19) pandemic has accelerated this trend.The crisis associated with the pandemic has shown that “big data” can provide timely signals on the state of the economy and help to track economic activity alongside more traditional data. Big data are commonly characterised as having three Vs: high volume, high velocity and high variety.[1]Although more Vs have been added to the list over recent years, these are not so widely accepted as they are difficult to quantify (e.g. “veracity”, or truthfulness of the data, and “value”, meaning that big data might create social or economic value).High volume refers to the massive amounts of data generated as a result of the proliferation of devices, services and human interaction. High velocity refers to the fast speed at which the data are created and processed. High variety relates to the wide range and complexity of data types and sources.[2]See Hammer, C., Kostroch, D.C. and Quirós-Romero, G., “Big Data: Potential, Challenges and Statistical Implications”,Staff Discussion Notes, Vol. 2017, Issue 006, International Monetary Fund, 2017.Big data are appealing because they are available at high frequency; however, they are often relatively unstructured and are, by definition, large in size. This in turn poses various challenges for traditional econometric models. Some of these can be addressed by machine learning (ML) algorithms, which also have the advantage of potentially capturing complex non-linear relationships. Even though there is no single definition of machine learning, the basic idea behind it is that computers (machines) can learn from past data, identify general patterns – often characterised by non-linear relationships – and make predictions using algorithms capturing those patterns. Machine learning is therefore a subset of artificial intelligence, and most of its methods are largely based on concepts from statistics and statistical learning theory.[3]Artificial intelligence can be defined as the computer science that focuses on the development of machines that mimic human cognitive functions such as learning or problem solving.\nThis article reviews how policy institutions – international organisations and central banks – use big data and/or machine learning methods to analyse the business cycle.Specifically, these new data sources and tools are used to improve nowcasting and short-term forecasting of real GDP. They are also employed to gain useful insights for assessing cyclical developments and building narratives. A number of illustrative examples are provided.\nThe article is organised as follows.Section 2 reviews the main sources of big data that central banks and other policy institutions have been exploring for business cycle analysis over recent years. It provides an overview of existing literature and also includes two examples of how big data have been used to monitor economic activity and labour market developments during the pandemic. Section 3 discusses the main advantages of ML methods in dealing with big data and analysing the business cycle. This section includes two examples using newspaper articles to build measures of economic sentiment and economic policy uncertainty. Section 4 presents the main conclusions and discusses opportunities and challenges faced by central banks when using machine learning and big data.\n\n2 How do big data help to gauge the current state of the economy?\nPolicy institutions have recently started to incorporate structured and unstructured big data in their economic analysis.Big data can be structured – such as those collected in large financial datasets that can be matched to firm-level financial statements – or unstructured. Unstructured data range from large and near-real-time data gleaned from the internet (e.g. internet search volumes, data from social networks such as Twitter and Facebook, newspaper articles) to large-volume data obtained from non-official sources (e.g. trading platforms and payment systems or GPS-based technologies).\nStructured data, such as those from financial and payment transactions, can provide critical real-time information for assessing aggregate consumption and economic activity.As the use of credit and debit cards to purchase goods and services has increased, the underlying financial transaction data have provided useful information to track consumption and economic activity. At the same time, payments data are available promptly and subject to few revisions since they are financial records. Central banks had already started to regard these data as a valuable source of information before the pandemic emerged. Analysis based on data for the Netherlands, Norway. Portugal and Spain, among others, finds that retail payment systems data (i.e. credit and debit card payments at the point of sale and ATM withdrawals) helped retail sales, private consumption (especially of non-durables) and even real GDP to be forecast in the previous expansionary phase.[4]For the Netherlands, see Verbaan, R., Bolt, W. and van der Cruijsen, C., “Using debit card payments data for nowcasting Dutch household consumption”,DNB Working Papers, No 571, De Nederlandsche Bank, 2017. For Spain, see Conesa, C., Gambacorta, L., Gorjon, S. and Lombardi, M.J., “The use of payment systems data as early indicators of economic activity”,Applied Economics Letters, Vol. 22, Issue 8, 2015, pp. 646-650. For Portugal, see Esteves, P., “Are ATM/POS data relevant when nowcasting private consumption?”,Working Paper, No 25/2009, Banco de Portugal, 2009. For Norway, see Aastveit, K.A., Fastbø, T.M., Granziera, E., Paulsen, K.S. and Torstensen, K.N., “Nowcasting Norwegian household consumption with debit card transaction data”,Working Paper, No 17/2020, Norges Bank, 2020.For Italy, some gains in forecast accuracy have been reported when information from highly aggregated but large value payments (i.e. TARGET2) has been included in GDP nowcasting models.[5]For Italy, see Aprigliano, V., Ardizzi, G. and Monteforte, L., “Using Payment System Data to Forecast Economic Activity”,International Journal of Central Banking, Vol. 15, No 4, October 2019, pp. 55-80.\nTurning to unstructured big data, the use of text data from newspapers to understand and forecast the business cycle has increased significantly in the recent years.In business cycle analysis, text data from newspapers and social media have been used to construct proxy measures for unobservable variables such as “sentiment” or “uncertainty” which are likely to be associated with macroeconomic fluctuations. These proxies can be obtained at relatively low cost (in contrast to expensive survey-based measures) and on a timely basis (e.g. daily) by means of automated natural language processing methods. For instance, news-based sentiment indicators can serve as early warning indicators of financial crises.[6]See Huang, C., Simpson, S., Ulybina, D. and Roitman, A., “News-based Sentiment Indicators”,IMF Working Paper, Vol. 2019, Issue 273, International Monetary Fund, 2019.Newspaper-based sentiment and economic policy uncertainty indexes for Italy and Spain have proved helpful in monitoring economic activity in real time and nowcasting GDP.[7]For Italy, see Aprigliano, V., Emiliozzi, S., Guaitoli, G., Luciani, A., Marcucci, J. and Monteforte, L., “The power of text-based indicators in forecasting the Italian economic activity”,Working Papers, No 1321, Banca d’Italia, 2021. For Spain, see Aguilar, P., Ghirelli, C., Pacce, M. and Urtasun, A., “Can news help measure economic sentiment? An application in COVID-19 times”,Economics Letters, Vol. 199, 2021, and Ghirelli, C., Pérez, J.J. and Urtasun, A., “A new economic policy uncertainty index for Spain”,Economics Letters, Vol. 182, 2019, pp. 64-67.Similarly, in Belgium daily average economic media news sentiment is found to be useful for nowcasting survey-based consumer confidence.[8]See Algaba, A., Borms, S., Boudt, K. and Verbeken, B., “Daily news sentiment and monthly surveys: A mixed-frequency dynamic factor model for nowcasting consumer confidence”,Working Paper Research, No 396, Nationale Bank van België/Banque Nationale de Belgique, 2021.At the ECB, newspaper-based daily sentiment indicators have been estimated for the four largest euro area countries and the euro area as a whole. These indicators demonstrate a high correlation with survey-based sentiment indicators and real GDP; they are also found to be useful for nowcasting GDP, particularly at the beginning of the quarter when other more traditional indicators (e.g. surveys) referring to the current quarter have not been released yet (see Box 3 in Section 3). In addition, economic policy uncertainty indexes have been estimated for the same set of countries. The ML methods employed also allow uncertainty to be decomposed into sub-components that point towards the main sources (see Box 4 in Section 3).\nSimilarly, the use of internet searches has also started to feature in short-term forecasting models.Several Eurosystem studies show that internet searches can provide information about future consumption decisions. Recent examples include analysis linking Google search data to euro area car sales, the use of Google search data to enhance German GDP nowcasting model and the analysis exploiting synthetic indicators based on Google searches for forecasting private consumption in Spain. For the euro area as a whole, Google data provide useful information for GDP nowcasting when macroeconomic information is lacking (i.e. in the first four weeks of the quarter), but as soon as official data relating to the current quarter become available, their relative nowcasting power diminishes.[9]For nowcasting of euro area car sales, see Nymand-Andersen, P. and Pantelidis, E., “Google econometrics: nowcasting euro area car sales and big data quality requirements”,Statistics Paper Series, No 30, ECB, 2018. For nowcasting of Spanish private consumption, see Gil, M., Pérez, J.J., Sanchez Fuentes, A.J. and Urtasun, A., “Nowcasting Private Consumption: Traditional Indicators, Uncertainty Measures, Credit Cards and Some Internet Data”,Working Paper, No 1842, Banco de España, 2018. For nowcasting of German GDP, see Götz, T.B. and Knetsch, T.A., “Google data in bridge equation models for GDP”,International Journal of Forecasting, Vol. 35, Issue 1, January-March 2019, pp. 45-66. For nowcasting of euro area GDP, see Ferrara L. and Simoni, A., “When are Google data useful to nowcast GDP? An approach via pre-selection and shrinkage”,Working Paper, No 717, Banque de France, 2019.\nInternet-based data can also help when assessing the tightness of the labour and housing markets.Analysis for the US labour market shows that including Google-based job-search indicators improves the accuracy of unemployment forecasts, particularly over the medium-term horizon (i.e. three to 12 months ahead).[10]D’Amuri, F. and Marcucci, J., “The predictive power of Google searches in forecasting US unemployment”,International Journal of Forecasting, Vol. 33, Issue 4, October-December 2017, pp. 801-816.In the euro area, a measure of labour market tightness based on the number of clicks on job postings has recently been built for the Irish economy.[11]Furthermore, they showed that online job posting data can provide granular information about skills most demanded by employers and jobs and salaries most searched by workers. See Adrjan, P. and Lydon, R., “Clicks and jobs: measuring labour market tightness using online data,”Economic Letters, Vol. 2019, No 6, Central Bank of Ireland, 2019.For the housing market, analysis for Italy found that metrics based on web-scraped data from an online portal for real estate services can be a leading indicator of housing prices.[12]Loberto, M., Luciani, A. and Pangallo, M., “The potential of big housing data: an application to the Italian real-estate market”,Working Papers, No 1171, Banca d’Italia, 2018.During the pandemic, Google searches on topics related to job retention schemes and layoffs provided early insight into the strong impact of the pandemic and related policy measures. Moreover, online data on job posting and hiring in the euro area have complemented official statistics (see Box 1).\n\nBox 1Monitoring labour market developments during the pandemic\nPrepared by Vasco Botelho and Agostino Consolo\nThis box shows how high-frequency data on hiring was helpful for monitoring labour market developments in the euro area during the pandemic. The COVID-19 crisis had a large downward impact on the number of hires in the euro area labour market. Lockdowns and other containment measures suppressed labour demand and discouraged the search efforts of some workers who lost their jobs and transitioned into inactivity.[13]For a comprehensive assessment of the impact of the pandemic on the euro area labour market, see the article entitled “The impact of the COVID-19 pandemic on the euro area labour market”,Economic Bulletin, Issue 8, ECB, 2020.Moreover, both the heightened macroeconomic uncertainty during the COVID-19 crisis and the widespread use of job retention schemes further reduced the incentives for firms to hire, albeit for different reasons. The heightened uncertainty encouraged firms to lower their operating costs and delay any plans to expand their workforce. By contrast, job retention schemes protected employment and supported jobs, thus incentivising labour hoarding and allowing firms to avoid high re-hiring costs when economic expansion resumes.[14]Labour hoarding can be defined as the part of labour input which is not fully utilised by a company during its production process at any given point in time. Labour hoarding can potentially help firms avoid re-hiring and training costs when economic conditions improve following a recession.\nThe LinkedIn hiring rate complements the information that can be retrieved from the official statistical data, providing a timely, high-frequency indicator on gross hires in the euro area during the pandemic.[15]For an initial assessment of the impact of the pandemic on the euro area labour market using high-frequency data and the LinkedIn hiring rate, see the box entitled “High-frequency data developments in the euro area labour market”,Economic Bulletin, Issue 5, ECB, 2020.Hires in the euro area can only be observed imperfectly in the official statistical data, by analysing transitions between employment and non-employment. Two main caveats arise when using official data to assess hire behaviour in the euro area. First, official data are not very timely, generally only becoming available around two quarters later. Second, these data only allow quantification of net flows into (or out of) employment and do not provide any information on job-to-job transitions.[16]Transitions from employment into employment gather information on both job-to-job transitions and workers that have not moved jobs during the same period (the vast majority). Job-to-job transitions are, however, important for adjustment in the labour market, as they contribute positively to nominal wage growth. See Karahan, F., Michaels, R., Pugsley, B., Şahin, A. and Schuh, R., “Do Job-to-Job Transitions Drive Wage Fluctuations over the Business Cycle?”,American Economic Review,Vol. 107, No 5, pp. 353-357, 2017, who find this result for the United States, and Berson, C., De Philippis, M. and Viviano, E., “Job-to-job flows and wage dynamics in France and Italy”,Occasional Papers,No 563, Bank of Italy, Economic Research and International Relations Area, 2020, who find a similar result for France and Italy, albeit to a lesser extent.The LinkedIn hiring rate provides a more timely, high-frequency signal that can provide information on the number of hires in the euro area. It comprises high-frequency data on gross hires, identifying both movements from non-employment into employment and job-to-job transitions.\nThe standardised LinkedIn hiring rate is first calculated for each of the four largest euro area countries (France, Germany, Italy and Spain – the EA-4) by filtering out seasonal patterns and country-specific artificial trends related to the market performance of LinkedIn. The EA-4 country information is aggregated as a weighted average of the country-specific standardised hiring rates using employment as weights. The EA-4 hiring rate declined significantly at the start of the pandemic before recovering during the second half of 2020 (Chart A, panel (a)). After standing at around 6% above average during the first two months of 2020, it fell suddenly to 63% below average in April 2020 following the onset of the COVID-19 crisis and slowly rebounded to surpass its average level in November 2020. It then returned to below average in January 2021, when more stringent lockdowns were imposed, and recovered again thereafter. Interestingly, the decline in the number of hires paralleled the increase in job retention schemes during the pandemic. In April 2021 the standardised hiring rate stood at 14% above average in the EA-4 aggregate.\nMonitoring the EA-4 labour market using high-frequency data\n(percentages)\nSources: Eurostat, LinkedIn, German Institute for Employment Research (IAB), ifo Institute, French Ministry of Labour, Employment and Economic Inclusion, Italian National Institute for Social Security, Spanish Ministry of Inclusion, Social Security and Migrations, and ECB staff calculations.Notes: The hiring rate is calculated as the percentage of LinkedIn members who started a job in a given month and added a new employer to their profile in that month, divided by the total number of LinkedIn members in that country. To adjust for artificial trends related to the market performance of the platform and for seasonal patterns and spikes due to specific calendar dates, for each country we have filtered out the effects of a series of monthly dummy variables and a linear, yearly trend on the hiring rate. This allow us to express the estimated standardised hiring rate as percentage deviations from the sample average. The forecast of the monthly unemployment rate follows the box entitled “High-frequency data developments in the euro area labour market” in Issue 5/2020 of the ECB’s Economic Bulletin, starting in January 2020, implying that the range of plausible forecasts for the unemployment rate in 2020-21 is conditional on the unemployment rate in December 2019.\nThe high-frequency information provided by the hiring rate can also be used to assess fluctuations in the unemployment rate during the pandemic. Following the box entitled “High-frequency data developments in the euro area labour market” in Issue 5/2020 of the ECB’s Economic Bulletin, we conduct a forecasting exercise linking the high-frequency information of the LinkedIn hiring rate to the job finding rate and using the implied path of the aggregate job finding rate as a proxy for the point-in-time, steady-state unemployment rate. This is then used to forecast the fluctuations in the unemployment rate during the pandemic.[17]In a similar way, several profiles are drawn up for the unemployment rate forecast on the basis of the estimated long-term coefficients for the job finding rate and the assumptions underpinning the separation rates. We consider two scenarios for the separation rate: (i) no change with respect to the fourth quarter of 2019, and (ii) a monthly increase in the separation rate comparable to half of that observed during the average month during the global financial crisis. This is an important caveat to this exercise. Separation rates have also been affected by the significant policy support that has benefited both firms and workers alike, including the widespread use of job retention schemes. As such, the impact that a reduction in policy support may have on the separation rate and the unemployment rate as economic activity resumes warrants further analysis.We thus compare the observed fluctuations in the unemployment rate from March 2020 onwards with those implied by the high-frequency information within the standardised hiring rate for the EA-4 aggregate.\nThe forecast for the unemployment rate using the high-frequency hiring rate provides an early signal of the increase in the unemployment rate for the EA-4 aggregate. Chart A (panel (b)) compares the actual unemployment rate with the ex ante conditional forecast of the unemployment rate using the high-frequency hiring rate and based on the unemployment rate in December 2019. The early signal peak in the unemployment rate forecast in April 2020 at 8.8% is comparable in magnitude with the later August 2020 peak in the actual unemployment rate at 9.1%. More recently, in March 2021 the actual unemployment rate of the EA-4 aggregate was 8.5%, within the plausible range of between 7.8% and 8.7% forecast using the high-frequency hiring rate. The early peak for the forecast unemployment rate was driven by the contraction in the high-frequency hiring rate, which reflected the hiring freezes that followed the widespread use of job retention schemes and allowed separations to remain broadly constant over the initial period of the pandemic. By contrast, most of the recent variation in the unemployment rate (including its stabilisation) has stemmed from an increase in the separation rate.\nThe experience gained with structured and unstructured data prior to the pandemic made it easier to deploy models quickly to facilitate the real-time assessment of the economic situation during the pandemic.In particular, these data have been used to assess the degree of slack in the labour market and to measure the decline in economic activity, seen from both the supply and the demand side. During this period of sudden economic disruption, high-frequency alternative data such as electricity consumption, card payments, job postings, air quality and mobility statistics have been crucial for gaining a timely picture of the economic impact of the pandemic and the associated containment measures, weeks before hard and survey data were released. Payment data have been key to understanding the developments in private consumption, one of the demand components most severely affected by the crisis.[18]Carvalho et al., for instance, use credit card spending data to track the impact of the pandemic on consumption in Spain. They find a strong response in consumption due to lockdowns and their easing at national and regional levels, particularly in the goods basket of low-income households. Carvalho, V.M., Hansen, S., Ortiz, Á., García, J.R., Rodrigo, T., Rodriguez Mora, S. and Ruiz, J., “Tracking the COVID-19 Crisis with High-Resolution Transaction Data”,CEPR Discussion Papers, No 14642, Centre for Economic Policy Research, 2020.Consumption of key inputs such electricity, gas and fuel was used as a proxy for production in some sectors. A timely understanding of developments in the services sector, with a special focus on small businesses in certain service activities such as tourism which have borne the brunt of the crisis, was also very important. High-frequency information available for these sectors related mostly to sales (e.g. sales in tax returns, card payments), online bookings and Google searches. Other indicators such as freight movements, numbers of flights and air quality were informative as rough proxies for economic activity.\nOne effective way of summarising information from a set of high-frequency indicators is to use economic activity trackers.Box 2 provides an example of a weekly economic activity tracker for the euro area devised by the ECB. Similarly, the European Commission’s Joint Research Centre and Directorate-General for Economic and Financial Affairs have been tracking the COVID-19 crisis by combining traditional macroeconomic indicators with a high number of non-conventional, real-time and extremely heterogeneous indicators for the four largest economies in the euro area.[19]In particular, this includes Google searches, views of Wikipedia pages, air quality indicators (where pollution acts as an indicator of activity), aviation micro data, news-based indicators on subjects such as the economy, unemployment and inflation, news-based sentiment, electricity prices and consumption corrected for weather conditions, indicators from Airbnb data, indicators of mobility based on mobile phone data, Google mobility indicators and HGV toll data.They have developed a toolbox with a suite of diverse models, including linear and non-linear models and several ML methods, to exploit the large number of indicators in the dataset for nowcasting GDP. The GDP forecasts are produced by first estimating the whole set (thousands) of models and then applying automatic model selection to average out the forecasts and produce the final forecast.\n\nBox 2A weekly economic activity tracker for the euro area\nPrepared by Gabriel Pérez-Quirós and Lorena Saiz\nSince the onset of the pandemic, several central banks and international institutions have developed experimental daily or weekly economic activity trackers by combining several high-frequency indicators.[20]Several papers were presented at the ECB Workshop “Tracking the economy with high-frequency data”, 16 October 2020.The Federal Reserve Bank of New York, for example, produces the Weekly Economic Index (WEI) that combines seven weekly indicators for the US economy.[21]See Lewis, D.J., Mertens, K., Stock, J.H. and Trivedi, M., “Measuring Real Activity Using a Weekly Economic Index”,Federal Reserve Bank of New York Staff Report, No 920, 2020.Based on a similar methodology, the Deutsche Bundesbank publishes the weekly activity index (WAI) for the German economy, which combines nine weekly indicators but also includes monthly industrial production and quarterly GDP.[22]See Eraslan, S. and Götz, T., “An unconventional weekly activity index for Germany”,Technical Paper, No 02/2020, Deutsche Bundesbank, 2020.Also, the OECD has developed a weekly activity tracker for several countries based on Google Trends data.[23]See Woloszko, N., “Tracking activity in real time with Google Trends”,OECD Economics Department Working Papers, No 1634, OECD, 2020.\nAlthough these indicators are appealing, their development presents three key technical issues. First, the short time span available for high-frequency data makes them less reliable for establishing econometric relations which prove stable over time, compared to long time series of monthly economic indicators.[24]Readers might be interested in the following recommended surveys of the literature: Banbura, M., Giannone, D. and Reichlin, L., “Nowcasting”, in Clements, M.P. and Hendry D.F. (eds),Oxford Handbook of Economic Forecasting, Oxford University Press, 2011, pp. 63-90; Camacho, M., Pérez-Quirós, G. and Poncela, P., “Short-term Forecasting for Empirical Economists: A Survey of the Recently Proposed Algorithms”,Foundations and Trends in Econometrics, Vol. 6, No 2, 2013, pp. 101-161.Second, high-frequency indicators are extremely noisy, exhibit complex seasonal patterns and, in some cases, may be subject to frequent data revisions. In the special circumstances associated with the COVID-19 crisis, these indicators were very informative (i.e. the signal-to-noise ratio was high), but in normal times it is still open to question whether these will only add noise to the already reliable signal obtained from the standard monthly indicators.[25]Delle Chiaie, S. and Pérez-Quirós, G., “High frequency indicators. why? when? and how? A users’ guide”, mimeo, 2021.Third, the procedure to select indicators has not been standardised. Up to now, most work has used high-frequency indicators that are readily available for each economy. The lack of harmonised selection procedures reduces the scope to “learn from the cross-section” and accentuates the representativeness problem mentioned above.\nThe weekly economic activity tracker for the euro area proposed in this box addresses these issues by combining reliable monthly indicators that have a long history of good predictive performance with timely high-frequency (non-standard) indicators. The indicators have been selected according to several criteria: (i) availability of a long enough history (at least three years), (ii) not too noisy, and (iii) the weight of the indicator in the aggregate that combines all of them (a principal component in the case of the indicator discussed here) is statistically significant and economically meaningful.[26]The weekly frequency indicators are electricity consumption, German HGV toll mileage index, Google searches (restaurants, jobs, travel, hotels) and financial indicators (CISS, EURO STOXX, VSTOXX). The monthly frequency indicators are airport cargo and employment for the four largest euro area countries, euro area industrial production, industrial orders, car registrations, retail sales (volume), intra and extra euro area exports of goods (value), PMI composite output and economic sentiment indicator.\nThe design of the tracker is based on principal component analysis (PCA) with unbalanced data, as described by Stock and Watson.[27]Stock, J.H. and Watson, M.W.,“Macroeconomic Forecasting Using Diffusion Indexes”,Journal of Business and Economic Statistics, Vol. 20, Issue 2, 2002, pp. 147-162.First, a tracker using only weekly series is computed by PCA to fill the missing observations at the beginning and, if necessary, the end of the sample. The weekly series are transformed into month-on-month growth rates.[28]Since some months have five weeks and others four, the convention used is that the monthly growth rate for the fifth week of the month is always compared with the last week of the previous month.If necessary, seasonal adjustment methods are used to eliminate any seasonal effects. Second, the monthly variables are transformed into weekly frequency by imputing the same monthly level for all weeks of the month. Then, the month-on-month growth rates are computed for every week. With all this information, the PCA is run again including all the indicators which were originally available at weekly and monthly frequency. The first principal component is the tracker, which represents the evolution of monthly activity on a weekly frequency (Chart A, panel (a)).[29]By design, the tracker does not have units since PCA requires data standardisation. Therefore, the tracker needs to be re-scaled to make it compatible with the mean and variance of real GDP growth. The scaling factor can be determined using the relation between monthly activity and quarterly activity explained in Mariano and Murasawa. See Mariano, R.S. and Murasawa, Y., “A new coincident index of business cycles based on monthly and quarterly series”,Journal of Applied Econometrics, Vol. 18, Issue 4, 2003, pp. 427-443.Visualising the tracker in levels and monthly frequency gives an idea of the magnitude of the output loss associated with the pandemic compared with pre-pandemic levels. Most importantly, the evolution of the tracker in levels over 2020 mirrors the evolution of GDP very well (Chart A, panel (b)). Overall, the relatively good performance of the tracker, which strikes a good balance between timely and reliable indicators, makes it a useful tool for tracking economic activity in real time.\nEuro area economic activity tracker\n(panel (a): month-on-month percentages; panel (b): levels, 100=December 2019 or Q4 2019)\nSources: ECB staff calculations and Eurostat.Note: The latest observations are for the week of 29 May 2021 for the trackers and Q1 2021 for GDP.\n\n3 What makes machine learning algorithms useful tools for analysing big data?\nWhile big data can help improve the forecasts of GDP and other macroeconomic aggregates, their full potential can be exploited by employing ML algorithms.Section 2 shows that in many cases, the improvement in forecasting performance relates to specific situations, such as when traditional monthly indicators for the reference quarter are not yet available. This section focuses on the modelling framework, arguing that ML methods help to reap the benefits of using big data. The main goal of ML techniques is to find patterns in data or to predict a target variable. Although ML algorithms estimate and validate predictive models in a subset of data (training sample), the ultimate aim is to obtain the best forecasting performance using a different subset of data (test sample). The distinction between machine learning and traditional methods is not clear-cut since some traditional methods (e.g. linear regression, principal components) are also quite popular in the ML literature. However, the literature on machine learning has developed a host of new and sophisticated models that promise to strongly enrich the toolbox of applied economists. Moreover, it also seems fair to say that, so far, machine learning has been mostly focused on prediction, while more traditional econometric and statistical analysis is also interested in uncovering the causal relationships between economic variables.[30]A good overview of ML concepts and applications in the context of central banking and policy analysis can be found in Chakraborty, C. and Joseph, A., “Machine learning at central banks”,Staff Working Paper, No 674, Bank of England, 2017.This is changing fast, as more and more researchers in the ML field address the issue of inference and causality, although this frontier research is not yet widely applied in the policy context.[31]See, for example, Farrell, M.H., Liang, T. and Misra, S., “Deep Neural Networks for Estimation and Inference”,Econometrica, Vol. 89, No 1, 2021, or Semenova, V., Goldman, M., Chernozhukov, V. and Taddy, M., “Estimation and Inference on Heterogeneous Treatment Effects in High-Dimensional Dynamic Panels”,arXiv.org, 2021.The aim of this section is to discuss how machine learning can usefully complement traditional econometric methods, in particular to leverage the opportunities for analysing the business cycle offered by big data. It also reviews several contributions to forecasting/nowcasting GDP (see Box 3) and provides examples of how ML algorithms can provide interesting insights for policy, such as pointing towards the sources of economic policy uncertainty (see Box 4).\nThe size of the newly available databases in itself often constitutes an obstacle to the use of traditional econometrics.Techniques have been adopted to reduce the dimensionality of the data, including traditional methods such as factor models and principal component analysis, but more often going into newer versions of machine learning. While a description of specific methods is beyond the scope of this article, it is important to note that ML methods have several desirable features for summarising the data, allowing precise reduction of high-dimensional data into a number of manageable indicators.\nThe first key advantage of ML methods is their ability to extract and select the relevant information from large volumes of, unstructured data.When dealing with big data, the presence of a large amount of mostly irrelevant information engenders the problem of data selection. This issue is magnified by the presence of large, unstructured datasets.[32]See Giannone, D., Lenza, M. and Primiceri, G., “Economic Predictions with Big Data: The Illusion of Sparsity”,Econometrica, forthcoming.In some simple cases, the forecaster can pick variables manually; this is normally possible when forecasting very specific quantities. The seminal work of Choi and Varian with Google Trends, for instance, focuses on car sales, unemployment claims, travel destination planning and consumer confidence.[33]Choi, H. and Varian, H., “Predicting the Present with Google Trends”,Economic Record, Vol. 88, Issue s1, The Economic Society of Australia, June 2012, pp. 2-9.Where macroeconomic aggregates are involved, the choosing of relevant variables quickly becomes intractable. ML methods offer very useful tools for selecting the most informative variables and exploiting their information potential. Several techniques derived from the model-averaging literature have also proved popular and successful in improving forecasting accuracy. In these methods, a large number of econometric models are first estimated, their forecasting performance is then evaluated, and the final forecast is obtained by averaging the forecasts of the best models, thus retaining those models and explanatory variables that provide useful information. Similarly, what are known as ensemble methods such as random forests and bagging combine different “views” of the data given by competing models, adding flexibility and robustness to the predictions.\nThe second key advantage of ML methods is their ability to capture quite general forms of non-linearities.This is a general advantage of ML methods, regardless of the volume of data concerned; however, the issue is that, by their very nature, big data may be particularly prone to non-linearities. For instance, the data stemming from social networks present a good way to understand these inherent non-linearities. In this case, a specific topic can generate cascade or snowball effects within the network which cannot be channelled in linear regression models. Other examples include Google Trends and Google search categories, which are compiled using ML algorithms that determine the category to which an internet search belongs.[34]These data are only available as “semi-processed” time series; in particular, they are first detrended according to some criteria known by Google, then resized so that they always have values between 0 and 100.Text data are also obtained by applying highly non-linear ML algorithms to news items, for example. More generally, non-linearities and interactions between variables are common in macroeconomics owing to the presence of financial frictions and uncertainty. Several works have found that ML methods can be useful for macroeconomic forecasting, since they better capture non-linearities (e.g. Coulombe et al.). These methods can, for instance, capture the non-linear relationship between financial conditions and economic activity, among others, and hence more accurately predict activity and recessions in particular (see Box 3). Also, ML methods can outperform standard methods (e.g. credit scoring models, logistic regression) when predicting consumer and corporate defaults, since they capture non-linear relationships between the incidence of default and the characteristics of the individuals.[35]See Coulombe, P.G., Leroux, M., Stefanovic, D. and Surprenant, S., “How is Machine Learning Useful for Macroeconomic Forecasting?”,arXiv.org, 2020. For recession probabilities, see Vrontos, S.D., Galakis, J. and Vrontos, I.D., “Modelling and predicting U.S. recessions using machine learning techniques”, International Journal of Forecasting, Vol. 37, Issue 2, 2021, pp. 647-671. For random forest to capture non-linearity between financial conditions and economic activity, see Kiley, M.T., “Financial Conditions and Economic Activity: Insights from Machine Learning”, Finance and Economics Discussion Series, 2020-095, Board of Governors of the Federal Reserve System, 2020. For predictions of consumer defaults, see Albanesi, S. and Vamossy, D.F.,“Predicting Consumer Default: A Deep Learning Approach”, NBER Working Paper Series, No w26165, National Bureau of Economic Research, 2019. For corporate defaults, see Pike, T., Sapriza, H. and Zimmermann, T., “Bottom-up leading macroeconomic indicators: An application to non-financial corporate defaults using machine learning”, Finance and Economics Discussion Series, 2019-070, Board of Governors of the Federal Reserve System, 2019.\nThe COVID-19 pandemic isan important source of non-linearities.During the pandemic, many macroeconomic variables have recorded extreme values that are far from the range of past values. Econometric methods such as linear time series analysis seek to find average patterns in past data. If current data are very different, linearly extrapolating from past patterns may lead to biased results. Central banks, the European Commission and other institutions have adapted their nowcasting frameworks to capture non-standard data and non-linearities.[36]See, for instance, Huber, F., Koop, G., Onorante, L., Pfarrhofer, M. and Schreiner, J., “Nowcasting in a pandemic using non-parametric mixed frequency VARs”,Journal of Econometrics, in press, 2020.\nFinally, ML techniques are the main tool used to capture a wide set of phenomena that would otherwise remain unquantified.The most prominent example in recent years is the dramatic surge of text data analysis. Today, broad corpuses of text are analysed and converted into numbers that forecasters can use. For instance, a wide range of timely, yet noisy confidence indicators based on text currently complement the traditional surveys, which are available with considerable lags and where agents do not necessarily “vote with their behaviour”, as well as market-based indicators, where expectations and other factors such as risk aversion compound in the data. A first generation of work built on word counts has been followed by more sophisticated approaches.[37]Le, Q. and Mikolov, T., “Distributed Representations of Sentences and Documents”,Proceedings of the 31st International Conference on Machine Learning, Vol. 32, 2014, pp. 1188-1196.Second-generation techniques based on unsupervised learning are also used in public institutions, and in particular in central banks, to assess the effect of their communication. Finally, following Baker et al., concepts such as economic policy uncertainty which were previously difficult to quantify are now currently assessed on the basis of their economic consequences and used in forecasting.[38]See Baker, S.R., Bloom, N. and Davis, S.J., “Measuring Economic Policy Uncertainty”,Quarterly Journal of Economics, Vol. 131, Issue 4, 2016, pp. 1593-1636.See Box 3 and Box 4 for examples.\n\nBox 3Nowcasting euro area real GDP growth with newspaper-based sentiment\nPrepared by Julian Ashwin, Eleni Kalamara and Lorena Saiz\nThis box presents economic sentiment indicators for the euro area derived from newspaper articles in the four largest euro area countries in their main national languages.[39]This box summarises the main findings of the paper by Ashwin, Kalamara and Saiz.Ashwin, J., Kalamara, E. and Saiz, L., “Nowcasting Euro Area GDP with News Sentiment: A Tale of Two Crises”, manuscript, 2021.[40]The articles come from 15 major print newspapers in France, Germany, Italy and Spain. They have been extracted from Dow Jones Factiva DNA database for the period from 1 January 1998 to 31 December 2020.Available at daily frequency, these indicators contain timely economic signals which are comparable to those from well-known sentiment indicators such as the Purchasing Managers’ Index (PMI). Furthermore, they can materially improve nowcasts of real GDP growth in the euro area.\nIn the literature, two approaches are typically followed for building sentiment metrics from textual data. The most popular is to use simple word counts based on predetermined sets of words, known as dictionaries or lexicons. However, most of the dictionaries have been developed for the English language. For the euro area, the multilingual environment makes it necessary to either develop new dictionaries for other languages or translate texts into English. Alternatively, more computationally demanding model-based methods such as semantic clustering or topic modelling can extract topics which can be approximated to sentiment and its drivers. In this box, the sentiment metrics are based on counts of words in the news articles translated into English, relying on several well-known English language dictionaries.[41]The news articles are translated into English using Google Translate API. Robustness checks have been performed comparing this method with using dictionaries in the national languages or even translating the dictionaries into English. Overall, translating the articles into English provides the most robust and reliable results.For the sake of space, only the sentiment metrics based on the financial stability-based dictionary and the general-purpose dictionary VADER are reported.[42]The financial stability dictionary is taken from Correa, R., Garud, K., Londono-Yarce, J.-M. and Mislang, N., “Constructing a Dictionary for Financial Stability”,IFDP Notes, Board of Governors of the Federal Reserve System, June 2017. The VADER (Valence Aware Dictionary and sEntiment Reasoner) dictionary is taken from Hutto, C.J. and Gilbert, E., “VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text”,Eighth International AAAI Conference on Weblogs and Social Media (8th ICWSM 2014), Ann Arbor, MI, June 2014.\nRegardless of the dictionary used, and despite some noisiness, the newspaper-based sentiment metrics are highly correlated with the PMI composite index in the period from 2000 to 2019 (Chart A, panel (a)). This confirms that these measures are actually capturing sentiment. However, the choice of dictionary matters when it comes to detecting turning points. The first sentiment metric captures the Great Recession very well, unsurprisingly given the financial nature of this crisis. But this metric fails to encapsulate the COVID-19 crisis (Chart A, panel (b)), although its evolution is consistent with the behaviour of the financial markets and the financing conditions which have remained favourable in the context of very strong policy response. By contrast, the general-purpose dictionary is more consistent and robust across time. Therefore, it appears that the nature of economic shocks may play a significant role in identifying the most appropriate text dictionary to be used.\nPMI and newspaper-based sentiment indexes for the euro area\n(standardised units)\nSources: ECB staff calculations, Factiva, IHS Markit and Eurostat.Notes: The news-based sentiment indicator is based on newspaper articles from the four largest euro area countries. The metric used is the sum of positive and negative words using either a financial stability dictionary (Correa et al.) or VADER, a more general-purpose dictionary. The PMI composite index and the news-based sentiment indicators are standardised using historical mean and variance.\nVarious studies have found that text analysis can significantly improve forecasts of key macroeconomic variables.[43]See, for example, Thorsrud, L.A., “Words are the New Numbers: A Newsy Coincident Index of the Business Cycle”,Journal of Business & Economic Statistics,Vol. 38, Issue 2, 2020, pp. 393-409; Larsen, V.H. and Thorsrud, L.A., “The value of news for economic developments”,Journal of Econometrics, Vol. 210, Issue 1, 2019, pp. 203-218; Kalamara, E., Turrell, A., Redl, C., Kapetanios, G. and Kapadia, S., “Making text count: economic forecasting using newspaper text”,Staff Working Paper, No 865, Bank of England, May 2020; Shapiro, A.H., Sudhof, M. and Wilson, D.J., “Measuring news sentiment”,Journal of Econometrics, in press, 2020.Some forecast accuracy gains (not shown) are found for real-time GDP nowcasts derived using the PMI composite index and the text-based sentiment indicators as key predictors. They are typically concentrated in the nowcasts produced in the first half of the quarter (i.e. first six weeks), when most other indicators used to nowcast GDP are not yet available. This result is in line with other works in the literature. However, an important point is that the type of model matters to fully reap the true benefits of the timeliness of text-based information. Standard linear methods (e.g. ordinary least squares linear regression) work well in calm times when there are no big shifts in the economic outlook. When extreme economic shocks occur, however, ML models can capture non-linearities and filter out the noise (Chart B). Ridge regressions captured the financial crisis better, as shown by the fact that they have the lowest Root Mean Squared Forecast Error (RMSFE), particularly when including the sentiment metric based on the financial stability dictionary. However, the best-performing models during the pandemic have been the neural networks, which were the worst-performing models during the financial crisis. This could be explained by the fact that before the financial crisis, there were no other similar crises in the training sample from which the model could learn. Indeed, one of the criticisms of the more complex ML models is that they need large amounts of data to learn (i.e. they are “data hungry”).\nForecast accuracy\nRMSFE\n(percentage points)\nSource: ECB staff calculations.Notes: The chart reports the RMSFE over a rolling window of eight quarters. The forecasts are updated at the end of the first month of the reference quarter. The reference variable is the vintage of real GDP growth as of 24 March 2021.\n\nBox 4Sources of economic policy uncertainty in the euro area and their impact on demand components\nPrepared by Andrés Azqueta-Gavaldón, Dominik Hirschbühl, Luca Onorante and Lorena Saiz\nThis box describes how big data and machine learning (ML) analysis can be applied to the measurement of uncertainty using textual data. Similarly to “economic sentiment”, uncertainty is not directly observable and can only be measured using proxies. Recent developments in the literature have shown that textual data can provide good proxies for this latent variable. For instance, the seminal work by Baker, Bloom and Davies proposed building an economic policy uncertainty (EPU) index using a pre-specified set of keywords in newspaper articles.[44]See Baker, S.R., Bloom, N. and Davis, S.J., “Measuring Economic Policy Uncertainty”,The Quarterly Journal of Economics, Vol. 131, Issue 4, 2016, pp. 1593-1636.Recent research by the ECB has built an EPU index across the four largest euro area countries by applying ML algorithms to newspaper articles.[45]More specifically, a continuous bag-of-words model is used to identify the words most closely related to “economy” and “uncertainty” in the context of each language. Then, a Latent Dirichlet Allocation (LDA) algorithm is applied to classify news articles into topics. See Azqueta-Gavaldón, A., Hirschbühl, D., Onorante, L. and Saiz, L., “Economic policy uncertainty in the euro area: an unsupervised machine learning approach”,Working Paper Series, No 2359, ECB, January 2020.The main advantage of this approach is that it can be easily applied to different languages without relying on keywords, given that the underlying algorithm classifies text into topics without prior information. This feature makes it less prone to selection bias. Moreover, this approach retrieves topics underpinning aggregate economic policy uncertainty (e.g. fiscal, monetary or trade policy uncertainty) in newspaper articles. This can be particularly useful for building narratives and economic analysis.[46]See the box entitled “Sources of economic policy uncertainty in the euro area: a machine learning approach”,Economic Bulletin, Issue 5, ECB, 2019.ML methods applied to a sample of newspaper articles from France, Germany, Italy and Spain over the sample period from 2000 to 2019 consistently revealed the following topics or sources of economic policy uncertainty: monetary policy; fiscal policy; political, geopolitical and trade policy; European regulation; domestic regulation; and energy policy.\nEconomic policy uncertainty stems from different sources which affect consumers’ and firms’ decisions differently. For instance, increases in uncertainty regarding future tariffs can have an impact on a firm’s determination to build a new production plant or to start exporting to a new market. This is because the role of future conditions is particularly relevant for costly, irreversible decisions. By contrast, uncertainty about the future monetary policy stance can be important for both firms’ and consumers’ spending decisions, since it will influence their expectations about future economic developments and financing conditions.\nA simple structural vector autoregression (SVAR) analysis confirms that increases in (ML-based) EPU have a significant negative impact on private consumption and business investment proxied by investment in machinery and equipment in the euro area. The impact on investment is greater than on consumption, suggesting that uncertainty may have more of an impact on the supply side.[47]See Born, B. and Pfeifer, J., “Policy risk and the business cycle”,Journal of Monetary Economics, Vol. 68, 2014, pp. 68-85, and Fernández-Villaverde, J., Guerrón-Quintana, P., Kuester, K. and Rubio-Ramírez, J., “Fiscal Volatility Shocks and Economic Activity”,American Economic Review, Vol. 105, No 11, 2015, pp. 3352-3384.As regards sources of economic policy uncertainty, the focus is only on energy, trade and monetary policy uncertainty for the sake of space. As expected, monetary policy uncertainty shocks have a clear negative impact on both investment and consumption. By contrast, the impact of increases in trade policy uncertainty is insignificant in both cases. Moreover, increases in energy policy uncertainty depress consumption to a greater extent than other sources, while their effect on investment, albeit weaker, is more persistent over time. While these are aggregate results, EPU is likely to play a more relevant role for firm-level capital investment than at aggregate level.[48]For instance, Gulen and Ion find evidence that the relation between policy uncertainty and capital investment is not uniform in the cross-section, being significantly stronger for firms with a higher degree of investment irreversibility and for firms that are more dependent on government spending. Husted, Rogers and Sun document evidence that monetary policy uncertainty significantly delays firm-level investment in the United States. See Gulen, H. and Ion, M., “Policy Uncertainty and Corporate Investment”,Review of Financial Studies, Vol. 29, Issue 3, 2016, pp. 523-564, and Husted, L., Rogers, J. and Sun, B., “Monetary policy uncertainty”,Journal of Monetary Economics, Vol. 115, 2020, pp. 20-36.\nImpulse responses of consumption (panel (a)) and investment (panel (b)) to economic policy uncertainty shocks\n(y-axis: percentage points; x-axis: quarters)\nSources: Azqueta-Gavaldón et al. and Eurostat.Notes: The impulse responses illustrate the response of consumption and investment to a positive one standard deviation shock in each of the measures of economic policy uncertainty. They are estimated with Bayesian structural vector autoregressions (SVAR), and the shocks are identified using a Cholesky decomposition with the variables in the following order: exports of goods and services, measure of economic policy uncertainty, private consumption, machinery and equipment investment, shadow short rate and EURO STOXX. All the variables are in quarterly growth rates, except for the shadow short rate, which is in levels. The estimation period is from 2000 to 2019. The measures of uncertainty are standardised so that the size of the shock is comparable. The confidence band corresponds to the 68% credibility band of the SVAR with the economic policy uncertainty index.\n\n4 Conclusions, challenges and opportunities\nThis article has described how big data and ML methods can complement standard analysis of the business cycle.A case in point is the coronavirus pandemic, which represents an extraordinary shock. This crisis has propelled the dissemination and refinement of ML techniques and big data at an unprecedented speed. In particular, it has shown that alternative sources of data can provide more timely signals on the state of the economy and help to track economic activity. Furthermore, it is an important showcase for non-linearities in the economy, which has required existing models to be adapted or new approaches to be developed. In this respect, ML methods can deal with non-linearities more easily than traditional methods. Besides new opportunities, these new data sources and methods also pose some challenges.\nBig data allow a wider range of timely indicators to be used for forecasting (e.g. text-based or internet-based indicators), although in some cases this can entail replicability and accountability issues.Text-based sentiment indicators are particularly useful, for instance, given that they can be produced automatically at higher frequency and at lower cost than survey-based indicators. While the construction of conventional economic data, such as industrial production, follows harmonised procedures to ensure high quality, continuity and comparability over time and countries, alternative data are neither collected primarily for economic analysis, nor sourced and validated by independent statistical offices. Therefore, their application in decision-making processes exposes central banks to various risks, given that the replicability of results and accountability could be impaired. Since alternative data are collected for other purposes (e.g. credit card transactions) or come as the by-product of another service (e.g. news articles from the digitisation of newspapers), the data are often very noisy and require careful treatment. Moreover, the existence of significant data accessibility issues and limitations to data sharing could impair the replicability of the results in some cases. All these risks require careful consideration when investing scarce resources in software development and legal issues, as well as customising IT infrastructure.[49]Doerr et al. note that a key challenge for central banks is to set up the necessary IT infrastructure. For most applications concerning business cycle analysis, the required computational power is rather low. Doerr, S., Gambacorta, L. and Serena, J.M., “Big data and machine learning in central banking”,BIS Working Papers, No 930, Bank for International Settlements, 2021.\nAlthough useful as complements, at the moment these tools cannot be considered as substitutes for standard data and methods due to issues of interpretability and statistical inference.ML methods can help overcome the shortcomings of big data and exploit their full potential. When combined with big data, ML methods are capable of outperforming traditional statistical methods and providing an accurate picture of economic developments. Despite the good forecasting performance, the complexity of the methods often makes it difficult to interpret revisions to the forecasts and most importantly to communicate them. However, rapid advances are being made on enhancing the interpretability of ML techniques (most recently based on Shapley values).[50]Joseph proposed a new framework based on Shapley regressions that generalises statistical inference for non-linear or non-parametric models such as artificial neural networks, support vector machines and random forests. See Joseph, A., “Parametric inference with universal function approximators”,Staff Working Paper, No 784, Bank of England, 2019, revised 22 July 2020.In addition, ML techniques are not originally designed to identify causal relationships, which is of critical importance to policymakers. Enhancing the ability of ML methods to capture causality is currently the biggest challenge; this has the potential to make ML techniques promising complements and viable alternatives to established methods.[51]See Farrell, M.H., Liang, T. and Misra, S., “Deep Neural Networks for Estimation and Inference”,Econometrica, Vol. 89, Issue 1, January 2021, pp. 181-213.\nDisclaimerPlease note that related topic tags are currently available for selected content only.\nCopyright 2025,European Central Bank\n\nOur website uses cookies\nWe use functional cookies to store user preferences; analytics cookies to improve website performance; third-party cookies set by third-party services integrated into the website. You have the choice to accept or reject them. For more information or to review your preference on the cookies and server logs we use, we invite you to:\nRead our privacy statementLearn more about how we use cookies\n\nThank you!\n\nThank you!\n\nWe have updated our privacy policy\nWe are always working to improve this website for our users. To do this, we use the anonymous data provided\n\t\t\tby cookies.See what has changed in our privacy policy\n\nYour cookie preference has expired\nWe are always working to improve this website for our users. To do this, we use the anonymous data provided\n\t\t\tby cookies.Learn more about how we use cookies"
}