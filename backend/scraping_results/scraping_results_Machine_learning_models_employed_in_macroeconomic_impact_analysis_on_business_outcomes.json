{
    "content1": "\nMachine learning for economics research: when, what and how\n\nIntroduction\n\nThe size and complexity of economic data are increasing rapidly as the economy becomes more and more digital. This presents both opportunities and challenges for analysts who want to process and interpret these data to gain insights into economic phenomena. Machine learning (ML) has emerged as a powerful tool for analyzing large and complex datasets across disciplines and could mitigate some of the challenges that the digitalization of the economy poses to economics research and analysis. ML models can effectively process large volumes of diverse data, which could allow more complex economic models to be built. As a result, the use of ML in economics research is expanding.\n\nA review of articles published in 10 leading economics journals1 shows that the number of articles about ML has increased significantly in the last few years (Chart 1). This trend is expected to continue as researchers explore new ways to apply ML techniques to a range of economic problems. Nevertheless, the suitability and applicability of these tools is not widely understood among economists and data scientists. To bridge this gap, we review selected papers by authors who use ML tools and that have been published in prominent economics journals. The goal of this review is to help economists interested in leveraging ML tools for their research and analysis as well as data scientists seeking to apply their skills to economic applications.2\n\nChart 1: Published research papers that use machine learning, 2018–22\n\nNote: The data include articles from the following 10 journals: American Economic Review, Econometrica, Journal of Economic Perspectives, Journal of Monetary Economics, Journal of Political Economy, Journal of Econometrics, Quarterly Journal of Economics, Review of Economic Studies, American Economic Journal: Macroeconomics, American Economic Journal: Microeconomics. The relevant papers are identified using the following search terms: machine learning, ensemble learning, deep learning, statistical learning, reinforcement learning and natural language processing.\n\nWe aim to showcase the benefits of using ML in economics research and policy analysis and offer suggestions on where, when and how to effectively apply ML models. We take a suggestive approach, rather than an explanatory one. In particular, we focus on supervised ML methods commonly used for prediction problems, such as regressions and classifications. The article is organized into three main sections, each focusing on a key question:\n\nFinally, we briefly discuss the limitations of ML.\n\nThe key lessons of the review are summarized as follows:\n\nDespite the potential benefits of ML, our review identifies a few challenges that must be overcome for ML to be used effectively in economics research. For instance, ML models require large amounts of data and ample computational resources. This can limit some researchers because it can be difficult to obtain high-quality data. Data also may be incomplete or biased. Additionally, ML models are prone to overfitting and can be challenging to interpret, which limits their utility. Moreover, most ML models do not have standard errors, and other statistical properties have not yet been well-defined. This can make it difficult to draw conclusions from the results. Therefore, we recommend caution when using ML models.\n\nDespite these limitations, we find that researchers successfully use ML alongside traditional econometric tools to advance understanding of economic systems. By combining the strengths of both fields, researchers can improve the accuracy and reliability of economic analyses to better inform policy decisions.\n\nUse of machine learning in economics\n\nThe literature suggests that ML models could add value to economic research and analysis by:\n\nFor instance, Varian (2014) suggests that big data, due to its sheer size and complexity, may require the more powerful manipulation tools that ML can offer. Also, ML can help with variable selections when researchers have more potential predictors (or features) than appropriate. Finally, ML techniques are handy in dealing with big data because they can capture more flexible relationships between the data than nonlinear models, potentially offering new insights.\n\nMullainathan and Spiess (2017) argue that the success of ML is largely due to its ability to discover complex structures in data that are not specified in advance. As well, they suggest that applying ML to economics requires finding relevant tasks where the focus is on improving the accuracy of predictions or uncovering patterns from complex datasets that can be generalized. Also, Athey and Imbens (2019) argue that ML methods have been particularly successful in big data settings that have information on a large number of features, many pieces of information on each feature, or both. They suggest that researchers using ML tools for economics research and analysis should clearly articulate their goals and explain whether certain properties of ML algorithms are important.\n\nProcessing non-traditional data\n\nNon-traditional datasets, such as images, text, audio and video, can be difficult to process using traditional econometric models. In those cases, ML models can extract valuable information that can be incorporated into traditional economic models.\n\nFor instance, Hansen, McMahon and Prat (2018) use an ML algorithm for probabilistic topic modelling to assess how transparency—a key design feature of central banks—affects the deliberations of monetary policy-makers. Similarly, Larsen, Thorsrud and Zhulanova (2021) use a large collection of news articles and ML algorithms to investigate how the media help households form inflation expectations, while Angelico et al. (2022) use data from the X platform (formerly known as Twitter) and an ML model to measure inflation expectations. Likewise, Henderson, Storeygard and Weil (2012) use satellite images to measure growth in gross domestic product (GDP) at the sub-national and supranational regions, while Naik, Raskar and Hidalgo (2016) use a computer vision algorithm that measures the correlation between population density and household income with the perceived safety of streetscapes. Gorodnichenko, Pham and Talavera (2023) use deep learning to detect emotions embedded in US Federal Reserve press conferences and examine how those detected emotions influence financial markets. Likewise, Alexopoulos et al. (2022) use machine learning models to extract soft information from congressional testimonies. They analyze textual, audio and video data from Federal Reserve chairs to assess how they affect financial markets.\n\nCapturing strong non-linearity\n\nML could be useful if the data and application contain strong non-linearity, which is hard to capture with traditional approaches. For instance, Kleinberg et al. (2018) evaluate whether ML models can help improve US judges’ decisions on granting bail. Although the outcome is binary, granting bail demands that judges process complex data to make prudent decisions.\n\nSimilarly, Maliar, Maliar and Winant (2021) use ML to solve “dynamic economic models by casting them into non-linear regression equations.” Here, ML is used to deal with multicolinearity and to perform the model reduction. Mullainathan and Obermeyer (2022) use ML to test how effective physicians are at diagnosing heart attacks. The authors utilize a large and complex dataset similar to one that physicians had available at the time of diagnosis to uncover potential sources of error in medical decisions.\n\nProcessing traditional data\n\nML could be useful for processing traditional datasets that are large and complex and have many variables. In such cases, the ML models can help:\n\nFor instance, Bianchi, Ludvigson and Ma (2022) combine a data-rich environment with an ML model to provide new estimates of time-varying expectational errors embedded in survey responses. They conclude that ML can be used to correct errors in human judgment and improve predictive accuracy.\n\nSimilarly, Bandiera et al. (2020) use high-frequency, high-dimensional diary data and an ML algorithm to measure the behaviour of chief executive officers. Farbmacher, Löw and Spindler (2022) use ML to detect fraudulent insurance claims using unstructured data comprising inputs of varying lengths and variables with many categories. They argue that ML alleviates the challenges that traditional methods encounter when working with these types of data. Also, Dobbie et al. (2021) suggest that the accuracy of ML-based, data-driven decisions for consumer lending could be free from bias and more accurate than examiner-based decisions.\n\nML is probably not useful for cases where the data are not very complex. This could be related to shape, size, collinearity or non-linearity. In these cases, traditional econometric models would likely be sufficient. However, if the data become more complex, such as when dealing with big data, the value added by ML models could be higher after a certain threshold, as shown by the vertical dotted line in Figure 1.\n\nFigure 1: Relative merits of machine learning and traditional econometric methods\n\nNote: The plot is adapted from Dell and Harding (2023) and Harding and Hersh (2018).\n\nCommonly preferred machine learning models\n\nDifferent ML models are better suited for different types of applications and data characteristics. In this section, we discuss which models are most effective for a given type of economic application.\n\nDeep learning\n\nNatural language processing, which primarily relies on analyzing textual data, has many applications in economics.4 For instance, it could be used for topic modelling or sentiment analysis. The model of choice for topic modelling to quantify text is LDA, proposed by Blei, Ng and Jordan (2003). LDA is an ML algorithm for probabilistic topic modelling. It breaks down documents in terms of the fraction of time spent on a variety of topics (Hansen, McMahon and Prat 2018; Larsen, Thorsrud and Zhulanova 2021).\n\nThe use of deep-learning models for NLP is evolving rapidly, and various large language models could be used to process textual data. However, transformer models are more efficient at extracting valuable information from textual data (Dell and Harding 2023; Gorodnichenko, Pham and Talavera 2023; Vaswani et al. 2017). Moreover, almost all general-purpose large language models, including GPT-3 and chatGPT, are trained using generative pre-trained transformers (Brown et al. 2020).\n\nAn interesting application of computer vision models in economics is the use of a broad set of satellite images or remote sensing data for analysis. For instance, Xie et al. (2016) use satellite high-﻿resolution images and machine learning to develop accurate predictors of household income, wealth and poverty rates in five African countries. Similarly, Henderson, Storeygard and Weil (2012) use satellite data to measure GDP growth at the sub-national and supranational level.\n\nDonaldson and Storeygard (2016) document the opportunities and challenges in using satellite data and ML in economics. The authors conclude that models with such data have the potential to perform economic analysis at lower geographic levels and higher time frequencies than commonly available models. Various deep-learning models can extract useful information from images, including transformers (Chen et al. 2020). But the ConvNext model (Liu, Mao et al. 2022) is becoming more successful at efficiently processing image datasets (Dell and Harding 2023).\n\nEnsemble learning\n\nEnsemble learning models could be useful if the data are small, include many features and contain collinearity or non-linearity, which are hard to model. For instance, Mullainathan and Spiess (2017) compare the performance of different ML models in predicting house prices. They show how non-﻿parametric ML algorithms, such as random forests, can perform significantly better than ordinary least squares, even with moderate sample sizes and a limited number of covariates.\n\nSimilarly, Mullainathan and Obermeyer (2022) use ensemble learning models that combine gradient-﻿boosted trees and LASSO (least absolute shrinkage and selection operator), to study physicians’ decision making to uncover potential sources of errors. Also, Athey, Tibshirani and Wager (2019) propose using generalized random forests, a method for nonparametric statistical estimation. This method can be used for three statistical tasks:\n\nMoreover, the reviewed literature shows that ensemble learning models are popular in macroeconomic prediction (Richardson et al. 2020; Yoon 2021; Chapman and Desai 2021; Goulet Coulombe et al. 2022; Bluwstein et al. 2023).\n\nCausal machine learning\n\nCausal ML is helpful for inferring causalities in large and complex datasets. For instance, Wager and Athey (2018) extend a random forest by developing a nonparametric causal forest for estimating heterogeneous treatment effects. The authors demonstrate that any type of random forest, including classification and regression forests, can provide valid statistical inferences. In experimenting with these models, they find causal forests to be substantially more powerful than classical methods—especially in the presence of irrelevant covariates.\n\nSimilarly, Athey and Imbens (2016) use ML to estimate heterogeneity in causal effects in experimental and observational studies. Their approach is tailored for applications where many attributes of a unit relative to the number of units observed could be observed and where the functional form of the relationship between treatment effects and the attributes is unknown. This approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without sparsity assumptions. Davis and Heller (2017) demonstrate the applicability of these methods for predicting treatment heterogeneity using applications for summer jobs.\n\nMachine learning for economic applications\n\nFor large machine learning models applied to non-traditional data, Dell and Harding (2023) recommend using pre-trained models and transfer learning techniques. Approaches for processing non-traditional data that are based on deep learning are state-of-the-art. However, researchers face many difficulties when using them for economic applications. For instance, large amounts of data and ample computational resources are often necessary to train these models, both of which are scarce resources for economists. Moreover, these models are notoriously convoluted, and many economics researchers who would benefit from using these methods lack the technical background to implement them from scratch (Shen et al. 2021). Therefore, transfer learning, which involves adapting large models pre-trained on specific applications, could be adapted for a similar economic application.\n\nOff-the-shelf ensemble learning models are useful when dealing with panel data that have a strong collinearity or nonlinearity, but these models should be adapted to suit the task. For instance, Athey, Tibshirani and Wager (2019) adapt the popular random forests algorithm to perform non-parametric regression or estimate average treatment effects. Recall earlier the example of Farbmacher, Löw and Spindler (2020) where the authors adapt a standard neural network to detect fraudulent insurance claims. The authors also demonstrate that the adapted model—an explainable attention network—performs better than an off-the-shelf-model. Likewise, Goulet Coulombe (2020) adapts a canonical ML tool to create a macroeconomic random forest. The author shows that the adapted model, to some extent, is interpretable and provides improved forecasting performance compared with off-﻿the-shelf ML algorithms and traditional econometric approaches.\n\nThere are many other examples where the model or the procedure to train the model is adapted to improve performance. For instance, Athey and Imbens (2016) adopt the standard cross-validation approach to construct valid confidence intervals for treatment effects, even with many covariates relative to the sample size. Similarly, Chapman and Desai (2021) propose a variation of cross-﻿validation approaches to improve the predictive performance of macroeconomic nowcasting models—especially during economic crises.\n\nIn their online course, Dell and Harding (2023) offer the following practical recommendations for using ML in economic applications:\n\nOther emerging applications\n\nOther types of ML approaches, such as unsupervised learning and reinforcement learning, have not made a notable impact in economics research. However, some initial applications of these approaches can be found in the literature. For instance, Gu, Kelly and Xiu (2021) use an unsupervised dimension reduction model, called an auto-encoder neural network, for asset pricing. Similarly, Triepels, Daniels and Heijmans (2017) use an auto-encoder-based unsupervised model to detect anomalies in high-value payments systems. And Decarolis and Rovigatti (2021) use data on nearly 40 million Google keyword auctions and unsupervised ML algorithms to cluster keywords into thematic groups serving as relevant markets.\n\nReinforcement learning (RL) models could be used to model complex strategic decisions arising in many economic applications. For instance, Castro et al. (2020) use RL to estimate optimal decision rules of banks interacting in high-value payments systems. Similarly, Chen et al. (2021) use deep RL to solve dynamic stochastic general equilibrium models for adaptive learning at the interaction of monetary and fiscal policy, while Hinterlang and Tänzer (2021) use RL to optimize monetary policy decisions. Likewise, Zheng et al. (2022) use an RL approach to learn dynamic tax policies.\n\nLimitations\n\nThe key limitations of ML for economics research and analysis are outlined below:\n\nThe literature is evolving to address these challenges, but some of these challenges could take longer to mitigate than others. For instance, limited data exist in many economic applications, which restricts the applicability of large ML models. This could be potentially mitigated in certain applications as the economy becomes more digital, allowing researchers to gather more data at a much higher frequency than traditional economics datasets.\n\nResearchers are also making progress in overcoming the challenges of interpretability and explainability of models. For instance, one approach recently developed to address this issues is to use Shapley-value-based methodologies, such as those developed in Lundberg and Lee (2017) and Buckmann, Joseph and Robertson (2021). These methods are useful for macroeconomic prediction models, as shown in Buckmann, Joseph and Robertson (2021); Chapman and Desai (2021); Liu, Li et al. (2022) and Bluwstein et al. (2023). However, although such methods are based on game theory, they do not provide any optimal statistical criterion, and asymptotics for many of those approaches are not yet available. To overcome these issues, Babii, Ghysels and Striaukas (2022) and Babii et al. (2022) develop asymptotics in the context of linear regularized regressions and propose an ML-based sampling of mixed data. However, much progress needs to be made to use such asymptotic analysis for popular nonlinear ML approaches.\n\nConclusion\n\nWe highlight that ML is increasingly used for economics research and policy analysis, particularly for analyzing non-traditional data, capturing non-linearity and improving the accuracy of predictions. Importantly, ML can complement traditional econometric tools by identifying complex relationships and patterns in data that can then be incorporated into econometric models. As the digital economy and economic data continue to grow in complexity, ML remains a valuable tool for economic analysis. However, a few limitations need to be addressed to improve the utility of ML models, and the literature is progressing toward mitigating those challenges.\n\nLastly, Figure 2 presents the word clouds generated from the titles and abstracts of the articles in our dataset. These word clouds illustrate the frequency of certain terms, with larger font sizes indicating more frequent usage. For example, the terms “machine” and “learning” are prominently featured in both titles and abstracts, highlighting their relevance in those articles. They are followed by words such as “data,” “effect” and “decision.”\n\nFigure 2: Word clouds generated from titles and abstracts in the dataset\n\na. Titles\n\nb. Abstracts\n\nNote: Our dataset of articles about machine learning, collected from prominent economics journals, is visualized through word clouds of their titles and abstracts. These word clouds display term frequency, with larger fonts indicating higher usage. For instance, “machine” and “learning” appear prominently in titles and abstracts, underlining their significance.\n\nAppendix A: Latent Dirichlet allocation\n\nLatent Dirichlet allocation is a probabilistic ML model commonly used for topic modelling. It assumes that each text document in a corpus contains a mixture of various topics that reside within a latent layer and that each word in a document is associated with one of these topics. The model infers those topics based on the distribution of words across the entire corpus. The output of the model is a set of topic probabilities for each document and a set of word probabilities for each topic. It has many practical applications, including text classification, information retrieval and social network data analysis. Refer to Blei et al. (2003) for more details on the LDA model and its formulation.\n\nAppendix B: Transformers\n\nTransformers are a deep-learning model architecture commonly used in text and image processing tasks. The key feature of transformers is a self-attention mechanism to process sequential input data, such as words in a sentence. This self-attention allows the model to identify the most relevant parts of the input sequence for each output. Vaswani et al. (2017) provide more details of the model.\n\nGenerally, the architecture of a transformer comprises an encoder and a decoder consisting of multiple self-attention layers and feed-forward neural networks. The encoder processes the input sequence, such as a sentence in one language, and produces a sequence of context vectors. The decoder uses these context vectors to generate a sequence of outputs, such as translating a sentence into another language. The key benefit of transformer models is their ability to handle long-range dependencies in input sequences, making them particularly effective for NLP tasks that require understanding the context of words or phrases within a longer sentence or paragraph.\n\nAppendix C: ConvNext\n\nConvNext is a convolutional neural network (CNN) model inspired by the architecture of transformers. It is a deep-learning model commonly used for processing image and video data for various tasks like object detection, image classification and facial recognition. Liu, Mao et al. (2022) shed light on the model and its formulation.\n\nGenerally, CNN models have multiple layers sandwiched between input and output layers, including convolutional, pooling and fully connected layers. In the convolutional layers, the model performs a set of mathematical operations, called a convolution, on the input image to extract high-level features such as edges, corners and textures. The pooling layers downsample the convolutional layers’ output, such as by reducing the feature maps’ size for subsequent layers. Finally, the fully connected layers classify the image based on the extracted features. One of the key benefits of CNNs is their ability to learn spatial hierarchies of features. This means that the model can identify complex patterns and objects in images by first learning to recognize simpler features and gradually building up to more complicated ones.\n\nAppendix D: Ensemble learning\n\nEnsemble learning is a popular ML technique that combines multiple individual models, called base models or weak learners, to improve the accuracy and robustness of an overall prediction. Each base model is trained on randomly sampled subsets of the data, and their predictions are then combined to produce a final prediction. Ensemble learning using decision trees is more popular than other models. By combining multiple decision trees trained on different subsets of the data and using different parameters, ensemble learning can capture a broader range of patterns and complex relationships in the data to produce more accurate and robust predictions.\n\nBagging and boosting are two commonly used ensemble learning approaches. Bagging involves creating multiple decision trees, each trained on a randomly sampled subset of the training data. The final prediction is made by combining the predictions of all the individual decision trees, such as by taking the average or majority vote of the individual tree predictions. A random forest (Breiman 2001) is a popular example of this approach. In contrast, boosting involves training decision trees sequentially, with each new tree attempting to correct the errors of the previous tree by using the modified version of the original training data. The final prediction is made by combining the predictions of all the individual decision trees, with greater weight given to the predictions of the more accurate trees. A popular example is gradient boosting (Natekin and Knoll 2013). The advanced version of boosting methods such as XGBoost (Chen et al. 2015) and LightGBM (Ke et al. 2017) are the most popular. For more, see Sagi and Rokach (2018), which details the types, applications and limitations of ensemble learning.\n\nAppendix E: Transfer learning\n\nTransfer learning is an ML technique that involves leveraging knowledge gained from training a model on one task to improve the performance of a model on a different but related task. By using a pre-trained model as a starting point for a new task, the model can leverage the knowledge and patterns learned from the previous task to improve its performance. This can lead to faster convergence, better generalization and improved accuracy, especially in situations where the new task has limited data available for training.\n\nTransfer learning is particularly useful in deep learning. It is effective for large models with millions of parameters that require significant amounts of data for training. Transfer learning has been successfully applied in a wide range of tasks for processing nontraditional data. See Xie et al. (2016) for more details on the types, applications and limitations of transfer learning.\n\nEndnotes\n\nReferences\n\nAlexopoulos, M., X. Han, O. Kryvtsov and X. Zhang. 2022. “More Than Words: Fed Chairs’ Communication During Congressional Testimonies.” Bank of Canada Staff Working Paper No. 2022-20.\n\nAngelico, C., J. Marcucci, M. Miccoli and F. Quarta. 2022. “Can We Measure Inflation Expectations Using Twitter?” Journal of Econometrics 228 (2): 259–277.\n\nAthey, S. and G. W. Imbens. 2016. “Recursive Partitioning for Heterogeneous Causal Effects.” Proceedings of the National Academy of Sciences 113 (27): 7353–7360.\n\nAthey, S. and G. W. Imbens. 2019. “Machine Learning Methods That Economists Should Know About.” Annual Review of Economics 11: 685–725.\n\nAthey, S., J. Tibshirani and S. Wager. 2019. “Generalized Random Forests.” Annals of Statistics 47 (2): 1148–1178.\n\nBabii, A., R. T. Ball, E. Ghysels and J. Striaukas. 2022. “Machine Learning Panel Data Regressions with Heavy-Tailed Dependent Data: Theory and Application.” Journal of Econometrics, corrected proof. https://doi.org/10.1016/j.jeconom.2022.07.001\n\nBabii, A., E. Ghysels and J. Striaukas. 2022. “Machine Learning Time Series Regressions With an Application to Nowcasting.” Journal of Business & Economic Statistics 40 (3): 1094–1106.\n\nBandiera, O., A. Prat, S. Hansen and R. Sadun. 2020. “CEO Behavior and Firm Performance.” Journal of Political Economy 128 (4): 1325–1369.\n\nBianchi, F., S. C. Ludvigson and S. Ma. 2022. “Belief Distortions and Macroeconomic Fluctuations.” American Economic Review 112 (7): 2269–2315.\n\nBlei, D. M., A. Y. Ng and M. I. Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3: 993–1022.\n\nBluwstein, K., M. Buckmann, A. Joseph, S. Kapadia and O. Simsek. 2023. “Credit Growth, the Yield Curve and Financial Crisis Prediction: Evidence from a Machine Learning Approach.” Journal of International Economics, pre-proof: 103773.\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever and D. Amodei. 2020. “Language Models are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\nBuckmann, M., A. Joseph and H. Robertson. 2021. “Opening the Black Box: Machine Learning Interpretability and Inference Tools With an Application to Economic Forecasting.” In Data Science for Economics and Finance, edited by S. Consoli, D. Reforgiato Recupero and M. Saisana, 43–63. Cham, Switzerland: Springer Nature Switzerland AG.\n\nCastro, P. S., A. Desai, H. Du, R. Garratt and F. Rivadeneyra. 2021. “Estimating Policy Functions in Payment Systems Using Reinforcement Learning.” Bank of Canada Staff Working Paper No. 2021-﻿7.\n\nChapman, J. T. and A. Desai. 2022. “Macroeconomic Predictions Using Payments Data and Machine Learning.” Bank of Canada Staff Working Paper No. 2022-10.\n\nChen, M., A. Joseph, M. Kumhof, X. Pan, R. Shi and X. Zhou. 2021. “Deep Reinforcement Learning in a Monetary Model.” arXiv preprint, arXiv:2104.09368v1.\n\nChen, M., A. Radford, R. Child, J. Wu, H. Jun, D. Luan and I. Sutskever. 2020. “Generative Pretraining from Pixels.” In Proceedings of the 37th International Conference on Machine Learning 119: 1691–1703.\n\nChen, T., T. He, M. Benesty, V. Khotilovich, Y. Tang, H. Cho, K. Chen, R. Mitchell, I. Cano, T. Zhou, M. Li, J. Xie, M. Lin, Y. Geng and Y. Li. 2015. “Xgboost: Extreme Gradient Boosting.” R package version 0.4-2 1 (4): 1–4.\n\nDavis, J. M. V. and S. B. Heller. 2017. “Using Causal Forests to Predict Treatment Heterogeneity: An Application to Summer Jobs.” American Economic Review 107 (5): 546–550.\n\nDecarolis, F. and G. Rovigatti. 2021. “From Mad Men to Maths Men: Concentration and Buyer Power in Online Advertising.” American Economic Review 111 (10): 3299–3327.\n\nDell, M. and M. Harding. 2023. “Machine Learning and Big Data.” Presentation at the American Economic Association 2023 Continuing Education Program, January 8–10, New Orleans, La. https://www.aeaweb.org/conference/cont-ed/2023-webcasts\n\nDobbie, W., A. Liberman, D. Paravisini and V. Pathania. 2021. “Measuring Bias in Consumer Lending.” Review of Economic Studies 88 (6): 2799–2832.\n\nDonaldson, D. and A. Storeygard. 2016. “The View from Above: Applications of Satellite Data in Economics.” Journal of Economic Perspectives 30 (4): 171–198.\n\nFarbmacher, H., L. Löw and M. Spindler. 2022. “An Explainable Attention Network for Fraud Detection in Claims Management.” Journal of Econometrics 228 (2); 244–258.\n\nGentzkow, M., B. Kelly and M. Taddy. 2019. “Text as Data.” Journal of Economic Literature 57 (3): 535–574.\n\nGorodnichenko, Y., T. Pham and O. Talavera. 2023. “The Voice of Monetary Policy.” American Economic Review 113 (2): 548–584.\n\nGoulet Coulombe, P. 2020. “The Macroeconomy as a Random Forest.” Available at SSRN: https://ssrn.com/abstract=3633110.\n\nGoulet Coulombe, P., M. Leroux, D. Stevanovic and S. Surprenant. 2022. “How Is Machine Learning Useful for Macroeconomic Forecasting?” Journal of Applied Econometrics 37 (5): 920–964.\n\nGu, S., B. Kelly and D. Xiu. 2021. “Autoencoder Asset Pricing Models.” Journal of Econometrics 222 (1): 429–450.\n\nHansen, S., M. McMahon and A. Prat. 2018. “Transparency and Deliberation Within the FOMC: A Computational Linguistics Approach.” Quarterly Journal of Economics 133 (2): 801–870.\n\nHarding, M. and J. Hersh. 2018. “Big Data in Economics.” IZA World of Labor 451. doi:10.15185/izawol.451\n\nHenderson, J. V., A. Storeygard and D. N. Weil. 2012. “Measuring Economic Growth from Outer Space.” American Economic Review 102 (2): 994–1028.\n\nHinterlang, N. and A. Tänzer. 2021. “Optimal Monetary Policy Using Reinforcement Learning.” Deutsche Bundesbank Discussion Paper No. 51/2021.\n\nKe, G., Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and T.-Y. Liu. 2017. “Lightgbm: A Highly Efficient Gradient Boosting Decision Tree.” Advances in Neural Information Processing Systems 30.\n\nKleinberg, J., H. Lakkaraju, J. Leskovec, J. Ludwig and S. Mullainathan. 2018. “Human Decisions and Machine Predictions.” Quarterly Journal of Economics 133 (1): 237–293.\n\nLarsen, V. H., L. A. Thorsrud and J. Zhulanova. 2021. “News-driven Inflation Expectations and Information Rigidities.” Journal of Monetary Economics 117: 507–520.\n\nLiu, J., C. Li, P. Ouyang, J. Liu and C. Wu. 2022. “Interpreting the Prediction Results of the Tree-based Gradient Boosting Models for Financial Distress Prediction with an Explainable Machine Learning Approach.” Journal of Forecasting 42 (5): 1037–1291.\n\nLiu, Z., H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell and S. Xie. 2022. “A ConvNet for the 2020s.” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition June: 11966–11976.\n\nLundberg, S. M. and S.-I. Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” Advances in Neural Information Processing Systems 30.\n\nMaliar, L., S. Maliar and P. Winant. 2021. “Deep Learning for Solving Dynamic Economic Models.” Journal of Monetary Economics 122: 76–101.\n\nMullainathan, S. and Z. Obermeyer. 2022. “Diagnosing Physician Error: A Machine Learning Approach to Low-value Health Care.” Quarterly Journal of Economics 137 (2): 679–727.\n\nMullainathan, S. and J. Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106.\n\nNaik, N., R. Raskar and C. A. Hidalgo. 2016. “Cities Are Physical Too: Using Computer Vision To Measure the Quality and Impact of Urban Appearance.” American Economic Review 106 (5): 128–132.\n\nNatekin, A. and A. Knoll. 2013. “Gradient Boosting Machines, a Tutorial.” Frontiers in Neurorobotics 7.\n\nRichardson, A., T. van Florenstein Mulder and T. Vehbi. 2021. “Nowcasting GDP Using Machine-Learning Algorithms: A Real-Time Assessment.” International Journal of Forecasting 37 (2): 941–948.\n\nSagi, O. and L. Rokach. 2018. “Ensemble Learning: A Survey.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8 (4): e1249.\n\nShen, Z., R. Zhang, M. Dell, B. C. G. Lee, J. Carlson and W. Li. 2021. “Layoutparser: A Unified Toolkit for Deep Learning Based Document Image Analysis.” In Document Analysis and Recognition — ICDAR 2021, edited by J. Lladós, D. Lopresti and S. Uchida, 131–146. Proceedings of the 16th International Conference, Lausanne, Switzerland, September 5–10.\n\nTriepels, R., H. Daniels and R. Heijmans. 2017. “Anomaly Detection in Real-Time Gross Settlement Systems.” In Proceedings of the 19th International Conference on Enterprise Information Systems – Volume 1: ICEIS, 433–441. Porto, Portugal, April 26–29.\n\nVarian, H. R. 2014. “Big Data: New Tricks for Econometrics.” Journal of Economic Perspectives 28 (2): 3–28.\n\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30: 5999–6009.\n\nWager, S. and S. Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–1242.\n\nXie, M., N. Jean, M. Burke, D. Lobell and S. Ermon. 2016. “Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping.” In AAAI’16: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 3929–3935. Phoenix, Arizona, February 12–17.\n\nYoon, J. 2021. “Forecasting of Real GDP Growth Using Machine Learning Models: Gradient Boosting and Random Forest Approach.” Computational Economics 57 (1): 247–265.\n\nZheng, S., A. Trott, S. Srinivasa, D. C. Parkes and R. Socher. 2022. “The AI Economist: Taxation Policy Design Via Two-Level Deep Multiagent Reinforcement Learning.” Science Advances 8 (18).\n\nAcknowledgements\n\nThe opinions here are solely those of the authors and do not necessarily reflect those of the Bank of Canada. We thank Jacob Sharples for his assistance on the project. We also thank Andreas Joseph, Dave Campbell, Jonathan Chiu, Narayan Bulusu and Stenio Fernandes for their suggestions and comments on the article.\n\nDisclaimer\n\nBank of Canada staff analytical notes are short articles that focus on topical issues relevant to the current economic and financial context, produced independently from the Bank’s Governing Council. This work may support or challenge prevailing policy orthodoxy. Therefore, the views expressed in this note are solely those of the authors and may differ from official Bank of Canada views. No responsibility for them should be attributed to the Bank.\n\nDOI: https://doi.org/10.34989/san-2023-16\n\nOn this page\n\nAbout\n\nAffiliate sites\n\nLegal\n\nConnect with us\n\nWe use cookies to help us keep improving this website.\n",
    "content2": "\nHow Machine learning Models Impacting Economic Predictions? A Review\n\nAbstract and Figures\n\nDiscover the world's research\n\nCitations (0)\n\nReferences (11)\n\nRecommended publications\n\nMachine Learning Models in Predicting Mortgage Prices\n\nEarly Detection of Heart Valve Dysfunction in Adolescents Using Computational Models\n\nEvaluating the Impact of Data Quality on AI Predictions in Cancer Studies\n\nApplying sensitivity analysis to missing data in classifiers\n",
    "content3": "\nUsing machine learning and big data to analyse the business cycle\n\nUsing machine learning and big data to analyse the business cycle\n\nPrepared by Dominik Hirschbühl, Luca Onorante and Lorena Saiz\n\nPublished as part of the ECB Economic Bulletin, Issue 5/2021.\n\n1 Introduction\n\nPolicymakers take decisions in real time based on incomplete information about current economic conditions. Central banks and economic analysts largely rely on official statistics together with soft data and surveys, to assess the state of the economy. Although a wide range of high-quality conventional data is available, the datasets are released with lags ranging from a few days or weeks to several months after the reference period. For these reasons, central banks have been looking at ways to exploit timelier data and employ more sophisticated methods to enhance accuracy when forecasting metrics that are relevant for policymaking.\n\nOver recent years, policy institutions have started to explore new sources of data and alternative statistical methods for the real-time assessment of economic activity. Since the financial crisis, they have stepped up their efforts to systematically use micro and survey data to better gauge changes in aggregate consumption, investment and the labour market. In parallel, technological advances have allowed users to start examining unconventional sources such as text data and images from newspaper articles, social media and the internet together with numerical data from payments. Also now available are alternative statistical methods such as regression trees, neural networks and support-vector machines that may help the potential insights that can be gained from these data sources to be fully exploited.\n\nThe coronavirus (COVID-19) pandemic has accelerated this trend. The crisis associated with the pandemic has shown that “big data” can provide timely signals on the state of the economy and help to track economic activity alongside more traditional data. Big data are commonly characterised as having three Vs: high volume, high velocity and high variety.\n[\n1\n]\nHigh volume refers to the massive amounts of data generated as a result of the proliferation of devices, services and human interaction. High velocity refers to the fast speed at which the data are created and processed. High variety relates to the wide range and complexity of data types and sources.\n[\n2\n]\nBig data are appealing because they are available at high frequency; however, they are often relatively unstructured and are, by definition, large in size. This in turn poses various challenges for traditional econometric models. Some of these can be addressed by machine learning (ML) algorithms, which also have the advantage of potentially capturing complex non-linear relationships. Even though there is no single definition of machine learning, the basic idea behind it is that computers (machines) can learn from past data, identify general patterns – often characterised by non-linear relationships – and make predictions using algorithms capturing those patterns. Machine learning is therefore a subset of artificial intelligence, and most of its methods are largely based on concepts from statistics and statistical learning theory.\n[\n3\n]\n\nThis article reviews how policy institutions – international organisations and central banks – use big data and/or machine learning methods to analyse the business cycle. Specifically, these new data sources and tools are used to improve nowcasting and short-term forecasting of real GDP. They are also employed to gain useful insights for assessing cyclical developments and building narratives. A number of illustrative examples are provided.\n\nThe article is organised as follows. Section 2 reviews the main sources of big data that central banks and other policy institutions have been exploring for business cycle analysis over recent years. It provides an overview of existing literature and also includes two examples of how big data have been used to monitor economic activity and labour market developments during the pandemic. Section 3 discusses the main advantages of ML methods in dealing with big data and analysing the business cycle. This section includes two examples using newspaper articles to build measures of economic sentiment and economic policy uncertainty. Section 4 presents the main conclusions and discusses opportunities and challenges faced by central banks when using machine learning and big data.\n\n2 How do big data help to gauge the current state of the economy?\n\nPolicy institutions have recently started to incorporate structured and unstructured big data in their economic analysis. Big data can be structured – such as those collected in large financial datasets that can be matched to firm-level financial statements – or unstructured. Unstructured data range from large and near-real-time data gleaned from the internet (e.g. internet search volumes, data from social networks such as Twitter and Facebook, newspaper articles) to large-volume data obtained from non-official sources (e.g. trading platforms and payment systems or GPS-based technologies).\n\nStructured data, such as those from financial and payment transactions, can provide critical real-time information for assessing aggregate consumption and economic activity. As the use of credit and debit cards to purchase goods and services has increased, the underlying financial transaction data have provided useful information to track consumption and economic activity. At the same time, payments data are available promptly and subject to few revisions since they are financial records. Central banks had already started to regard these data as a valuable source of information before the pandemic emerged. Analysis based on data for the Netherlands, Norway. Portugal and Spain, among others, finds that retail payment systems data (i.e. credit and debit card payments at the point of sale and ATM withdrawals) helped retail sales, private consumption (especially of non-durables) and even real GDP to be forecast in the previous expansionary phase.\n[\n4\n]\nFor Italy, some gains in forecast accuracy have been reported when information from highly aggregated but large value payments (i.e. TARGET2) has been included in GDP nowcasting models.\n[\n5\n]\n\nTurning to unstructured big data, the use of text data from newspapers to understand and forecast the business cycle has increased significantly in the recent years. In business cycle analysis, text data from newspapers and social media have been used to construct proxy measures for unobservable variables such as “sentiment” or “uncertainty” which are likely to be associated with macroeconomic fluctuations. These proxies can be obtained at relatively low cost (in contrast to expensive survey-based measures) and on a timely basis (e.g. daily) by means of automated natural language processing methods. For instance, news-based sentiment indicators can serve as early warning indicators of financial crises.\n[\n6\n]\nNewspaper-based sentiment and economic policy uncertainty indexes for Italy and Spain have proved helpful in monitoring economic activity in real time and nowcasting GDP.\n[\n7\n]\nSimilarly, in Belgium daily average economic media news sentiment is found to be useful for nowcasting survey-based consumer confidence.\n[\n8\n]\nAt the ECB, newspaper-based daily sentiment indicators have been estimated for the four largest euro area countries and the euro area as a whole. These indicators demonstrate a high correlation with survey-based sentiment indicators and real GDP; they are also found to be useful for nowcasting GDP, particularly at the beginning of the quarter when other more traditional indicators (e.g. surveys) referring to the current quarter have not been released yet (see Box 3 in Section 3). In addition, economic policy uncertainty indexes have been estimated for the same set of countries. The ML methods employed also allow uncertainty to be decomposed into sub-components that point towards the main sources (see Box 4 in Section 3).\n\nSimilarly, the use of internet searches has also started to feature in short-term forecasting models. Several Eurosystem studies show that internet searches can provide information about future consumption decisions. Recent examples include analysis linking Google search data to euro area car sales, the use of Google search data to enhance German GDP nowcasting model and the analysis exploiting synthetic indicators based on Google searches for forecasting private consumption in Spain. For the euro area as a whole, Google data provide useful information for GDP nowcasting when macroeconomic information is lacking (i.e. in the first four weeks of the quarter), but as soon as official data relating to the current quarter become available, their relative nowcasting power diminishes.\n[\n9\n]\n\nInternet-based data can also help when assessing the tightness of the labour and housing markets. Analysis for the US labour market shows that including Google-based job-search indicators improves the accuracy of unemployment forecasts, particularly over the medium-term horizon (i.e. three to 12 months ahead).\n[\n10\n]\nIn the euro area, a measure of labour market tightness based on the number of clicks on job postings has recently been built for the Irish economy.\n[\n11\n]\nFor the housing market, analysis for Italy found that metrics based on web-scraped data from an online portal for real estate services can be a leading indicator of housing prices.\n[\n12\n]\nDuring the pandemic, Google searches on topics related to job retention schemes and layoffs provided early insight into the strong impact of the pandemic and related policy measures. Moreover, online data on job posting and hiring in the euro area have complemented official statistics (see Box 1).\n\nBox 1\nMonitoring labour market developments during the pandemic\n\nPrepared by Vasco Botelho and Agostino Consolo\n\nThis box shows how high-frequency data on hiring was helpful for monitoring labour market developments in the euro area during the pandemic. The COVID-19 crisis had a large downward impact on the number of hires in the euro area labour market. Lockdowns and other containment measures suppressed labour demand and discouraged the search efforts of some workers who lost their jobs and transitioned into inactivity.\n[\n13\n]\nMoreover, both the heightened macroeconomic uncertainty during the COVID-19 crisis and the widespread use of job retention schemes further reduced the incentives for firms to hire, albeit for different reasons. The heightened uncertainty encouraged firms to lower their operating costs and delay any plans to expand their workforce. By contrast, job retention schemes protected employment and supported jobs, thus incentivising labour hoarding and allowing firms to avoid high re-hiring costs when economic expansion resumes.\n[\n14\n]\n\nThe LinkedIn hiring rate complements the information that can be retrieved from the official statistical data, providing a timely, high-frequency indicator on gross hires in the euro area during the pandemic.\n[\n15\n]\nHires in the euro area can only be observed imperfectly in the official statistical data, by analysing transitions between employment and non-employment. Two main caveats arise when using official data to assess hire behaviour in the euro area. First, official data are not very timely, generally only becoming available around two quarters later. Second, these data only allow quantification of net flows into (or out of) employment and do not provide any information on job-to-job transitions.\n[\n16\n]\nThe LinkedIn hiring rate provides a more timely, high-frequency signal that can provide information on the number of hires in the euro area. It comprises high-frequency data on gross hires, identifying both movements from non-employment into employment and job-to-job transitions.\n\nThe standardised LinkedIn hiring rate is first calculated for each of the four largest euro area countries (France, Germany, Italy and Spain – the EA-4) by filtering out seasonal patterns and country-specific artificial trends related to the market performance of LinkedIn. The EA-4 country information is aggregated as a weighted average of the country-specific standardised hiring rates using employment as weights. The EA-4 hiring rate declined significantly at the start of the pandemic before recovering during the second half of 2020 (Chart A, panel (a)). After standing at around 6% above average during the first two months of 2020, it fell suddenly to 63% below average in April 2020 following the onset of the COVID-19 crisis and slowly rebounded to surpass its average level in November 2020. It then returned to below average in January 2021, when more stringent lockdowns were imposed, and recovered again thereafter. Interestingly, the decline in the number of hires paralleled the increase in job retention schemes during the pandemic. In April 2021 the standardised hiring rate stood at 14% above average in the EA-4 aggregate.\n\nMonitoring the EA-4 labour market using high-frequency data\n\n(percentages)\n\nSources: Eurostat, LinkedIn, German Institute for Employment Research (IAB), ifo Institute, French Ministry of Labour, Employment and Economic Inclusion, Italian National Institute for Social Security, Spanish Ministry of Inclusion, Social Security and Migrations, and ECB staff calculations.\nNotes: The hiring rate is calculated as the percentage of LinkedIn members who started a job in a given month and added a new employer to their profile in that month, divided by the total number of LinkedIn members in that country. To adjust for artificial trends related to the market performance of the platform and for seasonal patterns and spikes due to specific calendar dates, for each country we have filtered out the effects of a series of monthly dummy variables and a linear, yearly trend on the hiring rate. This allow us to express the estimated standardised hiring rate as percentage deviations from the sample average. The forecast of the monthly unemployment rate follows the box entitled “High-frequency data developments in the euro area labour market” in Issue 5/2020 of the ECB’s Economic Bulletin, starting in January 2020, implying that the range of plausible forecasts for the unemployment rate in 2020-21 is conditional on the unemployment rate in December 2019.\n\nThe high-frequency information provided by the hiring rate can also be used to assess fluctuations in the unemployment rate during the pandemic. Following the box entitled “High-frequency data developments in the euro area labour market” in Issue 5/2020 of the ECB’s Economic Bulletin, we conduct a forecasting exercise linking the high-frequency information of the LinkedIn hiring rate to the job finding rate and using the implied path of the aggregate job finding rate as a proxy for the point-in-time, steady-state unemployment rate. This is then used to forecast the fluctuations in the unemployment rate during the pandemic.\n[\n17\n]\nWe thus compare the observed fluctuations in the unemployment rate from March 2020 onwards with those implied by the high-frequency information within the standardised hiring rate for the EA-4 aggregate.\n\nThe forecast for the unemployment rate using the high-frequency hiring rate provides an early signal of the increase in the unemployment rate for the EA-4 aggregate. Chart A (panel (b)) compares the actual unemployment rate with the ex ante conditional forecast of the unemployment rate using the high-frequency hiring rate and based on the unemployment rate in December 2019. The early signal peak in the unemployment rate forecast in April 2020 at 8.8% is comparable in magnitude with the later August 2020 peak in the actual unemployment rate at 9.1%. More recently, in March 2021 the actual unemployment rate of the EA-4 aggregate was 8.5%, within the plausible range of between 7.8% and 8.7% forecast using the high-frequency hiring rate. The early peak for the forecast unemployment rate was driven by the contraction in the high-frequency hiring rate, which reflected the hiring freezes that followed the widespread use of job retention schemes and allowed separations to remain broadly constant over the initial period of the pandemic. By contrast, most of the recent variation in the unemployment rate (including its stabilisation) has stemmed from an increase in the separation rate.\n\nThe experience gained with structured and unstructured data prior to the pandemic made it easier to deploy models quickly to facilitate the real-time assessment of the economic situation during the pandemic. In particular, these data have been used to assess the degree of slack in the labour market and to measure the decline in economic activity, seen from both the supply and the demand side. During this period of sudden economic disruption, high-frequency alternative data such as electricity consumption, card payments, job postings, air quality and mobility statistics have been crucial for gaining a timely picture of the economic impact of the pandemic and the associated containment measures, weeks before hard and survey data were released. Payment data have been key to understanding the developments in private consumption, one of the demand components most severely affected by the crisis.\n[\n18\n]\nConsumption of key inputs such electricity, gas and fuel was used as a proxy for production in some sectors. A timely understanding of developments in the services sector, with a special focus on small businesses in certain service activities such as tourism which have borne the brunt of the crisis, was also very important. High-frequency information available for these sectors related mostly to sales (e.g. sales in tax returns, card payments), online bookings and Google searches. Other indicators such as freight movements, numbers of flights and air quality were informative as rough proxies for economic activity.\n\nOne effective way of summarising information from a set of high-frequency indicators is to use economic activity trackers. Box 2 provides an example of a weekly economic activity tracker for the euro area devised by the ECB. Similarly, the European Commission’s Joint Research Centre and Directorate-General for Economic and Financial Affairs have been tracking the COVID-19 crisis by combining traditional macroeconomic indicators with a high number of non-conventional, real-time and extremely heterogeneous indicators for the four largest economies in the euro area.\n[\n19\n]\nThey have developed a toolbox with a suite of diverse models, including linear and non-linear models and several ML methods, to exploit the large number of indicators in the dataset for nowcasting GDP. The GDP forecasts are produced by first estimating the whole set (thousands) of models and then applying automatic model selection to average out the forecasts and produce the final forecast.\n\nBox 2\nA weekly economic activity tracker for the euro area\n\nPrepared by Gabriel Pérez-Quirós and Lorena Saiz\n\nSince the onset of the pandemic, several central banks and international institutions have developed experimental daily or weekly economic activity trackers by combining several high-frequency indicators.\n[\n20\n]\nThe Federal Reserve Bank of New York, for example, produces the Weekly Economic Index (WEI) that combines seven weekly indicators for the US economy.\n[\n21\n]\nBased on a similar methodology, the Deutsche Bundesbank publishes the weekly activity index (WAI) for the German economy, which combines nine weekly indicators but also includes monthly industrial production and quarterly GDP.\n[\n22\n]\nAlso, the OECD has developed a weekly activity tracker for several countries based on Google Trends data.\n[\n23\n]\n\nAlthough these indicators are appealing, their development presents three key technical issues. First, the short time span available for high-frequency data makes them less reliable for establishing econometric relations which prove stable over time, compared to long time series of monthly economic indicators.\n[\n24\n]\nSecond, high-frequency indicators are extremely noisy, exhibit complex seasonal patterns and, in some cases, may be subject to frequent data revisions. In the special circumstances associated with the COVID-19 crisis, these indicators were very informative (i.e. the signal-to-noise ratio was high), but in normal times it is still open to question whether these will only add noise to the already reliable signal obtained from the standard monthly indicators.\n[\n25\n]\nThird, the procedure to select indicators has not been standardised. Up to now, most work has used high-frequency indicators that are readily available for each economy. The lack of harmonised selection procedures reduces the scope to “learn from the cross-section” and accentuates the representativeness problem mentioned above.\n\nThe weekly economic activity tracker for the euro area proposed in this box addresses these issues by combining reliable monthly indicators that have a long history of good predictive performance with timely high-frequency (non-standard) indicators. The indicators have been selected according to several criteria: (i) availability of a long enough history (at least three years), (ii) not too noisy, and (iii) the weight of the indicator in the aggregate that combines all of them (a principal component in the case of the indicator discussed here) is statistically significant and economically meaningful.\n[\n26\n]\n\nThe design of the tracker is based on principal component analysis (PCA) with unbalanced data, as described by Stock and Watson.\n[\n27\n]\nFirst, a tracker using only weekly series is computed by PCA to fill the missing observations at the beginning and, if necessary, the end of the sample. The weekly series are transformed into month-on-month growth rates.\n[\n28\n]\nIf necessary, seasonal adjustment methods are used to eliminate any seasonal effects. Second, the monthly variables are transformed into weekly frequency by imputing the same monthly level for all weeks of the month. Then, the month-on-month growth rates are computed for every week. With all this information, the PCA is run again including all the indicators which were originally available at weekly and monthly frequency. The first principal component is the tracker, which represents the evolution of monthly activity on a weekly frequency (Chart A, panel (a)).\n[\n29\n]\nVisualising the tracker in levels and monthly frequency gives an idea of the magnitude of the output loss associated with the pandemic compared with pre-pandemic levels. Most importantly, the evolution of the tracker in levels over 2020 mirrors the evolution of GDP very well (Chart A, panel (b)). Overall, the relatively good performance of the tracker, which strikes a good balance between timely and reliable indicators, makes it a useful tool for tracking economic activity in real time.\n\nEuro area economic activity tracker\n\n(panel (a): month-on-month percentages; panel (b): levels, 100=December 2019 or Q4 2019)\n\nSources: ECB staff calculations and Eurostat.\nNote: The latest observations are for the week of 29 May 2021 for the trackers and Q1 2021 for GDP.\n\n3 What makes machine learning algorithms useful tools for analysing big data?\n\nWhile big data can help improve the forecasts of GDP and other macroeconomic aggregates, their full potential can be exploited by employing ML algorithms. Section 2 shows that in many cases, the improvement in forecasting performance relates to specific situations, such as when traditional monthly indicators for the reference quarter are not yet available. This section focuses on the modelling framework, arguing that ML methods help to reap the benefits of using big data. The main goal of ML techniques is to find patterns in data or to predict a target variable. Although ML algorithms estimate and validate predictive models in a subset of data (training sample), the ultimate aim is to obtain the best forecasting performance using a different subset of data (test sample). The distinction between machine learning and traditional methods is not clear-cut since some traditional methods (e.g. linear regression, principal components) are also quite popular in the ML literature. However, the literature on machine learning has developed a host of new and sophisticated models that promise to strongly enrich the toolbox of applied economists. Moreover, it also seems fair to say that, so far, machine learning has been mostly focused on prediction, while more traditional econometric and statistical analysis is also interested in uncovering the causal relationships between economic variables.\n[\n30\n]\nThis is changing fast, as more and more researchers in the ML field address the issue of inference and causality, although this frontier research is not yet widely applied in the policy context.\n[\n31\n]\nThe aim of this section is to discuss how machine learning can usefully complement traditional econometric methods, in particular to leverage the opportunities for analysing the business cycle offered by big data. It also reviews several contributions to forecasting/nowcasting GDP (see Box 3) and provides examples of how ML algorithms can provide interesting insights for policy, such as pointing towards the sources of economic policy uncertainty (see Box 4).\n\nThe size of the newly available databases in itself often constitutes an obstacle to the use of traditional econometrics. Techniques have been adopted to reduce the dimensionality of the data, including traditional methods such as factor models and principal component analysis, but more often going into newer versions of machine learning. While a description of specific methods is beyond the scope of this article, it is important to note that ML methods have several desirable features for summarising the data, allowing precise reduction of high-dimensional data into a number of manageable indicators.\n\nThe first key advantage of ML methods is their ability to extract and select the relevant information from large volumes of, unstructured data. When dealing with big data, the presence of a large amount of mostly irrelevant information engenders the problem of data selection. This issue is magnified by the presence of large, unstructured datasets.\n[\n32\n]\nIn some simple cases, the forecaster can pick variables manually; this is normally possible when forecasting very specific quantities. The seminal work of Choi and Varian with Google Trends, for instance, focuses on car sales, unemployment claims, travel destination planning and consumer confidence.\n[\n33\n]\nWhere macroeconomic aggregates are involved, the choosing of relevant variables quickly becomes intractable. ML methods offer very useful tools for selecting the most informative variables and exploiting their information potential. Several techniques derived from the model-averaging literature have also proved popular and successful in improving forecasting accuracy. In these methods, a large number of econometric models are first estimated, their forecasting performance is then evaluated, and the final forecast is obtained by averaging the forecasts of the best models, thus retaining those models and explanatory variables that provide useful information. Similarly, what are known as ensemble methods such as random forests and bagging combine different “views” of the data given by competing models, adding flexibility and robustness to the predictions.\n\nThe second key advantage of ML methods is their ability to capture quite general forms of non-linearities. This is a general advantage of ML methods, regardless of the volume of data concerned; however, the issue is that, by their very nature, big data may be particularly prone to non-linearities. For instance, the data stemming from social networks present a good way to understand these inherent non-linearities. In this case, a specific topic can generate cascade or snowball effects within the network which cannot be channelled in linear regression models. Other examples include Google Trends and Google search categories, which are compiled using ML algorithms that determine the category to which an internet search belongs.\n[\n34\n]\nText data are also obtained by applying highly non-linear ML algorithms to news items, for example. More generally, non-linearities and interactions between variables are common in macroeconomics owing to the presence of financial frictions and uncertainty. Several works have found that ML methods can be useful for macroeconomic forecasting, since they better capture non-linearities (e.g. Coulombe et al.). These methods can, for instance, capture the non-linear relationship between financial conditions and economic activity, among others, and hence more accurately predict activity and recessions in particular (see Box 3). Also, ML methods can outperform standard methods (e.g. credit scoring models, logistic regression) when predicting consumer and corporate defaults, since they capture non-linear relationships between the incidence of default and the characteristics of the individuals.\n[\n35\n]\n\nThe COVID-19 pandemic is an important source of non-linearities. During the pandemic, many macroeconomic variables have recorded extreme values that are far from the range of past values. Econometric methods such as linear time series analysis seek to find average patterns in past data. If current data are very different, linearly extrapolating from past patterns may lead to biased results. Central banks, the European Commission and other institutions have adapted their nowcasting frameworks to capture non-standard data and non-linearities.\n[\n36\n]\n\nFinally, ML techniques are the main tool used to capture a wide set of phenomena that would otherwise remain unquantified. The most prominent example in recent years is the dramatic surge of text data analysis. Today, broad corpuses of text are analysed and converted into numbers that forecasters can use. For instance, a wide range of timely, yet noisy confidence indicators based on text currently complement the traditional surveys, which are available with considerable lags and where agents do not necessarily “vote with their behaviour”, as well as market-based indicators, where expectations and other factors such as risk aversion compound in the data. A first generation of work built on word counts has been followed by more sophisticated approaches.\n[\n37\n]\nSecond-generation techniques based on unsupervised learning are also used in public institutions, and in particular in central banks, to assess the effect of their communication. Finally, following Baker et al., concepts such as economic policy uncertainty which were previously difficult to quantify are now currently assessed on the basis of their economic consequences and used in forecasting.\n[\n38\n]\nSee Box 3 and Box 4 for examples.\n\nBox 3\nNowcasting euro area real GDP growth with newspaper-based sentiment\n\nPrepared by Julian Ashwin, Eleni Kalamara and Lorena Saiz\n\nThis box presents economic sentiment indicators for the euro area derived from newspaper articles in the four largest euro area countries in their main national languages.\n[\n39\n]\n\n[\n40\n]\nAvailable at daily frequency, these indicators contain timely economic signals which are comparable to those from well-known sentiment indicators such as the Purchasing Managers’ Index (PMI). Furthermore, they can materially improve nowcasts of real GDP growth in the euro area.\n\nIn the literature, two approaches are typically followed for building sentiment metrics from textual data. The most popular is to use simple word counts based on predetermined sets of words, known as dictionaries or lexicons. However, most of the dictionaries have been developed for the English language. For the euro area, the multilingual environment makes it necessary to either develop new dictionaries for other languages or translate texts into English. Alternatively, more computationally demanding model-based methods such as semantic clustering or topic modelling can extract topics which can be approximated to sentiment and its drivers. In this box, the sentiment metrics are based on counts of words in the news articles translated into English, relying on several well-known English language dictionaries.\n[\n41\n]\nFor the sake of space, only the sentiment metrics based on the financial stability-based dictionary and the general-purpose dictionary VADER are reported.\n[\n42\n]\n\nRegardless of the dictionary used, and despite some noisiness, the newspaper-based sentiment metrics are highly correlated with the PMI composite index in the period from 2000 to 2019 (Chart A, panel (a)). This confirms that these measures are actually capturing sentiment. However, the choice of dictionary matters when it comes to detecting turning points. The first sentiment metric captures the Great Recession very well, unsurprisingly given the financial nature of this crisis. But this metric fails to encapsulate the COVID-19 crisis (Chart A, panel (b)), although its evolution is consistent with the behaviour of the financial markets and the financing conditions which have remained favourable in the context of very strong policy response. By contrast, the general-purpose dictionary is more consistent and robust across time. Therefore, it appears that the nature of economic shocks may play a significant role in identifying the most appropriate text dictionary to be used.\n\nPMI and newspaper-based sentiment indexes for the euro area\n\n(standardised units)\n\nSources: ECB staff calculations, Factiva, IHS Markit and Eurostat.\nNotes: The news-based sentiment indicator is based on newspaper articles from the four largest euro area countries. The metric used is the sum of positive and negative words using either a financial stability dictionary (Correa et al.) or VADER, a more general-purpose dictionary. The PMI composite index and the news-based sentiment indicators are standardised using historical mean and variance.\n\nVarious studies have found that text analysis can significantly improve forecasts of key macroeconomic variables.\n[\n43\n]\nSome forecast accuracy gains (not shown) are found for real-time GDP nowcasts derived using the PMI composite index and the text-based sentiment indicators as key predictors. They are typically concentrated in the nowcasts produced in the first half of the quarter (i.e. first six weeks), when most other indicators used to nowcast GDP are not yet available. This result is in line with other works in the literature. However, an important point is that the type of model matters to fully reap the true benefits of the timeliness of text-based information. Standard linear methods (e.g. ordinary least squares linear regression) work well in calm times when there are no big shifts in the economic outlook. When extreme economic shocks occur, however, ML models can capture non-linearities and filter out the noise (Chart B). Ridge regressions captured the financial crisis better, as shown by the fact that they have the lowest Root Mean Squared Forecast Error (RMSFE), particularly when including the sentiment metric based on the financial stability dictionary. However, the best-performing models during the pandemic have been the neural networks, which were the worst-performing models during the financial crisis. This could be explained by the fact that before the financial crisis, there were no other similar crises in the training sample from which the model could learn. Indeed, one of the criticisms of the more complex ML models is that they need large amounts of data to learn (i.e. they are “data hungry”).\n\nForecast accuracy\n\nRMSFE\n\n(percentage points)\n\nSource: ECB staff calculations.\nNotes: The chart reports the RMSFE over a rolling window of eight quarters. The forecasts are updated at the end of the first month of the reference quarter. The reference variable is the vintage of real GDP growth as of 24 March 2021.\n\nBox 4\nSources of economic policy uncertainty in the euro area and their impact on demand components\n\nPrepared by Andrés Azqueta-Gavaldón, Dominik Hirschbühl, Luca Onorante and Lorena Saiz\n\nThis box describes how big data and machine learning (ML) analysis can be applied to the measurement of uncertainty using textual data. Similarly to “economic sentiment”, uncertainty is not directly observable and can only be measured using proxies. Recent developments in the literature have shown that textual data can provide good proxies for this latent variable. For instance, the seminal work by Baker, Bloom and Davies proposed building an economic policy uncertainty (EPU) index using a pre-specified set of keywords in newspaper articles.\n[\n44\n]\nRecent research by the ECB has built an EPU index across the four largest euro area countries by applying ML algorithms to newspaper articles.\n[\n45\n]\nThe main advantage of this approach is that it can be easily applied to different languages without relying on keywords, given that the underlying algorithm classifies text into topics without prior information. This feature makes it less prone to selection bias. Moreover, this approach retrieves topics underpinning aggregate economic policy uncertainty (e.g. fiscal, monetary or trade policy uncertainty) in newspaper articles. This can be particularly useful for building narratives and economic analysis.\n[\n46\n]\nML methods applied to a sample of newspaper articles from France, Germany, Italy and Spain over the sample period from 2000 to 2019 consistently revealed the following topics or sources of economic policy uncertainty: monetary policy; fiscal policy; political, geopolitical and trade policy; European regulation; domestic regulation; and energy policy.\n\nEconomic policy uncertainty stems from different sources which affect consumers’ and firms’ decisions differently. For instance, increases in uncertainty regarding future tariffs can have an impact on a firm’s determination to build a new production plant or to start exporting to a new market. This is because the role of future conditions is particularly relevant for costly, irreversible decisions. By contrast, uncertainty about the future monetary policy stance can be important for both firms’ and consumers’ spending decisions, since it will influence their expectations about future economic developments and financing conditions.\n\nA simple structural vector autoregression (SVAR) analysis confirms that increases in (ML-based) EPU have a significant negative impact on private consumption and business investment proxied by investment in machinery and equipment in the euro area. The impact on investment is greater than on consumption, suggesting that uncertainty may have more of an impact on the supply side.\n[\n47\n]\nAs regards sources of economic policy uncertainty, the focus is only on energy, trade and monetary policy uncertainty for the sake of space. As expected, monetary policy uncertainty shocks have a clear negative impact on both investment and consumption. By contrast, the impact of increases in trade policy uncertainty is insignificant in both cases. Moreover, increases in energy policy uncertainty depress consumption to a greater extent than other sources, while their effect on investment, albeit weaker, is more persistent over time. While these are aggregate results, EPU is likely to play a more relevant role for firm-level capital investment than at aggregate level.\n[\n48\n]\n\nImpulse responses of consumption (panel (a)) and investment (panel (b)) to economic policy uncertainty shocks\n\n(y-axis: percentage points; x-axis: quarters)\n\nSources: Azqueta-Gavaldón et al. and Eurostat.\nNotes: The impulse responses illustrate the response of consumption and investment to a positive one standard deviation shock in each of the measures of economic policy uncertainty. They are estimated with Bayesian structural vector autoregressions (SVAR), and the shocks are identified using a Cholesky decomposition with the variables in the following order: exports of goods and services, measure of economic policy uncertainty, private consumption, machinery and equipment investment, shadow short rate and EURO STOXX. All the variables are in quarterly growth rates, except for the shadow short rate, which is in levels. The estimation period is from 2000 to 2019. The measures of uncertainty are standardised so that the size of the shock is comparable. The confidence band corresponds to the 68% credibility band of the SVAR with the economic policy uncertainty index.\n\n4 Conclusions, challenges and opportunities\n\nThis article has described how big data and ML methods can complement standard analysis of the business cycle. A case in point is the coronavirus pandemic, which represents an extraordinary shock. This crisis has propelled the dissemination and refinement of ML techniques and big data at an unprecedented speed. In particular, it has shown that alternative sources of data can provide more timely signals on the state of the economy and help to track economic activity. Furthermore, it is an important showcase for non-linearities in the economy, which has required existing models to be adapted or new approaches to be developed. In this respect, ML methods can deal with non-linearities more easily than traditional methods. Besides new opportunities, these new data sources and methods also pose some challenges.\n\nBig data allow a wider range of timely indicators to be used for forecasting (e.g. text-based or internet-based indicators), although in some cases this can entail replicability and accountability issues. Text-based sentiment indicators are particularly useful, for instance, given that they can be produced automatically at higher frequency and at lower cost than survey-based indicators. While the construction of conventional economic data, such as industrial production, follows harmonised procedures to ensure high quality, continuity and comparability over time and countries, alternative data are neither collected primarily for economic analysis, nor sourced and validated by independent statistical offices. Therefore, their application in decision-making processes exposes central banks to various risks, given that the replicability of results and accountability could be impaired. Since alternative data are collected for other purposes (e.g. credit card transactions) or come as the by-product of another service (e.g. news articles from the digitisation of newspapers), the data are often very noisy and require careful treatment. Moreover, the existence of significant data accessibility issues and limitations to data sharing could impair the replicability of the results in some cases. All these risks require careful consideration when investing scarce resources in software development and legal issues, as well as customising IT infrastructure.\n[\n49\n]\n\nAlthough useful as complements, at the moment these tools cannot be considered as substitutes for standard data and methods due to issues of interpretability and statistical inference. ML methods can help overcome the shortcomings of big data and exploit their full potential. When combined with big data, ML methods are capable of outperforming traditional statistical methods and providing an accurate picture of economic developments. Despite the good forecasting performance, the complexity of the methods often makes it difficult to interpret revisions to the forecasts and most importantly to communicate them. However, rapid advances are being made on enhancing the interpretability of ML techniques (most recently based on Shapley values).\n[\n50\n]\nIn addition, ML techniques are not originally designed to identify causal relationships, which is of critical importance to policymakers. Enhancing the ability of ML methods to capture causality is currently the biggest challenge; this has the potential to make ML techniques promising complements and viable alternatives to established methods.\n[\n51\n]\n\nDisclaimer\nPlease note that related topic tags are currently available for selected content only.\n\nCopyright 2025, European Central Bank\n\nOur website uses cookies\n\nWe use functional cookies to store user preferences; analytics cookies to improve website performance; third-party cookies set by third-party services integrated into the website. You have the choice to accept or reject them. For more information or to review your preference on the cookies and server logs we use, we invite you to:\n\nRead our privacy statement\nLearn more about how we use cookies\n"
}