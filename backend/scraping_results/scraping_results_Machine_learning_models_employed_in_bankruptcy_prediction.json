{
    "content1": "\nAbstract\n\nWith improved machine learning models, studies on bankruptcy prediction show improved accuracy. This paper proposes three relatively newly-developed methods for predicting bankruptcy based on real-life data. The result shows among the methods (support vector machine, neural network with dropout, autoencoder), neural network with added layers with dropout has the highest accuracy. And a comparison with the former methods (logistic regression, genetic algorithm, inductive learning) shows higher accuracy.\n\nKeywords\n\nSupport Vector Machine, Autoencoder, Neural Network, Bankruptcy, Machine Learning\n\nShare and Cite:\n\n1. Introduction\n\nMachine learning is a subfield of computer science. It allows computers to build analytical models of data and find hidden insights automatically, without being unequivocally coded. It has been applied to a variety of aspects in modern society, ranging from DNA sequences classification, credit card fraud detection, robot locomotion, to natural language processing. It can be used to solve many types of tasks such as classification. Bankruptcy prediction is a typical example of classification problems.\n\nMachine learning was born from pattern recognition. Earlier works of the same topic (machine learning in bankruptcy) use models including logistic regression, genetic algorithm, and inductive learning.\n\nLogistic regression is a statistical method allowing researchers to build predictive function based on a sample. This model is best used for understanding how several independent variables influence a single outcome variable [1] . Though useful in some ways, logistic regression is also limited.\n\nGenetic algorithm is based on natural selection and evolution. It can be used to extract rules in propositional and first-order logic, and to choose the appropriate sets of if-then rules for complicated classification problems [2] .\n\nInductive learning’s main category is decision tree algorithm. It identifies training data or earlier knowledge patterns and then extracts generalized rules which are then used in problem solving [2] .\n\nTo see if the accuracy of bankruptcy prediction can be further improved, we propose three latest models―support vector machine (SVM), neural network, and autoencoder.\n\nSupport vector machine is a supervised learning method which is especially effective in cases of high dimensions, and is memory efficient because it uses a subset of training points in the decision function. Also, it specifies kernel functions according to the decision function [3] . Its nice math property guarantees a simple convex optimization problem to converge to a single global problem.\n\nNeural networks, unlike conventional computers, are expressive models that learn by examples. They contain multiple hidden layers, thus are capable of learning very complicated relationships between inputs and outputs. And they operate significantly faster than conventional techniques. However, due to limited training data, overfitting will affect the ultimate accuracy. To prevent this, a technique called dropout―temporarily and randomly removes units (hidden and visible)―to the neural network [4] .\n\nAutoencoder, also known as Diabolo network, is an unsupervised learning algorithm that sets the target values to be equal to the inputs. By doing this, it suppresses the computation of representing a few functions, which improves accuracy. Also, the amount of training data required to learn these functions is reduced [5] .\n\nThis paper is structured as follows. Section 2 describes the motivation for this idea. Section 3 describes relevant previous work. Section 4 formally describes the three models. In Section 5 we present our experimental results where we do a parallel comparison within the three models we choose and a longitudinal comparison with the three older models. Section 6 is the conclusion. Section 7 is the reference.\n\n2. Motivation\n\nThe three models we choose (SVM, neural network, autoencoder) are relatively newly-developed but have already been applied to many fields.\n\nSVM has been used successfully in many real-world problems such as text categorization, object tracking, and bioinformatics (Protein classification, Cancer classification). Text categorization is especially helpful in daily life―web searching and email filtering provide huge convenience and work efficiency.\n\nNeural networks learn by examples instead of algorithms, thus, they have been widely applied to problems where it is hard or impossible to apply algorithmic methods [6] . For instance, finger print recognition is an exciting application. People can now use their unique fingerprints as keys to unlock their phones and payment accounts, free from the troubling, long passwords.\n\nAutoencoders are especially successful in solving difficult tasks like natural language processing (NLP). They have been used to solve the previous seemingly intractable problems in NLP, including word embeddings, machine translation, document clustering, sentiment analysis, and paraphrase detection.\n\nHowever, the usage of the three models in economics or finance is comparatively hard to find. So, we aim to find out if they still work well in economical field by running them with real-life data in a predicting bankruptcy task.\n\nAnother motivation is finding out if the accuracy of this particular problem (bankruptcy prediction) can be improved after reading previous works―The discovery of experts’ decision rules from qualitative bankruptcy data using genetic algorithms [2] , and Predicting Bankruptcy with Robust Logistic Regression [1] ―which uses older models. Thus, a comparison of the models and results is included in this paper.\n\n3. Related Work\n\nMachine learning enables computers to find insights from data automatically. The idea of using machine learning to predict bankruptcy has previously been used in the context of Predicting Bankruptcy with Robust Logistic Regression by Richard P. Hauser and David Booth [1] . This paper uses robust logistic regression which finds the maximum trimmed correlation between the samples remained after removing the overly large samples and the estimated model using logistic regression [1] . This model has its limitation. The value of this technique relies heavily on researchers’ abilities to include the correct independent variables. In other words, if researchers fail to identify all the relevant independent variables, logistic regression will have little predictive value [7] . Its overall accuracy is 75.69% in the training set and 69.44% in testing set.\n\nAnother work, the discovery of experts’ decision rules from qualitative bankruptcy data using genetic algorithms, in 2003 by Myoung-Jong Kim and Ingoo Han uses the same dataset as we do. They apply older models―inductive learning algorithms (decision tree), genetic algorithms, and neural networks without dropout. Since the length of genomes in GA is fixed, a given problem cannot easily be encoded. And GA gives no guarantee of finding the global maxima. The problem of inductive learning is with the one-step-ahead node splitting without backtracking, which may generate a suboptimal tree. Also, decision trees can be unstable because small variations in the data might result in a completely different tree being generated [3] . And the absence of dropout in the neural network model increases the possibility of overfitting which affects accuracy. The overall accuracies are 89.7%, 94.0%, and 90.3% respectively.\n\nThe models we choose either contain a newly developed technique, like dropout, or completely new models that have hardly been utilized in bankruptcy prediction.\n\n4. Model Description\n\nThis section describes the proposed three models.\n\n4.1. Support Vector Machine\n\nSpecifically, we use support vector classify (SVC), a subcategory of SVM, in this task. It constructs a hyper-plane, as shown in Figure 1, in a high dimensional space which is used for classification. Generally, a good separation represented by the solid line in Figure 1 means the distance(the space between the dotted lines) to the nearest training data points (the red and blue dots) of any class (represented by the color red and blue) is the largest. This is also known as functional margin [3] .\n\nWith training vectors in two classes and a vector,\n\nxi∈ℝp,i=1,⋯,n, y∈{1,−1}n\n\nrespectively, SVM aims at solving the problem:\n\nmin\nω,b,ζ\n1\n2\nωTω+C∑\nn\ni=1\nζi\n\nsubject to\n\nyi(ωTϕ(xi)+b)≥1−ζi\n\nIts dual is\n\nmin\nα\n1\n2\nαTQα−eTα\n\nsubject to\n\nyTα=0,  0≤αi≤C, i=1,⋯,n\n\nwhere e is a common vector, C>0\nis upper bound, Q is n by n positive semidefinite matrix, Qij≡yiyjk(xi⋅xj)\n, and K(xi,xj)=ϕ(xi)Tϕ(xj)\nis the kernel.\n\nFigure 1. SVM model [3] .\n\nHere the function implicitly maps the training vectors into a higher dimensional space.\n\nThe decision function is:\n\nsgn(\n∑\nn\ni=1\ny\ni\nα\ni\nK(\nx\ni\n,x)+ρ)\n[3]\n\n4.2. Neural Network with Dropout\n\nNeural networks’ inputs are modelled as layers of neurons. Its structure is shown in the following figure.\n\nAs shown in Figure 1, the formal neuron uses n inputs\nx\n1\n,\nx\n2\n,⋯,\nx\nn\nx\nto classify the signals coming from dendrites, and are then synoptically weighted correspondingly with\nw\n1\n,\nw\n2\n,⋯,\nw\nn\nw\nthat measure their permeabilities. Then, the excitation level of the neuron is calculated as the weighted sum of input values:\n\nξ=\n∑\ni=1\nn\nw\ni\nx\ni\n\nf in Figure 2 represents activation function.\n\nWhen the value of excitation level x reaches the threshold h, the output y (state) of the neuron is induced. This simulates the electric impulse generated by axon [8] .\n\nDropout is a technique that further improves neural network’s accuracy. In Figure 3, let L be the number of hidden layers,\nl∈{1,⋯,L}\nl\nthe hidden layers of the neural network,\nz(l)\nand\ny(l)\nthe vectors of inputs and outputs of layer\nl\nl\n, respectively.\nW(l)\nand\nb(l)\nb\nare the weights and biases at layer\nl\nl\n. For\nl∈{0,⋯,L−1}\nl\nand any hidden unit i, the network then can be described as:\n\nz\n(l+1)\n=\nw\n(l+1)\ny\nl\n+\nb\n(l+1)\n,iii\n\ny\n(l+1)\n=f(\nz\n(l+1)\n)\n,ii\n\nFigure 2. Neural network model.\n\nFigure 3. Artificial neural network.\n\nwhere f is any activation function.\n\nWith dropout, the feed-forward operation becomes:\n\nr(l)-Bernoulli(p), j\n\ny\n(l)\n=\nr\n(l)\ny\n(l)\n,\n\nz\n(l+1)\n=\nw\n(l+1)\ny\nl\n+\nb\n(l+1)\n,iii [4] .\n\n4.3. Autoencoder\n\nConsider an n/p/n autoencoder.\n\nIn Figure 4, let F and G denote sets, n and p be positive integers where 0 < p < n, and B be a class of functions from Fn to Gp.\n\nDefine\nX={\nx\n1\n,⋯,\nx\nm\n}\nas a set of training vectors in Fn. When there are external targets, let\nY={\ny\n1\n,⋯,\ny\nm\n}\ndenote the corresponding set of target vectors in Fn. And ∆ is a distortion function (e.g. Lp norm, Hamming distance) defined over Fn.\n\nFor any A Î A and B Î B, the input vector x Î Fn becomes output vector A ◦ B(x) Î Fn through the autoencoder. The goal is to find A Î A and B Î B that minimize the overall distortion function:\n\nminE(A,B)=minE(\nx\nt\n)=minΔA∘B(\nx\nt\n),\nx\nt\n[10] .\n\n4.4. Decision Tree\n\nGiven training vectors\nx\ni\n∈\nR\nn\n,\ni=1,⋯,l\ni\nand a label vector\ny∈\nR\nl\n, a decision tree groups the sample according to the same labels.\n\nLet Q represents the data at node m. The tree partitions the data\nθ=(j,\nt\nm\n)\n\nFigure 4. An n/p/n Autoencoder Architecture [Pierre Baldi, 2012].\n\n(feature\nj\nj\nand threshold\nt\nm\nt\n) into\nQ\nleft\n(θ)\nand\nQ\nright\n(θ)\nsubsets:\n\nQ\nleft\n(θ)=(x,y)|\nx\nj\n≤\nt\nm\nQ\nright\n(θ)=Q\\\nQ\nleft\n(θ)\n\nThe impurity function\nH( )\nis used to calculate the impurity at m, the choice of which depends on the task being solved (classification or regression)\n\nG(Q,θ)=\nn\nleft\nN\nm\nH(\nQ\nleft\n(θ))+\nn\nright\nN\nm\nH(\nQ\nright\n(θ))\n\nChoose the parameters that minimises the impurity\n\nθ\n∗\n=arg\nmin\nθ\nG(Q,θ)\n\nThen recur for subsets\nQ\nleft\n(\nθ\n∗\n)\nand\nQ\nright\n(\nθ\n∗\n)\nuntil reaching the maximum possible depth,\nN\nm\n<\nmin\nsamples\nN\nor\nN\nm\n=1\nN\n[3] .\n\n5. Experimental Result\n\nThe data we used shown in Table 1, called Qualitative Bankruptcy database, is created by Martin. A, Uthayakumar. j, and Nadarajan. m in February 2014 [10] . The attributes include industrial risk, management risk, financial flexibility, credibility, competitiveness, and operating risk.\n\n5.1. Parallel Comparison\n\n5.1.1. SVM (Linear Kernel)\n\nAs shown in Table 2, the accuracy increases when truncate increases in a SVM model.\n\n5.1.2. Neural Network (Activation = Softmax, Num_Classes = 2, Optimiser = Adam, Loss = Categorical _Crossentropy, Metrics = Accuracy)\n\nAs shown in Table 3, when other things in the model hold the same, dropout rate of 0.5 yields the highest accuracy.\n\nData set\nDimensionality\nInstances\nTraining Set\nTest Set\nValidation\nBankruptcy\n6 times1\n250\n80%\n10%\n10%\n\nData set\n\nDimensionality\n\nInstances\n\nTraining Set\n\nTest Set\n\nValidation\n\nBankruptcy\n\n6 times1\n\n250\n\n80%\n\n10%\n\n10%\n\nTable 1. Dataset Description.\n\nvariation\naccuracy\ntruncate = 50\n0.9899\ntruncate = 100\n0.9933\n\nvariation\n\naccuracy\n\ntruncate = 50\n\n0.9899\n\ntruncate = 100\n\n0.9933\n\nTable 2. Accuracy of Neural Network Model with Truncate 50 or 100.\n\nvariation\naccuracy\nwithout dropout\n0.9867 with loss 0.0462\nwith dropout (dropout rate = 0.1)\n0.9867 with loss 0.0292\nwith dropout (dropout rate = 0.3)\n0.9933 with loss 0.0300\nwith dropout (dropout rate = 0.4)\n0.9933 with loss 0.0401\nwith dropout (dropout rate = 0.5)\n0.9933 with loss 0.0278\nwith dropout (dropout rate = 0.7)\n0.9933 with loss 0.0428\nwith dropout (dropout rate = 0.8)\n0.9867 with loss 0.0318\n\nvariation\n\naccuracy\n\nwithout dropout\n\n0.9867 with loss 0.0462\n\nwith dropout (dropout rate = 0.1)\n\n0.9867 with loss 0.0292\n\nwith dropout (dropout rate = 0.3)\n\n0.9933 with loss 0.0300\n\nwith dropout (dropout rate = 0.4)\n\n0.9933 with loss 0.0401\n\nwith dropout (dropout rate = 0.5)\n\n0.9933 with loss 0.0278\n\nwith dropout (dropout rate = 0.7)\n\n0.9933 with loss 0.0428\n\nwith dropout (dropout rate = 0.8)\n\n0.9867 with loss 0.0318\n\nTable 3. Accuracy of Neural Network Model with and without Dropout.\n\nAs shown in Table 4 and Table 5, we can conclude that adding layers increases accuracy. Figure 5 and Figure 6 depict Table 5.\n\n5.1.3. Autoencoder (Encoding_Dim = 2, Activation = “Relu”, Optimizer = “Adam”, Lose = “Mse”)\n\nAs shown in Table 6, autoencoder with decision tree yields higher accuracy.\n\n5.2. Longitudinal Comparison\n\nAs shown in Table 7, neural network with truncate = 100 with added layers with dropout has the highest accuracy. And all the new models have higher accuracy than the old ones.\n\n6. Conclusions\n\nSupport vector machine, neural network with dropout, and autoencoder are three relatively new models applied in bankruptcy prediction problems. Their accuracies outperform those of the three older models (robust logistic regression, inductive learning algorithms, genetic algorithms). The improved aspects include the control for overfitting, the improved probability of finding the global maxima, and the ability to handle large feature spaces. This paper compared and concluded the progress of machine leaning models regarding bankruptcy prediction, and checked to see the performance of relatively new models in the context of bankruptcy prediction that have rarely been applied in that field.\n\nHowever, the three models also have drawbacks. SVM does not directly give probability estimates, but uses an expensive five-fold cross-validation instead.\n\nvariation\naccuracy\ntwo layer with dropout (dropout rate = 0.5)\n0.9933 with loss 0.0278\nthree layer (added layer with dense 200) with dropout (dropout rate = 0.5)\n0.9933 with loss 0.0221\nfour layer (added layer with dense 16) with dropout (dropout rate = 0.5)\n1.0000 with loss 0.0004\n\nvariation\n\naccuracy\n\ntwo layer with dropout (dropout rate = 0.5)\n\n0.9933 with loss 0.0278\n\nthree layer (added layer with dense 200) with dropout (dropout rate = 0.5)\n\n0.9933 with loss 0.0221\n\nfour layer (added layer with dense 16) with dropout (dropout rate = 0.5)\n\n1.0000 with loss 0.0004\n\nTable 4. Accuracy of Neural Network Model with Two, Three, and Four Layer.\n\nvariation\naccuracy\ntruncate = 50 with four layers (added layer dense 16,200) with dropout rate 0.5\n0.9950 with loss 0.0389\ntruncate = 100 with four layers (added layer dense 16,200) with dropout rate 0.5\n1.0000 with loss 0.0004\n\nvariation\n\naccuracy\n\ntruncate = 50 with four layers (added layer dense 16,200) with dropout rate 0.5\n\n0.9950 with loss 0.0389\n\ntruncate = 100 with four layers (added layer dense 16,200) with dropout rate 0.5\n\n1.0000 with loss 0.0004\n\nTable 5. Accuracy of Neural Network Model with Truncate 50 or 100 and With Four Layers.\n\nvariation\naccuracy\nwith SVM\n0.9867\nwith decision tree\n0.9933\n\nvariation\n\naccuracy\n\nwith SVM\n\n0.9867\n\nwith decision tree\n\n0.9933\n\nTable 6. Accuracy of Neural Network Model with SVM or With Decision Tree.\n\nmodel\naccuracy\nRobust logistic regression\n0.6944\ninductive learning algorithms (decision tree)\n0.897\ngenetic algorithms\n0.94\nneural networks without dropout\n0.903\nSVM truncate = 100\n0.9933\nTruncate = 100 with four layers (added layer dense 16,200) with dropout rate 0.5\n1.0000 with loss 0.0004\nautoencoder (with decision tree)\n0.9933\n\nmodel\n\naccuracy\n\nRobust logistic regression\n\n0.6944\n\ninductive learning algorithms (decision tree)\n\n0.897\n\ngenetic algorithms\n\n0.94\n\nneural networks without dropout\n\n0.903\n\nSVM truncate = 100\n\n0.9933\n\nTruncate = 100 with four layers (added layer dense 16,200) with dropout rate 0.5\n\n1.0000 with loss 0.0004\n\nautoencoder (with decision tree)\n\n0.9933\n\nTable 7. Accuracy of Neural Network Model with Different models.\n\nAlso, if the data sample is not big enough, especially when outnumbered by the number of features, SVM is likely to give bad performance [4] . With dropout, the time to train the neural network will be 2 to 3 times longer than training a standard neural network. An autoencoder captures as much information as possible, not necessarily the relevant information. And this can be a problem\n\nFigure 5. Neural network-loss.\n\nFigure 6. Neural network-accuracy.\n\nwhen the most relevant information only makes up a small percent of the input. The solutions to overcome these drawbacks are yet to be found.\n\nConflicts of Interest\n\nThe authors declare no conflicts of interest.\n\nReferences\n\nCopyright © 2025 by authors and Scientific Research Publishing Inc.\n\nThis work and the related PDF file are licensed under a Creative Commons Attribution 4.0 International License.\n",
    "content2": "\nArticle preview\n\nExpert Systems with Applications\n\nMachine learning techniques in bankruptcy prediction: A systematic literature review\n\nAbstract\n\nIntroduction\n\nSection snippets\n\nMethodology\n\nOverview of the methods in bankruptcy prediction\n\nDiscussion\n\nConclusion\n\nDeclaration of competing interest\n\nAcknowledgment\n\nReferences (202)\n\nImproving experimental studies about ensembles of classifiers for bankruptcy prediction and credit scoring\n\nExpert Systems with Applications\n\nPredicting bankruptcy of local government: A machine learning approach\n\nJournal of Economic Behavior and Organization\n\nProbabilistic modeling and visualization for bankruptcy prediction\n\nApplied Soft Computing Journal\n\nMachine learning models and bankruptcy prediction\n\nExpert Systems with Applications\n\nExtending business failure prediction models with textual website content using deep learning\n\nEuropean Journal of Operational Research\n\nPredicting failure in the U.S. banking sector: An extreme gradient boosting approach\n\nInternational Review of Economics and Finance\n\nVisualization and dynamic evaluation model of corporate financial structure with self-organizing map and support vector regression\n\nApplied Soft Computing Journal\n\nHybrid genetic algorithm and fuzzy clustering for bankruptcy prediction\n\nApplied Soft Computing Journal\n\nApplication of hybrid case-based reasoning for enhanced performance in bankruptcy prediction\n\nInformation Sciences\n\nAnticipating bank distress in the Eurozone: An Extreme Gradient Boosting approach\n\nJournal of Business Research\n\nCited by (3)\n\nWhat makes companies zombie? Detecting the most important zombification feature using tree-based machine learning\n\nAn experimental survey of imbalanced learning algorithms for bankruptcy prediction\n\nReinforced Distillation Learning: Fine-Grained Imbalanced Classifier for Financial Crisis Prediction\n\nRecommended articles\n\nRandomized Quaternion Minimal Gated Unit for sleep stage classification\n\nDistributed algorithm for parallel computation of the n queens solutions\n\nA novel metric for assessing structural complexity of data warehouse requirements models\n\nCookies are used by this site.\nCookie Settings\n\nAll content on this site: Copyright © 2025 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply.\n",
    "content3": "\nThe Top 10 Machine Learning Algorithms to Know\n\nA machine learning algorithm is a set of instructions that machines follow to complete tasks, particularly those involving identifying patterns and making predictions. These are the top machine learning algorithms beginners should know.\n\nIn machine learning, there’s something called the “No Free Lunch” theorem, which essentially states that not every problem can be solved by the same machine learning algorithm — a set of instructions that helps machines complete tasks, especially identifying patterns and making predictions. As a result, you should try many different algorithms for your problem, while using a hold-out “test set” of data to evaluate performance and select the winner.\n\nTop Machine Learning Algorithms You Should Know\n\nOf course, the algorithms you try must be appropriate for your problem, which is where picking the right machine learning task comes in. As an analogy, you might use a vacuum, broom, or mop to clean your house, but you wouldn’t bust out a shovel and start digging.\n\nThe Big Principle Behind Machine Learning Algorithms\n\nThere is a common principle that underlies all supervised machine learning algorithms for predictive modeling:\n\nMachine learning algorithms are described as learning a target function (f) that best maps input variables (X) to an output variable (Y): Y = f(X)\n\nThis is a general learning task where we would like to make predictions in the future (Y) given new examples of input variables (X). We don’t know what the function (f) looks like or its form. If we did, we would use it directly and not need to learn it from data using machine learning algorithms.\n\nThe most common type of machine learning is to learn the mapping Y = f(X) to make predictions of Y for new X. This is called predictive modeling or predictive analytics, and our goal is to make the most accurate predictions possible.\n\nRELATED\n4 Types of Machine Learning to Know\n\n10 Most Common Machine Learning Algorithms\n\nFor those eager to understand the basics of machine learning, here is a quick tour of the top 10 machine learning algorithms used by data scientists.\n\n1. Linear Regression\n\nLinear regression is perhaps one of the most well-known algorithms in statistics and machine learning. Commonly used in predictive modeling, it is primarily concerned with minimizing the error of a machine learning model or making the most accurate predictions possible, at the expense of explainability. We will borrow, reuse and steal algorithms from many different fields, including statistics, and use them towards these ends.\n\nThe representation of linear regression is an equation that describes a line that best fits the relationship between the input variables (x) and the output variables (y), by finding specific weightings for the input variables called coefficients (B).\n\nFor example: y = B0 + B1 * x\n\nWe will predict y given the input x and the goal of the linear regression learning algorithm is to find the values for the coefficients B0 and B1. Different techniques can be used to learn the linear regression model from data, such as a linear algebra solution for ordinary least squares and gradient descent optimization.\n\nSome good rules of thumb when using this technique are to remove variables that are very similar (correlated) and to remove noise from your data, if possible. It is a fast and simple technique and a good first algorithm to try.\n\n2. Logistic Regression\n\nLogistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values). Like linear regression, the goal is to find the values for the coefficients that weight each input variable. Unlike linear regression, the prediction for the output is transformed using a nonlinear function called the logistic function.\n\nThe logistic function looks like a big S and will transform any value into the range 0 to 1. This is useful because we can apply a rule to the output of the logistic function to snap values to 0 and 1 (e.g. IF less than 0.5 then output 1) and predict a class value.\n\nBecause of the way that the model is learned, the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class 0 or class 1. This can be useful for problems where you need to give more rationale for a prediction.\n\nLike linear regression, logistic regression works better when you remove attributes unrelated to the output variable as well as attributes that are very similar (correlated) to each other. It’s a fast model to learn and effective on binary classification problems.\n\n3. Linear Discriminant Analysis\n\nLogistic regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes, the linear discriminant analysis (LDA) algorithm is the preferred linear classification technique.\n\nThe representation of LDA is pretty straightforward. It consists of statistical properties of your data, calculated for each class. For a single input variable, this includes:\n\nPredictions are made by calculating a discriminant value for each class and making a prediction for the class with the largest value. The technique assumes that the data has a Gaussian distribution (bell curve), so it is a good idea to remove outliers from your data beforehand. It’s a simple and powerful method for classification predictive modeling problems.\n\n4. Classification and Regression Trees\n\nDecision trees are an important type of algorithm for predictive modeling machine learning. The representation of the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction. Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n\nTrees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.\n\n5. Naive Bayes\n\nNaive Bayes is a simple but surprisingly powerful algorithm for predictive modeling. The model consists of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes’ theorem. When your data is real-valued, it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities.\n\nNaive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data. Nevertheless, the technique is very effective on a large range of complex problems.\n\nHIRING NOW\nView All Remote Data Science Jobs\n\n6. K-Nearest Neighbors\n\nThe K-nearest neighbor (KNN) algorithm is very simple and very effective. The model representation for KNN is the entire training dataset. Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable. For classification problems, this might be the mode (or most common) class value.\n\nThe trick is deciding how to determine the similarity between the data instances. The simplest technique if your attributes are all of the same scale (all in inches for example) is to use the Euclidean distance — a number you can calculate directly based on the differences between each input variable.\n\nKNN can require a lot of memory or space to store all of the data, but only performs a calculation (or learn) when a prediction is needed. You can also update and curate your training instances over time to keep predictions accurate.\n\nThe idea of distance or closeness can break down in very high dimensions (lots of input variables), which can negatively affect the performance of the algorithm. This is called the curse of dimensionality. It suggests you only use those input variables that are most relevant to predicting the output variable.\n\n7. Learning Vector Quantization\n\nA downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset. The Learning Vector Quantization (LVQ) algorithm is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like.\n\nThe representation for LVQ is a collection of codebook vectors selected randomly in the beginning and adapted to best summarize the training dataset over various iterations of the learning algorithm. After learning, the codebook vectors can be used to make predictions just like KNN.\n\nThe most similar neighbor (best-matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction. Best results are achieved if you rescale your data to have the same range, such as between 0 and 1.\n\nIf you discover that KNN gives good results on your dataset, try using LVQ to reduce the memory requirements of storing the entire training dataset.\n\n8. Support Vector Machines\n\nSupport Vector Machines (SVM) are perhaps one of the most popular and talked about machine learning algorithms. A hyperplane is a line that splits the input variable space and, in SVM, it’s selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two dimensions, you can visualize this as a line. Let’s assume that all of our input points can be completely separated by this line. The SVM learning algorithm finds the coefficients that result in the best separation of the classes by the hyperplane.\n\nThe distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points are relevant in defining the hyperplane and constructing the classifier. These points are called the support vectors. They support or define the hyperplane. In practice, an optimization algorithm is used to find the values for the coefficients that maximize the margin.\n\nSVM might be one of the most powerful out-of-the-box classifiers and is worth trying on your dataset.\n\n9. Bagging and Random Forest\n\nRandom forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called bootstrap aggregation or bagging. The bootstrap is a powerful statistical method for estimating a quantity from a data sample, such as a mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to get a better estimation of the true mean value.\n\nThe same approach is used in bagging, but for estimating entire statistical models — most commonly decision trees. Multiple samples of your training data are taken to construct models for each data sample. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.\n\nRandom forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness. The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their own unique ways. Combining their predictions results in a better estimate of the true underlying output value.\n\nIf you get good results with a highly variable algorithm (like decision trees), you can often get better results by bagging that algorithm.\n\n10. Boosting and AdaBoost\n\nBoosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n\nAdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting. Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.\n\nAdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. The harder training data is to predict, the more weight it is given. Models are created one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence. After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on training data.\n\nBecause so much attention is put on correcting mistakes by the algorithm, it is important that you have clean data with outliers removed.\n\nWhich Machine Learning Algorithm Should I Use?\n\nNow that we’ve reviewed the main types of machine learning algorithms, it’s only natural to wonder, “Which algorithm should I use?” The answer to the question varies depending on many factors, including:\n\nEven an experienced data scientist cannot tell which algorithm will perform the best before trying different algorithms. Starting with these popular machine learning algorithms is always a good first step, especially for those new to machine learning.\n\nFrequently Asked Questions\n\nWhat is a machine learning algorithm?\n\nA machine learning algorithm is a set of processes or steps used by an artificial intelligence system to complete tasks.\n\nMachine learning algorithms are usually executed through computer programs, and instruct machines how and when to solve certain problems or perform certain computations.\n\nWhat are the 4 types of machine learning algorithms?\n\nThe 4 types of machine learning algorithms include:\n\nWhat is a machine learning algorithm example?\n\nA common example of a machine learning algorithm is linear regression, which involves statistically calculating a straight line that reflects the relationship between an independent and dependent variable.\n\nWhat is the easiest machine learning algorithm?\n\nWhile linear regression is an excellent machine learning algorithm for beginners, Naive Bayes, logistic regression and K-nearest neighbor are other algorithms that can help newcomers ease their way into machine learning problems.\n\nRecent Data Science Articles\n"
}